{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113831,
     "status": "ok",
     "timestamp": 1651187112540,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "lK6tS1J8m71Z",
    "outputId": "17b32de9-157a-48f3-8f5f-1d8bcd44211b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
      "\u001b[?25hCollecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 33.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0+cu113\n",
      "    Uninstalling torch-1.11.0+cu113:\n",
      "      Successfully uninstalled torch-1.11.0+cu113\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.12.0\n",
      "    Uninstalling torchtext-0.12.0:\n",
      "      Successfully uninstalled torchtext-0.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
      "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch==1.10.0 torchtext==0.11.0\n",
    "\n",
    "# Reload environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzNsZnK7UWyr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14486,
     "status": "ok",
     "timestamp": 1651187193107,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "UdcYIgPIUald",
    "outputId": "baf96d1f-3c7f-4a49-d571-5282f49f1de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "g_path = \"/content/drive/My Drive/pytorch/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZrVtJ2xhaSJ"
   },
   "outputs": [],
   "source": [
    "data_fl = 'data/IMDB_review_sentiment_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1651187198579,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "CRHqfw6sVSZZ",
    "outputId": "23c572b0-461b-4a79-be50-68f42ee23d2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6e6269c170>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reproducing the same result\n",
    "SEED = 2021\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Us8mAaVWFm"
   },
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "def clean_data(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove break\n",
    "        text = text.replace('br', '')\n",
    "        # remove punctuation\n",
    "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # remove newline\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        # strip the text\n",
    "        text = text.strip()\n",
    "        # lower the text\n",
    "        text = text.lower()\n",
    "\n",
    "        if text != '':\n",
    "          cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(preprocessing=clean_data,tokenize=tokenizer,batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
    "fields = [('text',TEXT),('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1651187205231,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "5qTpaDVpVZQu",
    "outputId": "6b6684ee-8f72-4f83-f8b8-baa612a9acda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'], 'label': '1'}\n",
      "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'] 1\n"
     ]
    }
   ],
   "source": [
    "#loading the entire data\n",
    "def load_data():\n",
    "  imdb_data = data.TabularDataset(path = g_path+data_fl,format = 'csv', fields = fields, skip_header = True)\n",
    "  return imdb_data\n",
    "\n",
    "imdb_data = load_data() \n",
    "print(vars(imdb_data.examples[0]))\n",
    "print(imdb_data.examples[0].text, imdb_data.examples[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "error",
     "timestamp": 1651462818375,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "r42POVJ6fwT1",
    "outputId": "7e8fedfc-913c-4a3d-9841-5b91edef1bb5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5afc12479a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imdb_data' is not defined"
     ]
    }
   ],
   "source": [
    "#splitting the data into training and validation dataset\n",
    "def split_data(imdb_data):\n",
    "  train_data, valid_data = imdb_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
    "  return train_data, valid_data\n",
    "\n",
    "train_data, valid_data = split_data(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202215,
     "status": "ok",
     "timestamp": 1651187412170,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "xNwDnzrGkea5",
    "outputId": "e1443242-0105-44d5-cead-58b0b8d20634"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:16<00:00, 23917.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 466\n",
      "Size of LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "#generate vocabulary\n",
    "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApKB9FJ7-IoC"
   },
   "outputs": [],
   "source": [
    "#preparing batches for training the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort=False, # Sort all examples in data using `sort_key`.\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True, # Use `sort_key` to sort examples in each batch.\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8v8UZ2GNykd"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "  def __init__(self, feature_dim, batch_first, **kwargs):\n",
    "    #Constructor\n",
    "    super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    #variables\n",
    "    self.batch_first = batch_first\n",
    "\n",
    "    #attention parameters (will be learned via back propagation)\n",
    "    self.W = nn.Parameter(torch.FloatTensor(feature_dim, feature_dim), requires_grad=True)\n",
    "    nn.init.kaiming_uniform_(self.W.data)\n",
    "\n",
    "    self.u = nn.Parameter(torch.FloatTensor(feature_dim,1), requires_grad=True)\n",
    "    nn.init.kaiming_uniform_(self.u.data)\n",
    "  \n",
    "\n",
    "  def get_mask(self, attentions, lengths):\n",
    "      \"\"\"\n",
    "      Construct mask for padded itemsteps, based on lengths\n",
    "      \"\"\"\n",
    "      max_len = max(lengths.data)\n",
    "      mask = torch.ones(attentions.size()).detach()\n",
    "\n",
    "      for i, l in enumerate(lengths.data):\n",
    "        idx = int(l.item())\n",
    "        if l < max_len:\n",
    "          mask[i, idx:] = 0\n",
    "      \n",
    "      return mask\n",
    "\n",
    "\n",
    "  def forward(self, x, x_len, mask=None):\n",
    "    \n",
    "    '''\n",
    "      x is the hidden states (output) from lstm layer\n",
    "      x_len contains information for leangth of each row (sentence) in the batch\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    get the dimension information based on lstm batch_first logic\n",
    "    '''\n",
    "    if self.batch_first:\n",
    "      batch_size, max_len = x.size()[:2]\n",
    "    else:\n",
    "      max_len, batch_size = x.size()[:2]\n",
    "\n",
    "\n",
    "    '''\n",
    "    x ~ [batch_size, seq_len, feature_dim] ~ feature_dim == (2 * hidden_dim)\n",
    "    x_len ~ [batch_size]\n",
    "    '''\n",
    "\n",
    "    #[batch_size, seq_len, 1]\n",
    "    e_ij = torch.matmul(\n",
    "                torch.tanh( \n",
    "                      torch.matmul(x, #[batch_size, seq_len, feature_dim]\n",
    "                                   self.W #[feature_dim, feature_dim]\n",
    "                                  ) #[batch_size, seq_len, feature_dim]\n",
    "                ), #[batch_size, seq_len, feature_dim]\n",
    "                self.u #[feature_dim, 1]\n",
    "            ) #[batch_size, seq_len, 1]\n",
    "\n",
    "   \n",
    "    #[batch_size, seq_len, 1]\n",
    "    a_ij = torch.softmax(e_ij, dim=1)  \n",
    "\n",
    "    if mask is not None:\n",
    "      # construct a mask, based on the sentence lengths\n",
    "      mask = self.get_mask(a_ij, x_len)\n",
    "      masked_a_ij = a_ij * mask\n",
    "      a_ij = masked_a_ij\n",
    "\n",
    "\n",
    "    # multiply each hidden state with the attention weights\n",
    "    #[batch_size, seq_len, feature_dim]\n",
    "    weighted_ip = x * a_ij\n",
    "\n",
    "    return weighted_ip, a_ij\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, **kwargs):\n",
    "    #Constructor\n",
    "    super(Classifier, self).__init__(**kwargs)\n",
    "\n",
    "    # variables\n",
    "    self.embedding_dim = 100\n",
    "    self.hidden_dim = 32\n",
    "    self.num_layers = 1\n",
    "    self.bidirectional = True\n",
    "    self.batch_first = True\n",
    "    self.output_dim = 1\n",
    "\n",
    "    #embedding layer\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                  embedding_dim=self.embedding_dim)\n",
    "\n",
    "    #lstm layer\n",
    "    self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                        hidden_size=self.hidden_dim, \n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=self.batch_first)\n",
    "    \n",
    "    self.attn = Attention(self.hidden_dim * 2, batch_first=True) # 2 is bidrectional\n",
    "\n",
    "    #dense layer / linear layer\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
    "\n",
    "    #activation function\n",
    "    self.act = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, txt, txt_len):\n",
    "    '''\n",
    "    # txt [batch_size, seq_len] \n",
    "    ~ seq_len is max sequence length among all the rows in batch\n",
    "    ~ it means the rows length with less than seq_len will be padded with zeros\n",
    "    ~ but the padding will be batchwise\n",
    "    # txt_len [batch_size]\n",
    "    ~ contains sequence length for each row in batch\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Step 1: pass through the embedding layer to convert text into vectors\n",
    "    '''\n",
    "    # embed_txt ~ [batch_size, seq_len, embedding_dim] \n",
    "    embed_txt = self.embedding(txt)\n",
    "\n",
    "    '''\n",
    "    Step 2: passing the embeddings through LSTM layer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Step 2.1: first packing the embeddings to tackle variable length input\n",
    "    For pytorch to know how to pack and unpack properly, \n",
    "    we feed in the length of the original sentence (before padding).\n",
    "    by default enforce_sorted=True, \n",
    "    which requires input sorted by decreasing length, just make sure the target y are also sorted accordingly. \n",
    "    '''\n",
    "    # packed the embedding (only the vocab words without padding)\n",
    "    embed_txt_packed_pad = nn.utils.rnn.pack_padded_sequence(embed_txt, txt_len, batch_first=True)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Step 2.2: passing the packed input to LSTM layer\n",
    "    '''\n",
    "    # LSTM block \n",
    "    lstm_out, (h_n, c_n) = self.lstm(embed_txt_packed_pad)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Step 2.3: retrieving back the lstm output with zero padding\n",
    "    '''\n",
    "    # packed the embedding (with padding)\n",
    "    embed_txt_pad_packed, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "    \n",
    "    # embed_txt_pad_packed ~ [batch_size, seq_len, embedding_dim] \n",
    "\n",
    "\n",
    "    '''\n",
    "    Step 3: passing the lstm output to Attention layer to get weighted output sequence\n",
    "    '''\n",
    "    # attn_out ~ [batch_size, (2 * hidden_dim)]\n",
    "    attn_out, _ = self.attn(embed_txt_pad_packed, lengths, mask=True)\n",
    "\n",
    "    '''\n",
    "    Step 4: sum all the weighted hidden states (modified by attention)\n",
    "    '''\n",
    "    # [batch_size, feature_dim]\n",
    "    weighted_sum_ip = attn_out.sum(dim=1)\n",
    "\n",
    "    '''\n",
    "    Step 5: feeding the weighted value to a linear layer\n",
    "    '''\n",
    "    # fc_out ~ [batch_size, output_dim]\n",
    "    fc_out = self.fc(weighted_sum_ip)\n",
    "\n",
    "    '''\n",
    "    Step 6: feeding the linear output to activation function\n",
    "    '''\n",
    "    # out ~ [batch_size, output_dim]\n",
    "    out = self.act(fc_out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWLNPJT3AOQL"
   },
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "#instantiate the model\n",
    "train_model = Classifier(vocab_size)\n",
    "train_model = train_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTt99WI1ARZx"
   },
   "outputs": [],
   "source": [
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXUIal2sWjem"
   },
   "outputs": [],
   "source": [
    "def valid_model(valid_iterator, train_model, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for valid_batch in valid_iterator:\n",
    "      \n",
    "      #retrieve text and no. of words\n",
    "      text, text_lengths = valid_batch.text\n",
    "          \n",
    "      #get prediction\n",
    "      predictions = train_model(text, text_lengths)\n",
    "      preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "\n",
    "      #compute the loss\n",
    "      loss = criterion(preds, valid_batch.label)\n",
    "\n",
    "      #compute the binary accuracy\n",
    "      acc = binary_accuracy(preds, valid_batch.label)\n",
    "\n",
    "      # compute loss and accuracy\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  valid_epoc_loss = epoch_loss / len(valid_iterator)\n",
    "  valid_epoch_acc = epoch_acc / len(valid_iterator)\n",
    "\n",
    "  return valid_epoc_loss, valid_epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56663,
     "status": "ok",
     "timestamp": 1651187577466,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "QZhRwRmFAWFE",
    "outputId": "432cf88e-97d3-41d9-c507-2824334a1f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:-  0\n",
      "training===>  loss:-  0.6997859563146319   accuracy:-  0.42857143921511515\n",
      "validation===>  loss:-  0.6864934961001078   accuracy:-  0.6000000238418579\n",
      "epoch:-  2\n",
      "training===>  loss:-  0.6483076810836792   accuracy:-  0.7428571517978396\n",
      "validation===>  loss:-  0.6756425102551779   accuracy:-  0.5333333412806193\n",
      "epoch:-  4\n",
      "training===>  loss:-  0.567549433026995   accuracy:-  0.8571428614003318\n",
      "validation===>  loss:-  0.68541419506073   accuracy:-  0.46666667858759564\n",
      "epoch:-  6\n",
      "training===>  loss:-  0.4776928893157414   accuracy:-  0.9142857193946838\n",
      "validation===>  loss:-  0.7253568768501282   accuracy:-  0.5333333512147268\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = optim.Adam(train_model.parameters())\n",
    "\n",
    "#define the loss\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#set the model in training phase\n",
    "train_model.train()\n",
    "\n",
    "N_EPOCHS = 6\n",
    "VALIDATION_EPOCH = 2\n",
    "\n",
    "for epoch in range(N_EPOCHS+1):\n",
    "\n",
    "  #initialize every epoch \n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  for batch in train_iterator:\n",
    "    #resets the gradients after every batch\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    #retrieve text and no. of words\n",
    "    text, text_lengths = batch.text\n",
    "\n",
    "    #get prediction\n",
    "    predictions = train_model(text, text_lengths)\n",
    "    preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "\n",
    "    #compute the loss\n",
    "    loss = criterion(preds, batch.label)\n",
    "\n",
    "    #compute the binary accuracy\n",
    "    acc = binary_accuracy(preds, batch.label)   \n",
    "\n",
    "    #backpropage the loss and compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    optimizer.step() \n",
    "\n",
    "    # compute loss and accuracy\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += acc.item()\n",
    "\n",
    "  if epoch%VALIDATION_EPOCH == 0:\n",
    "    train_model.eval() # set the model in eval phase\n",
    "    valid_epoc_loss, valid_epoch_acc = valid_model(valid_iterator, train_model, criterion)\n",
    "    train_model.train() # return back to training phase\n",
    "\n",
    "    print(\"epoch:- \",epoch)\n",
    "    print(\"training===> \",\"loss:- \", epoch_loss / len(train_iterator), \"  accuracy:- \", epoch_acc / len(train_iterator))\n",
    "    print(\"validation===> \",\"loss:- \", valid_epoc_loss, \"  accuracy:- \", valid_epoch_acc)\n",
    "\n",
    "  if epoch == N_EPOCHS-1:\n",
    "    torch.save(train_model.state_dict(), g_path+\"model/classification_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1651169294728,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "Gyf31XnuCsLt",
    "outputId": "ae43aa4c-3723-4b84-d710-e9d7cf4d1177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5918000936508179\n"
     ]
    }
   ],
   "source": [
    "###  Inference  ###\n",
    "\n",
    "#define hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "#instantiate the model\n",
    "test_model = Classifier(vocab_size)\n",
    "test_model = test_model.to(device)\n",
    "\n",
    "#loading the model\n",
    "model_path = g_path+\"model/classification_model.pt\"\n",
    "test_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_model.eval() # set the model in eval phase\n",
    "\n",
    "\n",
    "test_sentence = \"Are there any sports that you don't like?\"\n",
    "# test_sentence = \"I love the movie\"\n",
    "# test_sentence = \"I dislike the movie\"\n",
    "# test_sentence = \"I don't like the movie\"\n",
    "\n",
    "test_data = \" \".join(clean_data(test_sentence.split(\" \"))) # clean the data\n",
    "tokenized_test_data = tokenizer(test_data)  #tokenize the sentence\n",
    "\n",
    "indexed_test_data = [TEXT.vocab.stoi[t] for t in tokenized_test_data]  #convert to integer sequence\n",
    "txt_tensor = torch.LongTensor(indexed_test_data).to(device) #convert to tensor\n",
    "txt_tensor_ip = txt_tensor.unsqueeze(1).T #reshape in form of batch,no. of words\n",
    "\n",
    "length = [len(indexed_test_data)]  #compute no. of words\n",
    "length_tensor_ip = torch.LongTensor(length) #convert to tensor \n",
    "\n",
    "prediction = test_model(txt_tensor_ip, length_tensor_ip) #prediction\n",
    "\n",
    "print(prediction.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-VzOYvqTwuZ"
   },
   "source": [
    "**Example>>>>**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJR4s49UYWT3"
   },
   "outputs": [],
   "source": [
    "#[batch_size, (seq_len ~ variable), embed_dim]\n",
    "x = [\n",
    "        [\n",
    "         [0.16, 0.57, 0.12, 0.84],\n",
    "         [0.64, 0.28, 0.42, 0.86]\n",
    "        ],\n",
    "\n",
    "        [\n",
    "         [0.20, 0.91, 0.26, 0.16],\n",
    "         [0.75, 0.32, 0.25, 0.75],\n",
    "         [0.15, 0.16, 0.70, 0.48]\n",
    "        ],\n",
    "\n",
    "        [\n",
    "         [0.91, 0.10, 0.74, 0.22],\n",
    "         [0.25, 0.42, 0.29, 0.26],\n",
    "         [0.51, 0.70, 0.12, 0.26]\n",
    "        ],\n",
    "\n",
    "        [\n",
    "         [0.17, 0.91, 0.77, 0.88]\n",
    "        ],\n",
    "\n",
    "        [\n",
    "         [0.35, 0.90, 0.18, 0.46],\n",
    "         [0.44, 0.33, 0.16, 0.43],\n",
    "         [0.10, 0.97, 0.10, 0.70]\n",
    "        ]\n",
    "      ]\n",
    "x_len = torch.Tensor([2, 3, 3, 1,  3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpGBSuAdluRx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "first define the input to the attention\n",
    "#[batch_size, seq_len, embed_dim]\n",
    "batch_size : the number of input sentences at a time\n",
    "seq_len : the number (max) of words among the input sentences\n",
    "embed_dim : the vector dimension for each word (depends on RNN/LSTM hidden_dim)\n",
    "'''\n",
    "\n",
    "batch_size = 5\n",
    "seq_len = 3\n",
    "embed_dim = 4\n",
    "\n",
    "#[batch_size, seq_len, embed_dim]\n",
    "x_padded = torch.Tensor([\n",
    "        [[0.16, 0.57, 0.12, 0.84],\n",
    "         [0.64, 0.28, 0.42, 0.86],\n",
    "         [0.00, 0.00, 0.00, 0.00]],\n",
    "\n",
    "        [[0.20, 0.91, 0.26, 0.16],\n",
    "         [0.75, 0.32, 0.25, 0.75],\n",
    "         [0.15, 0.16, 0.70, 0.48]],\n",
    "\n",
    "        [[0.91, 0.10, 0.74, 0.22],\n",
    "         [0.25, 0.42, 0.29, 0.26],\n",
    "         [0.51, 0.70, 0.12, 0.26]],\n",
    "\n",
    "        [[0.17, 0.91, 0.77, 0.88],\n",
    "         [0.00, 0.00, 0.00, 0.00],\n",
    "         [0.00, 0.00, 0.00, 0.00]],\n",
    "\n",
    "        [[0.35, 0.90, 0.18, 0.46],\n",
    "         [0.44, 0.33, 0.16, 0.43],\n",
    "         [0.10, 0.97, 0.10, 0.70]]\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrQCIeskAElM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's define the weights to be learned \n",
    "these weights will help us to learn attention weights for earch word in each sentence\n",
    "--> these should be randomly initialized at begining\n",
    "--> these should be defined as nn.Parameter so it can be learned by backward propagation \n",
    "'''\n",
    "#[embed_dim, embed_dim]\n",
    "W = torch.Tensor([\n",
    "        [-0.88, 0.09, 0.04,  0.00],\n",
    "        [0.37,  0.00, 0.37,  0.83],\n",
    "        [0.94,  0.37, 0.40,  0.93],\n",
    "        [0.72,  0.28, 0.09,  0.00]])\n",
    "\n",
    "#[embed_dim, 1]\n",
    "u = torch.Tensor([\n",
    "        [0.72],\n",
    "        [0.3],\n",
    "        [0.64],\n",
    "        [0.00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1651089661322,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "QTOjosFbAUXS",
    "outputId": "a9b3a7a8-2778-42f0-8e79-6d14f53781fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first matmul shape:- \n",
      " torch.Size([5, 3, 4])\n",
      "first matmul:- \n",
      " tensor([[[0.7877, 0.2940, 0.3409, 0.5847],\n",
      "         [0.5544, 0.4538, 0.3746, 0.6230],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5203, 0.1590, 0.4631, 0.9971],\n",
      "         [0.2334, 0.3700, 0.3159, 0.4981],\n",
      "         [0.9308, 0.4069, 0.3884, 0.7838]],\n",
      "\n",
      "        [[0.0902, 0.4173, 0.3892, 0.7712],\n",
      "         [0.3952, 0.2026, 0.3048, 0.6183],\n",
      "         [0.1102, 0.1631, 0.3508, 0.6926]],\n",
      "\n",
      "        [[1.5445, 0.5466, 0.7307, 1.4714],\n",
      "         [1.0606, 0.4382, 0.3624, 0.6742],\n",
      "         [0.7397, 0.6355, 0.5902, 1.0730]],\n",
      "\n",
      "        [[0.5254, 0.2269, 0.4604, 0.9144],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "tanh shape:- \n",
      " torch.Size([5, 3, 4])\n",
      "tanh:- \n",
      " tensor([[[0.6571, 0.2858, 0.3283, 0.5261],\n",
      "         [0.5038, 0.4250, 0.3580, 0.5532],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4779, 0.1577, 0.4326, 0.7604],\n",
      "         [0.2293, 0.3540, 0.3058, 0.4606],\n",
      "         [0.7310, 0.3858, 0.3700, 0.6549]],\n",
      "\n",
      "        [[0.0900, 0.3947, 0.3707, 0.6476],\n",
      "         [0.3758, 0.1999, 0.2957, 0.5499],\n",
      "         [0.1098, 0.1617, 0.3371, 0.5996]],\n",
      "\n",
      "        [[0.9129, 0.4980, 0.6235, 0.8998],\n",
      "         [0.7859, 0.4122, 0.3473, 0.5877],\n",
      "         [0.6290, 0.5618, 0.5300, 0.7906]],\n",
      "\n",
      "        [[0.4819, 0.2231, 0.4304, 0.7232],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "second matmul shape:- \n",
      " torch.Size([5, 3, 1])\n",
      "second matmul:- \n",
      " tensor([[[0.7690],\n",
      "         [0.7194],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.6683],\n",
      "         [0.4670],\n",
      "         [0.8788]],\n",
      "\n",
      "        [[0.4204],\n",
      "         [0.5198],\n",
      "         [0.3433]],\n",
      "\n",
      "        [[1.2057],\n",
      "         [0.9118],\n",
      "         [0.9606]],\n",
      "\n",
      "        [[0.6893],\n",
      "         [0.0000],\n",
      "         [0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "#[batch_size, seq_len, embed_dim] \n",
    "tmp_1 = torch.matmul(x_padded,W)\n",
    "print(\"first matmul shape:- \\n\",tmp_1.shape)\n",
    "print(\"first matmul:- \\n\",tmp_1)\n",
    "\n",
    "#[batch_size, seq_len, embed_dim] \n",
    "tmp_2 = torch.tanh(tmp_1)\n",
    "print(\"tanh shape:- \\n\",tmp_2.shape)\n",
    "print(\"tanh:- \\n\",tmp_2)\n",
    "\n",
    "#[batch_size, seq_len, 1] \n",
    "tmp_3 = torch.matmul(tmp_2, u)\n",
    "print(\"second matmul shape:- \\n\",tmp_3.shape)\n",
    "print(\"second matmul:- \\n\",tmp_3)\n",
    "\n",
    "\n",
    "#[batch_size, seq_len, 1]\n",
    "attn = torch.matmul(\n",
    "    torch.tanh( torch.matmul(x_padded, #[batch_size, seq_len, embed_dim] \n",
    "                                W #[embed_dim, embed_dim]\n",
    "                              ) #[batch_size, seq_len]\n",
    "    ),\n",
    "    u #[embed_dim, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1651089724009,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "TrVTBdl2Svx-",
    "outputId": "142df3ff-6531-4c11-dd3f-8e4eb3aa7204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_score shape:- \n",
      " torch.Size([5, 3, 1])\n",
      "attn_score:- \n",
      " tensor([[[0.4141],\n",
      "         [0.3940],\n",
      "         [0.1919]],\n",
      "\n",
      "        [[0.3277],\n",
      "         [0.2679],\n",
      "         [0.4044]],\n",
      "\n",
      "        [[0.3300],\n",
      "         [0.3645],\n",
      "         [0.3055]],\n",
      "\n",
      "        [[0.3956],\n",
      "         [0.2948],\n",
      "         [0.3096]],\n",
      "\n",
      "        [[0.4990],\n",
      "         [0.2505],\n",
      "         [0.2505]]])\n"
     ]
    }
   ],
   "source": [
    "#[batch_size, seq_len, 1]\n",
    "attn_score = torch.softmax(attn, dim=1)\n",
    "print(\"attn_score shape:- \\n\", attn_score.shape)\n",
    "print(\"attn_score:- \\n\", attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1651091238726,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "4HNZqv9WTgUA",
    "outputId": "23ce223c-4bf1-4192-f2d2-7cfa46f989ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_score size:-  torch.Size([5, 3, 1])\n",
      "mask-shape:-  torch.Size([5, 3, 1])\n",
      "mask:- \n",
      " tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "After padding\n",
      "mask:- \n",
      " tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "#maximum sequence length in the batch\n",
    "max_len = max(x_len.data)\n",
    "\n",
    "#[batch_size, seq_len, 1]\n",
    "print(\"attn_score size:- \", attn_score.size())\n",
    "mask = torch.ones(attn_score.size())\n",
    "print(\"mask-shape:- \",mask.shape)\n",
    "print(\"mask:- \\n\",mask)\n",
    "\n",
    "for i, l in enumerate(x_len.data):\n",
    "  idx = int(l.item())\n",
    "  if l < max_len:\n",
    "      mask[i, idx:] = 0\n",
    "\n",
    "print(\"After padding\")\n",
    "print(\"mask:- \\n\",mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1651091365243,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "lxBgVDy9tR4Y",
    "outputId": "1fa7f600-1e46-4b1c-bf86-5c1c8a84822a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MASK\n",
      "attn_score shape:- \n",
      " torch.Size([5, 3, 1])\n",
      "attn_score:- \n",
      " tensor([[[0.4141],\n",
      "         [0.3940],\n",
      "         [0.1919]],\n",
      "\n",
      "        [[0.3277],\n",
      "         [0.2679],\n",
      "         [0.4044]],\n",
      "\n",
      "        [[0.3300],\n",
      "         [0.3645],\n",
      "         [0.3055]],\n",
      "\n",
      "        [[0.3956],\n",
      "         [0.2948],\n",
      "         [0.3096]],\n",
      "\n",
      "        [[0.4990],\n",
      "         [0.2505],\n",
      "         [0.2505]]])\n",
      "AFTER MASK\n",
      "masked_attn_score shape:-  torch.Size([5, 3, 1])\n",
      "masked_attn_score:- \n",
      " tensor([[[0.4141],\n",
      "         [0.3940],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.3277],\n",
      "         [0.2679],\n",
      "         [0.4044]],\n",
      "\n",
      "        [[0.3300],\n",
      "         [0.3645],\n",
      "         [0.3055]],\n",
      "\n",
      "        [[0.3956],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.4990],\n",
      "         [0.2505],\n",
      "         [0.2505]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE MASK\")\n",
    "print(\"attn_score shape:- \\n\", attn_score.shape)\n",
    "print(\"attn_score:- \\n\", attn_score)\n",
    "\n",
    "# apply the mask - zero out masked timesteps\n",
    "masked_attn_score = attn_score * mask\n",
    "\n",
    "print(\"AFTER MASK\")\n",
    "print(\"masked_attn_score shape:- \", masked_attn_score.shape)\n",
    "print(\"masked_attn_score:- \\n\", masked_attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1651091409948,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "DfNzEiqJSwBD",
    "outputId": "70772d4e-2f3b-4fb7-83a9-4d55af96a35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_x shape:- \n",
      " torch.Size([5, 3, 4])\n",
      "weighted_x:- \n",
      " tensor([[[0.0662, 0.2360, 0.0497, 0.3478],\n",
      "         [0.2522, 0.1103, 0.1655, 0.3389],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0655, 0.2982, 0.0852, 0.0524],\n",
      "         [0.2009, 0.0857, 0.0670, 0.2009],\n",
      "         [0.0607, 0.0647, 0.2831, 0.1941]],\n",
      "\n",
      "        [[0.3003, 0.0330, 0.2442, 0.0726],\n",
      "         [0.0911, 0.1531, 0.1057, 0.0948],\n",
      "         [0.1558, 0.2139, 0.0367, 0.0794]],\n",
      "\n",
      "        [[0.0672, 0.3600, 0.3046, 0.3481],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1747, 0.4491, 0.0898, 0.2296],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "#[batch_size, seq_len, embed_dim] \n",
    "weighted_x = x_padded * masked_attn_score\n",
    "print(\"weighted_x shape:- \\n\", weighted_x.shape)\n",
    "print(\"weighted_x:- \\n\", weighted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1651091421068,
     "user": {
      "displayName": "Jaydeep Chakraborty",
      "userId": "04716741579620071152"
     },
     "user_tz": 300
    },
    "id": "GglJu9aXSwLO",
    "outputId": "e8150fea-818d-4705-ebc5-b378ed705724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_sum_x shape:- \n",
      " torch.Size([5, 4])\n",
      "weighted_sum_x:- \n",
      " tensor([[0.3184, 0.3463, 0.2152, 0.6867],\n",
      "        [0.3271, 0.4486, 0.4353, 0.4475],\n",
      "        [0.5472, 0.3999, 0.3866, 0.2468],\n",
      "        [0.0672, 0.3600, 0.3046, 0.3481],\n",
      "        [0.1747, 0.4491, 0.0898, 0.2296]])\n"
     ]
    }
   ],
   "source": [
    "#[batch_size, embed_dim] \n",
    "weighted_sum_x = torch.sum(weighted_x, dim=1)\n",
    "print(\"weighted_sum_x shape:- \\n\", weighted_sum_x.shape)\n",
    "print(\"weighted_sum_x:- \\n\", weighted_sum_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEE9oinZT3vn"
   },
   "source": [
    "**Resources**\n",
    "\n",
    "---\n",
    "**Link** <br>\n",
    "1) https://www.kaggle.com/code/dannykliu/lstm-with-attention-clr-in-pytorch/notebook <br>\n",
    "2) https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/9 <br>\n",
    "3) https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/ <br>\n",
    "4) https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ <br>\n",
    "5) https://www.kaggle.com/code/robertke94/pytorch-bi-lstm-attention/notebook <br>\n",
    "6) https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a <br>\n",
    "7) https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc <br>\n",
    "8) https://www.kaggle.com/code/pavelvod/transformer-cnn-lstm-attention-heads <br>\n",
    "9) https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ <br>\n",
    "10) https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/ <br>\n",
    "11) https://colab.research.google.com/drive/1HmegzNQR6g5_Xt37BMgV0kX7wQklT7dD?usp=sharing#scrollTo=Vh9bXvzHkmfi <br>\n",
    "12) https://github.com/prakashpandey9/Text-Classification-Pytorch <br>\n",
    "13) https://www.programmerall.com/article/51852224642/ <br>\n",
    "14) https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/ <br>\n",
    "15) https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/ <br>\n",
    "16) https://lilianweng.github.io/posts/2018-06-24-attention/ <br>\n",
    "17) https://github.com/WHLYA/text-classification/blob/master/text%20classification/LSTM%2BAttention.ipynb <br>\n",
    "\n",
    "**Video** <br>\n",
    "1) https://www.youtube.com/watch?v=Bp-_DatyUCY <br>\n",
    "2) https://www.youtube.com/watch?v=oaV_Fv5DwUM <br>\n",
    "3) https://www.youtube.com/watch?v=KmAISyVvE1Y <br>\n",
    "4) https://www.youtube.com/watch?v=oUhGZMCTHtI <br>\n",
    "5) https://www.youtube.com/watch?v=MN__lSncZBs <br>\n",
    "6) https://www.coursera.org/lecture/nlp-sequence-models/attention-model-intuition-RDXpX\n",
    "\n",
    "**Paper** <br>\n",
    "1) https://arxiv.org/pdf/1904.02874.pdf <br>\n",
    "2) https://arxiv.org/pdf/1409.0473.pdf <br>\n",
    "3) https://arxiv.org/pdf/1804.06659.pdf <br>\n",
    "4) https://arxiv.org/pdf/1703.03130.pdf <br>\n",
    "5) https://arxiv.org/ftp/arxiv/papers/1902/1902.02181.pdf <br>\n",
    "6) https://mdpi-res.com/d_attachment/applsci/applsci-11-03883/article_deploy/applsci-11-03883.pdf?version=1619361335 <br>\n",
    "7) https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf <br>\n",
    "8) https://colinraffel.com/publications/iclr2016feed.pdf <br>\n",
    "9) http://univagora.ro/jour/index.php/ijccc/article/download/3142/1185/ "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMjJnTKiCqLWBm0+KXSa4F6",
   "collapsed_sections": [],
   "name": "RNN-Attention-PAD-Mask((seq) -> 1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
