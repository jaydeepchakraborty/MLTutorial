{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN-PAD((seq) -> 1).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPscUqeKBV+JqNvSNxzS/VH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -U torch==1.10.0 torchtext==0.11.0\n","\n","# Reload environment\n","exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"Gu7-OXFezzEf","executionInfo":{"status":"ok","timestamp":1651160183284,"user_tz":300,"elapsed":109704,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"aa9bba4b-1dbe-466f-c092-4ae4e4dbf9e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.8.0\n","  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n","\u001b[?25hCollecting torchtext==0.9.0\n","  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n","\u001b[K     |████████████████████████████████| 7.1 MB 48.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.12.0\n","    Uninstalling torchtext-0.12.0:\n","      Successfully uninstalled torchtext-0.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.0 torchtext-0.9.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchtext"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzNsZnK7UWyr"},"outputs":[],"source":["import random\n","import re\n","import pandas as pd\n","import spacy\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torchtext.legacy import data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","g_path = \"/content/drive/My Drive/pytorch/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdcYIgPIUald","executionInfo":{"status":"ok","timestamp":1651160246418,"user_tz":300,"elapsed":13919,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"121f55ca-d1a8-4039-c00e-dacbafc4ae42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_fl = 'data/IMDB_review_sentiment_small.csv'"],"metadata":{"id":"FZrVtJ2xhaSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#reproducing the same result\n","SEED = 2021\n","torch.manual_seed(SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRHqfw6sVSZZ","executionInfo":{"status":"ok","timestamp":1651160254407,"user_tz":300,"elapsed":129,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"8069c05e-455f-457b-a284-1c17cc60452f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7fcac6c2f0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["spacy_en = spacy.load('en')\n","def clean_data(texts):\n","    cleaned_text = []\n","    for text in texts:\n","        # remove break\n","        text = text.replace('br', '')\n","        # remove punctuation\n","        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n","        # remove multiple spaces\n","        text = re.sub(r' +', ' ', text)\n","        # remove newline\n","        text = re.sub(r'\\n', ' ', text)\n","        # strip the text\n","        text = text.strip()\n","        # lower the text\n","        text = text.lower()\n","\n","        if text != '':\n","          cleaned_text.append(text)\n","    return cleaned_text\n","\n","def tokenizer(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","TEXT = data.Field(preprocessing=clean_data,tokenize=tokenizer,batch_first=True,include_lengths=True)\n","LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n","fields = [('text',TEXT),('label', LABEL)]"],"metadata":{"id":"47Us8mAaVWFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loading the entire data\n","def load_data():\n","  imdb_data = data.TabularDataset(path = g_path+data_fl,format = 'csv', fields = fields, skip_header = True)\n","  return imdb_data\n","\n","imdb_data = load_data() \n","print(vars(imdb_data.examples[0]))\n","print(imdb_data.examples[0].text, imdb_data.examples[0].label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qTpaDVpVZQu","executionInfo":{"status":"ok","timestamp":1651160262673,"user_tz":300,"elapsed":933,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"7b48fca2-0bcb-4ffb-cc1f-9d3005b962bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'], 'label': '1'}\n","['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'] 1\n"]}]},{"cell_type":"code","source":["#splitting the data into training and validation dataset\n","def split_data(imdb_data):\n","  train_data, valid_data = imdb_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","  return train_data, valid_data\n","\n","train_data, valid_data = split_data(imdb_data)"],"metadata":{"id":"r42POVJ6fwT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#generate vocabulary\n","TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n","LABEL.build_vocab(train_data)\n","\n","#No. of unique tokens in text\n","print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n","#No. of unique tokens in label\n","print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNwDnzrGkea5","executionInfo":{"status":"ok","timestamp":1651160469566,"user_tz":300,"elapsed":201803,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"8bc78114-e9ef-491a-875d-cebfbbd9f996"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n","100%|█████████▉| 399999/400000 [00:16<00:00, 24229.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Size of TEXT vocabulary: 466\n","Size of LABEL vocabulary: 2\n"]}]},{"cell_type":"code","source":["#preparing batches for training the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","#set batch size\n","BATCH_SIZE = 5\n","\n","#Load an iterator\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data), \n","    batch_size = BATCH_SIZE,\n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True,\n","    device = device)"],"metadata":{"id":"ApKB9FJ7-IoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Classifier(nn.Module):\n","\n","  def __init__(self, vocab_size, **kwargs):\n","    #Constructor\n","    super(Classifier, self).__init__(**kwargs)\n","\n","    # variables\n","    self.embedding_dim = 100\n","    self.hidden_dim = 32\n","    self.num_layers = 1\n","    self.bidirectional = True\n","    self.batch_first = True\n","    self.output_dim = 1\n","\n","    #embedding layer\n","    self.embedding = nn.Embedding(num_embeddings=vocab_size, \n","                                  embedding_dim=self.embedding_dim)\n","\n","    #lstm layer\n","    self.lstm = nn.LSTM(input_size=self.embedding_dim,\n","                        hidden_size=self.hidden_dim, \n","                        num_layers=self.num_layers, \n","                        bidirectional=self.bidirectional,\n","                        batch_first=self.batch_first)\n","\n","    #dense layer / linear layer\n","    self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n","\n","    #activation function\n","    self.act = nn.Sigmoid()\n","\n","  def forward(self, txt, txt_len):\n","    '''\n","    # txt [batch_size, seq_len] \n","    ~ seq_len is max sequence length among all the rows in batch\n","    ~ it means the rows length with less than seq_len will be padded \n","    ~ but the padding will be batchwise\n","    # txt_len [batch_size]\n","    ~ contains sequence length for each row in batch\n","    '''\n","    \n","    '''\n","    Step 1: pass through the embedding layer to convert text into vectors\n","    '''\n","    # embed_txt ~ [batch_size, seq_len, embedding_dim] \n","    embed_txt = self.embedding(txt)\n","\n","    '''\n","    Step 2: passing the embeddings through LSTM layer\n","    '''\n","\n","    '''\n","    Step 2.1: first packing the embeddings to tackle variable length input\n","    For pytorch to know how to pack and unpack properly, \n","    we feed in the length of the original sentence (before padding).\n","    by default enforce_sorted=True, \n","    which requires input sorted by decreasing length, just make sure the target y are also sorted accordingly. \n","    '''\n","    # packed the embedding (only the vocab words without padding)\n","    embed_txt_packed_pad = nn.utils.rnn.pack_padded_sequence(embed_txt, txt_len, batch_first=True)\n","\n","    '''\n","    Step 2.2: passing the packed input to LSTM layer\n","    '''\n","    # lstm_out ~ [batch_size, seq_len, (2 * hidden_dim)] \n","    lstm_out, (h_n, c_n) = self.lstm(embed_txt_packed_pad)\n","\n","    '''\n","    Step 2.3: retrieving back the lstm output with zero padding\n","    '''\n","    # packed the embedding (with padding)\n","    embed_txt_pad_packed, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n","\n","    '''\n","    Step 3: sum all the hidden states\n","    '''\n","    # lstm_out ~ [include dimention, remove dimention, include dimention] \n","    # concat_out ~ [batch_size, (2 * hidden_dim)] #concatenate hidden states\n","    # concat_out = embed_txt_pad_packed[ : , -1, : ]  #concatenate hidden states\n","    sum_ip = embed_txt_pad_packed.sum(dim=1)  #summing up hidden states\n","    # avg_ip = embed_txt_pad_packed.mean(dim=1)  #averaging the hidden states\n","\n","    '''\n","    Step 4: feeding the weighted value to a linear layer\n","    '''\n","    # fc_out ~ [batch_size, output_dim]\n","    fc_out = self.fc(sum_ip)\n","\n","    '''\n","    Step 5: feeding the linear output to activation function\n","    '''\n","    # out ~ [batch_size, output_dim]\n","    out = self.act(fc_out)\n","\n","    return out"],"metadata":{"id":"I8v8UZ2GNykd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define hyperparameters\n","vocab_size = len(TEXT.vocab)\n","\n","#instantiate the model\n","train_model = Classifier(vocab_size)\n","train_model = train_model.to(device)"],"metadata":{"id":"hWLNPJT3AOQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(preds)\n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc"],"metadata":{"id":"pTt99WI1ARZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_model(valid_iterator, train_model, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  with torch.no_grad():\n","    for valid_batch in valid_iterator:\n","      \n","      #retrieve text and no. of words\n","      text, text_lengths = valid_batch.text\n","          \n","      #get prediction\n","      predictions = train_model(text, text_lengths)\n","      preds = predictions.squeeze(-1) #convert to 1D tensor\n","\n","      #compute the loss\n","      loss = criterion(preds, valid_batch.label)\n","\n","      #compute the binary accuracy\n","      acc = binary_accuracy(preds, valid_batch.label)\n","\n","      # compute loss and accuracy\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  valid_epoc_loss = epoch_loss / len(valid_iterator)\n","  valid_epoch_acc = epoch_acc / len(valid_iterator)\n","\n","  return valid_epoc_loss, valid_epoch_acc"],"metadata":{"id":"eFUDTGoZxl3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training the model\n","\n","#define the optimizer\n","optimizer = optim.Adam(train_model.parameters())\n","\n","#define the loss\n","criterion = nn.BCELoss()\n","criterion = criterion.to(device)\n","\n","#set the model in training phase\n","train_model.train()\n","\n","N_EPOCHS = 6\n","VALIDATION_EPOCH = 2\n","\n","for epoch in range(N_EPOCHS+1):\n","\n","  #initialize every epoch \n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  for batch in train_iterator:\n","    #resets the gradients after every batch\n","    optimizer.zero_grad() \n","\n","    #retrieve text and no. of words\n","    text, text_lengths = batch.text\n","\n","    #get prediction\n","    predictions = train_model(text, text_lengths)\n","    preds = predictions.squeeze(-1) #convert to 1D tensor\n","\n","    #compute the loss\n","    loss = criterion(preds, batch.label)\n","\n","    #compute the binary accuracy\n","    acc = binary_accuracy(preds, batch.label)   \n","\n","    #backpropage the loss and compute the gradients\n","    loss.backward()\n","\n","    #update the weights\n","    optimizer.step() \n","\n","    # compute loss and accuracy\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  if epoch%VALIDATION_EPOCH == 0:\n","    train_model.eval() # set the model in eval phase\n","    valid_epoc_loss, valid_epoch_acc = valid_model(valid_iterator, train_model, criterion)\n","    train_model.train() # return back to training phase\n","\n","    print(\"epoch:- \",epoch)\n","    print(\"training===> \",\"loss:- \", epoch_loss / len(train_iterator), \"  accuracy:- \", epoch_acc / len(train_iterator))\n","    print(\"validation===> \",\"loss:- \", valid_epoc_loss, \"  accuracy:- \", valid_epoch_acc)\n","\n","  if epoch == N_EPOCHS-1:\n","    torch.save(train_model.state_dict(), g_path+\"model/classification_model.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZhRwRmFAWFE","executionInfo":{"status":"ok","timestamp":1651163976969,"user_tz":300,"elapsed":97979,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"0148623a-65a0-4d7b-a8d2-375d42607857"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:-  0\n","training===>  loss:-  24.26220291001456   accuracy:-  0.4571428724697658\n","validation===>  loss:-  2.331149458885193   accuracy:-  0.5333333512147268\n","epoch:-  2\n","training===>  loss:-  2.5758355855941772   accuracy:-  0.5142857219491687\n","validation===>  loss:-  3.7094129721323648   accuracy:-  0.46666667858759564\n","epoch:-  4\n","training===>  loss:-  1.0857821806733097   accuracy:-  0.6857142938034875\n","validation===>  loss:-  10.070106188456217   accuracy:-  0.3333333383003871\n","epoch:-  6\n","training===>  loss:-  0.5492339253365311   accuracy:-  0.8000000076634544\n","validation===>  loss:-  2.233311414718628   accuracy:-  0.46666667858759564\n"]}]},{"cell_type":"code","source":["###  Inference  ###\n","\n","#define hyperparameters\n","vocab_size = len(TEXT.vocab)\n","\n","#instantiate the model\n","test_model = Classifier(vocab_size)\n","test_model = test_model.to(device)\n","\n","#loading the model\n","model_path = g_path+\"model/classification_model.pt\"\n","test_model.load_state_dict(torch.load(model_path))\n","\n","test_model.eval() # set the model in eval phase\n","\n","\n","test_sentence = \"Are there any sports that you don't like?\"\n","test_sentence = \"I love the movie\"\n","test_sentence = \"I dislike the movie\"\n","test_sentence = \"I don't like the movie\"\n","\n","test_data = \" \".join(clean_data(test_sentence.split(\" \"))) # clean the data\n","tokenized_test_data = tokenizer(test_data)  #tokenize the sentence\n","\n","indexed_test_data = [TEXT.vocab.stoi[t] for t in tokenized_test_data]  #convert to integer sequence\n","txt_tensor = torch.LongTensor(indexed_test_data).to(device) #convert to tensor\n","txt_tensor_ip = txt_tensor.unsqueeze(1).T #reshape in form of batch,no. of words\n","\n","length = [len(indexed_test_data)]  #compute no. of words\n","length_tensor_ip = torch.LongTensor(length) #convert to tensor \n","\n","prediction = test_model(txt_tensor_ip, length_tensor_ip) #prediction\n","\n","print(prediction.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gyf31XnuCsLt","executionInfo":{"status":"ok","timestamp":1651163991021,"user_tz":300,"elapsed":146,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"2b2e56fa-e6fa-4f95-a6af-651cdc9a2c32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5662572383880615\n"]}]},{"cell_type":"markdown","source":["**Example>>>>**\n","\n","---"],"metadata":{"id":"lkm2I5wp_Zhk"}},{"cell_type":"code","source":["#[batch_size, (seq_len ~ variable), embed_dim]\n","x = [\n","        [\n","         [0.16, 0.57, 0.12, 0.84],\n","         [0.64, 0.28, 0.42, 0.86]\n","        ],\n","\n","        [\n","         [0.20, 0.91, 0.26, 0.16],\n","         [0.75, 0.32, 0.25, 0.75],\n","         [0.15, 0.16, 0.70, 0.48]\n","        ],\n","\n","        [\n","         [0.91, 0.10, 0.74, 0.22],\n","         [0.25, 0.42, 0.29, 0.26],\n","         [0.51, 0.70, 0.12, 0.26]\n","        ],\n","\n","        [\n","         [0.17, 0.91, 0.77, 0.88]\n","        ],\n","\n","        [\n","         [0.35, 0.90, 0.18, 0.46],\n","         [0.44, 0.33, 0.16, 0.43],\n","         [0.10, 0.97, 0.10, 0.70]\n","        ]\n","      ]\n","x_len = torch.Tensor([2, 3, 3, 1,  3])"],"metadata":{"id":"4CjJ3Lws_bXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","first define the input to the attention\n","#[batch_size, seq_len, embed_dim]\n","batch_size : the number of input sentences at a time\n","seq_len : the number (max) of words among the input sentences\n","embed_dim : the vector dimension for each word (depends on RNN/LSTM hidden_dim)\n","'''\n","\n","batch_size = 5\n","seq_len = 3\n","embed_dim = 4\n","\n","#[batch_size, seq_len, embed_dim]\n","x_padded = torch.Tensor([\n","        [[0.16, 0.57, 0.12, 0.84],\n","         [0.64, 0.28, 0.42, 0.86],\n","         [0.00, 0.00, 0.00, 0.00]],\n","\n","        [[0.20, 0.91, 0.26, 0.16],\n","         [0.75, 0.32, 0.25, 0.75],\n","         [0.15, 0.16, 0.70, 0.48]],\n","\n","        [[0.91, 0.10, 0.74, 0.22],\n","         [0.25, 0.42, 0.29, 0.26],\n","         [0.51, 0.70, 0.12, 0.26]],\n","\n","        [[0.17, 0.91, 0.77, 0.88],\n","         [0.00, 0.00, 0.00, 0.00],\n","         [0.00, 0.00, 0.00, 0.00]],\n","\n","        [[0.35, 0.90, 0.18, 0.46],\n","         [0.44, 0.33, 0.16, 0.43],\n","         [0.10, 0.97, 0.10, 0.70]]\n","      ])"],"metadata":{"id":"JJXlEFOJ_gd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#[batch_size, embed_dim] \n","weighted_sum_x = torch.sum(x_padded, dim=1)\n","print(\"weighted_sum_x shape:- \\n\", weighted_sum_x.shape)\n","print(\"weighted_sum_x:- \\n\", weighted_sum_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyKSlsnR_ohf","executionInfo":{"status":"ok","timestamp":1651163183431,"user_tz":300,"elapsed":115,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"ac39e48d-9a97-4baa-8086-174ba020a752"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["weighted_sum_x shape:- \n"," torch.Size([5, 4])\n","weighted_sum_x:- \n"," tensor([[0.8000, 0.8500, 0.5400, 1.7000],\n","        [1.1000, 1.3900, 1.2100, 1.3900],\n","        [1.6700, 1.2200, 1.1500, 0.7400],\n","        [0.1700, 0.9100, 0.7700, 0.8800],\n","        [0.8900, 2.2000, 0.4400, 1.5900]])\n"]}]}]}