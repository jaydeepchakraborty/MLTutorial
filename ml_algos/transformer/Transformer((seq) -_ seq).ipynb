{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer((seq) -> seq).ipynb","provenance":[],"collapsed_sections":["OilFOBhFSsc1"],"authorship_tag":"ABX9TyNWk7PnyJEJLIrB4txeLCu1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":643},"id":"QJ64erD3OAhg","executionInfo":{"status":"ok","timestamp":1653955716714,"user_tz":300,"elapsed":131976,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"065b7c82-24a8-4c00-d71e-5d8090956abe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.10.0\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x65890000 @  0x7f9c84e8a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 881.9 MB 17 kB/s \n","\u001b[?25hCollecting torchtext==0.11.0\n","  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n","\u001b[K     |████████████████████████████████| 8.0 MB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.12.0\n","    Uninstalling torchtext-0.12.0:\n","      Successfully uninstalled torchtext-0.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0 torchtext-0.11.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchtext"]}}},"metadata":{}}],"source":["!pip install -U torch==1.10.0 torchtext==0.11.0\n","\n","# Reload environment\n","exit()"]},{"cell_type":"code","source":["import random\n","import re\n","import math\n","import pandas as pd\n","import spacy\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch import Tensor\n","from torchtext.legacy import data"],"metadata":{"id":"0BY_upUxOKjl","executionInfo":{"status":"ok","timestamp":1653955732693,"user_tz":300,"elapsed":1890,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","g_path = \"/content/drive/My Drive/\"\n","data_path = g_path + \"ml-data/\"\n","code_path = g_path + \"pytorch-nlp/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGTTHxHDOvbH","executionInfo":{"status":"ok","timestamp":1653869531275,"user_tz":300,"elapsed":21871,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"374a72fc-47ee-496b-d392-2979b8f6fb1b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_fl = 'data/eng_ben_small.csv'"],"metadata":{"id":"ZTAwhMMbO1x_","executionInfo":{"status":"ok","timestamp":1653838845817,"user_tz":300,"elapsed":3,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#reproducing the same result\n","SEED = 2021\n","torch.manual_seed(SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lu8dDOTyO9_S","executionInfo":{"status":"ok","timestamp":1653838850292,"user_tz":300,"elapsed":120,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"a7217e67-d445-4c92-cfe2-822a25fed9b3"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fa987c1d6b0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def tokenizer(text):\n","    return [tok for tok in text.split()]\n","\n","ENG_TEXT = data.Field(tokenize=tokenizer,batch_first=True,include_lengths=True)\n","BEN_TEXT = data.Field(tokenize=tokenizer,batch_first=True,include_lengths=True)\n","fields = [('eng',ENG_TEXT),('ben', BEN_TEXT)]"],"metadata":{"id":"DMwiY1xJPVNA","executionInfo":{"status":"ok","timestamp":1653784720558,"user_tz":300,"elapsed":1407,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#loading the entire data\n","def load_data():\n","  eng_ben_data = data.TabularDataset(path = data_path+data_fl,format = 'csv', fields = fields, skip_header = True)\n","  return eng_ben_data\n","\n","eng_ben_data = load_data() \n","print(vars(eng_ben_data.examples[0]))\n","print(eng_ben_data.examples[0].eng, eng_ben_data.examples[0].ben)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7mI6hc-PobR","executionInfo":{"status":"ok","timestamp":1653784722618,"user_tz":300,"elapsed":109,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"1b3ad27d-844d-46ec-a8bb-34e31bc5e07b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'eng': ['Go.'], 'ben': ['যাও।']}\n","['Go.'] ['যাও।']\n"]}]},{"cell_type":"code","source":["#splitting the data into training and validation dataset\n","def split_data(eng_ben_data):\n","  train_data, valid_data = eng_ben_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","  return train_data, valid_data\n","\n","train_data, valid_data = split_data(eng_ben_data)"],"metadata":{"id":"MHVEuh7OQJ-u","executionInfo":{"status":"ok","timestamp":1653784759869,"user_tz":300,"elapsed":126,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#generate vocabulary\n","ENG_TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n","BEN_TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\") \n","\n","#No. of unique tokens in text\n","print(\"Size of ENG_TEXT vocabulary:\",len(ENG_TEXT.vocab))\n","#No. of unique tokens in label\n","print(\"Size of BEN_TEXT vocabulary:\",len(BEN_TEXT.vocab))\n","\n","print(\"unk:- \", ENG_TEXT.vocab.stoi[\"<unk>\"])\n","print(\"pad:- \", ENG_TEXT.vocab.stoi[\"<pad>\"])\n","print(\"sos:- \", ENG_TEXT.vocab.stoi[\"<sos>\"]) #not present in dictionary\n","print(\"The first word in vocab is \", ENG_TEXT.vocab.itos[0])\n","print(\"The second word in vocab is \", ENG_TEXT.vocab.itos[1])\n","print(\"The third word in vocab is \", ENG_TEXT.vocab.itos[2])\n","print(\"The last word in vocab is \", ENG_TEXT.vocab.itos[len(ENG_TEXT.vocab)-1])\n","\n","\n","print(\"unk:- \", BEN_TEXT.vocab.stoi[\"<unk>\"])\n","print(\"pad:- \", BEN_TEXT.vocab.stoi[\"<pad>\"])\n","print(\"sos:- \", BEN_TEXT.vocab.stoi[\"<sos>\"]) #not present in dictionary\n","print(\"The first word in vocab is \", BEN_TEXT.vocab.itos[0])\n","print(\"The second word in vocab is \", BEN_TEXT.vocab.itos[1])\n","print(\"The third word in vocab is \", BEN_TEXT.vocab.itos[2])\n","print(\"The last word in vocab is \", BEN_TEXT.vocab.itos[len(BEN_TEXT.vocab)-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZE_0en-RQRPR","executionInfo":{"status":"ok","timestamp":1653785104840,"user_tz":300,"elapsed":1327,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"a00c456e-d6a2-4ff2-b683-d7159b829fd1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of ENG_TEXT vocabulary: 168\n","Size of BEN_TEXT vocabulary: 162\n","unk:-  0\n","pad:-  1\n","sos:-  0\n","The first word in vocab is  <unk>\n","The second word in vocab is  <pad>\n","The third word in vocab is  I\n","The last word in vocab is  won't\n","unk:-  0\n","pad:-  1\n","sos:-  0\n","The first word in vocab is  <unk>\n","The second word in vocab is  <pad>\n","The third word in vocab is  আমি\n","The last word in vocab is  হচ্ছে।\n"]}]},{"cell_type":"code","source":["#preparing batches for training the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","#set batch size\n","BATCH_SIZE = 5\n","\n","#Load an iterator\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data), \n","    batch_size = BATCH_SIZE,\n","    sort=True, # Sort all examples in data using `sort_key`.\n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True, # Use `sort_key` to sort examples in each batch.\n","    device = device)"],"metadata":{"id":"Qb2R26c9RpEC","executionInfo":{"status":"ok","timestamp":1653785161068,"user_tz":300,"elapsed":249,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Encoder Model"],"metadata":{"id":"YqjJs0ulSTvd"}},{"cell_type":"code","source":["class MulHeadEncAttention(nn.Module):\n","\n","    def __init__(self, name: str = '', conf: dict = {},):\n","        super(MulHeadEncAttention, self).__init__()\n","\n","        self.embed_dim = conf['trans_enc'][\"embed_dim\"]\n","        self.heads = conf['trans_enc'][\"heads\"] #number of heads\n","        self.head_dim = conf['trans_enc'][\"embed_dim\"] // conf['trans_enc'][\"heads\"]\n","\n","        assert (\n","            self.head_dim * self.heads == conf['trans_enc'][\"embed_dim\"]\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","\n","    def forward(self, value, key, query, mask):\n","        \n","        '''\n","        value ~ # [batch, seq_len, embed_dim]\n","        key ~ # [batch, seq_len, embed_dim]\n","        query ~ # [batch, seq_len, embed_dim]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        # Get number of training examples\n","        batch = query.shape[0]\n","\n","        '''\n","        value_len ~ the max seq_len in values \n","        key_len ~ the max seq_len in keys \n","        query_len ~ the max seq_len in query \n","        here all of them are same\n","        '''\n","        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, value_len, heads, head_dim]\n","        value = value.reshape(batch, value_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, key_len, heads, head_dim]\n","        key = key.reshape(batch, key_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, query_len, heads, head_dim]\n","        query = query.reshape(batch, query_len, self.heads, self.head_dim)\n","\n","        # [batch, value_len, heads, head_dim]\n","        values = self.values(value  # [batch, value_len, heads, head_dim]\n","                             )\n","        # [batch, key_len, heads, head_dim]\n","        keys = self.keys(key  # [batch, key_len, heads, head_dim]\n","                         )\n","        # [batch, query_len, heads, head_dim]\n","        queries = self.queries(query  # [batch, query_len, heads, head_dim]\n","                               )\n","        \n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","        # [batch, head, query_len, key_len] \n","        # ~ we can say, query_len is our target and \n","        # key_len is our source and \n","        # energy is how much attention to pay on each word in key to predict query\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", \n","                              [queries, # [batch, query_len, heads, head_dim]\n","                               keys # [batch, key_len, heads, head_dim]\n","                               ])\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","          energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","            # energy = energy.masked_fill(mask == 0, float(\"0\"))\n","\n","        # print(\"ENC energy:- \\n\",energy/ (self.embed_dim ** (1 / 2)))\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability ~ sqrt(d_k)\n","        # attention shape: [batch, heads, query_len, key_len]\n","        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n","\n","        print(\"ENC attention:- \\n\",attention)\n","        '''\n","        here key_len and value_len are same ~ denoted by l \n","        '''\n","        # [batch, query_len, heads, head_dim]\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,  # [batch, heads, query_len, key_len]\n","                                               values  # [batch, value_len, heads, head_dim]\n","                                               ])\n","\n","        print(\"ENC values:- \\n\",values)\n","        print(\"ENC out:- \\n\",out)\n","\n","        if mask is not None:\n","          out = out.masked_fill(values == 0, float(\"0\"))\n","\n","        # print(\"ENC after out:- \\n\", out)\n","        # [batch, query_len, heads, head_dim] --converted--> [batch, query_len, embed_dim]\n","        out = out.reshape(batch, query_len, self.heads * self.head_dim)\n","\n","        return out"],"metadata":{"id":"RvQ2lIXsTmwJ","executionInfo":{"status":"ok","timestamp":1653955748254,"user_tz":300,"elapsed":326,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class TransformerEncBlock(nn.Module):\n","    def __init__(self, name: str = '', conf: dict = {},):\n","        super(TransformerEncBlock, self).__init__()\n","\n","        self.attention = MulHeadEncAttention(name='MulHeadEncAttention', conf=conf)\n","        \n","        embed_dim = conf[\"trans_enc\"][\"embed_dim\"]\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        \n","        forward_expansion = conf[\"trans_enc\"][\"forward_expansion\"]\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_dim, forward_expansion * embed_dim),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_dim, embed_dim),\n","        )\n","\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","        dropout = conf['trans_enc'][\"dropout\"]\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","\n","      '''\n","      value ~ # [batch, seq_len, embed_dim]\n","      key ~ # [batch, seq_len, embed_dim]\n","      query ~ # [batch, seq_len, embed_dim]\n","      mask ~ # [batch, 1, 1, seq_len]\n","      '''\n","\n","      '''\n","      Step 1: passing the value, key, and query to self attention layer\n","      '''\n","      # [batch, seq_len, embed_dim] \n","      attention = self.attention(value,  # [batch, seq_len, embed_dim]\n","                                 key,  # [batch, seq_len, embed_dim]\n","                                 query,  # [batch, seq_len, embed_dim]\n","                                 mask  # [batch, 1, 1, seq_len]\n","                                 )\n","      # print(\"ENC query:- \\n\", query)\n","      # print(\"ENC attention:- \\n\", attention)\n","      '''\n","      Step 2: normalizing the output\n","      '''\n","      # [batch, seq_len, embed_dim]   \n","      # Add skip connection, run through normalization and finally dropout\n","      x = self.dropout(self.norm1(attention + query))\n","      forward = self.feed_forward(x)\n","      out = self.dropout(self.norm2(forward + x))\n","\n","      return out"],"metadata":{"id":"o9l2_G_3Xhnh","executionInfo":{"status":"ok","timestamp":1653955753435,"user_tz":300,"elapsed":92,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PositionalEncodingEncoder(nn.Module):\n","\n","    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n","        super(PositionalEncodingEncoder, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def pos_mask(self, src, conf):\n","      # Here src_pad_idx is always 0\n","      # because here src is embedding and embedding is 0 for padding_idx~1\n","      src_pad_idx = 0 \n","      src_mask = (src != src_pad_idx)\n","      # [batch, seq_len, embed_dim]\n","      # the values are True or False\n","      return src_mask\n","\n","    def forward(self, x: Tensor, conf: dict) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        pos_mask_val = self.pos_mask(x, conf)\n","\n","        x = x + self.pe[:x.size(0)]\n","        x = x.masked_fill(pos_mask_val == 0, float(\"0\"))\n","\n","        return self.dropout(x)"],"metadata":{"id":"jwVOuIEAWlo-","executionInfo":{"status":"ok","timestamp":1653955756674,"user_tz":300,"elapsed":90,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, src_vocab_size: int, name: str = '', conf: dict = {},):\n","\n","        super(TransformerEncoder, self).__init__()\n","        \n","        self.conf = conf\n","        embed_dim = conf['trans_enc']['embed_dim']\n","        self.word_embedding = nn.Embedding(num_embeddings = src_vocab_size, \n","                                           embedding_dim = embed_dim,\n","                                           padding_idx=1)\n","        \n","        # max_length is the max length of sentence in the entire input / batch\n","        max_length = conf['trans_enc']['max_length']\n","        max_length = conf['trans_enc']['max_length']\n","        self.position_embedding = PositionalEncodingEncoder(embed_dim,\n","                                                      max_length)\n","\n","        num_layers = conf['trans_enc'][\"num_Tlayers\"]\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerEncBlock(name='TransformerEncBlock', conf=conf)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        dropout = conf['trans_enc'][\"dropout\"]\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","\n","        '''\n","        x ~ # [batch, seq_len]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        '''\n","        Step 1: pass through the embedding layer to convert text into vectors\n","        '''\n","        # x_embed ~ [batch, seq_len, embed_dim] \n","        x_embed = self.word_embedding(x # [batch, seq_len]\n","                                      )  \n","        \n","        '''\n","        Step 2: position_embedding incorporates the position information\n","        '''\n","        if self.conf['trans_enc'][\"ps_embed_enc_ind\"]:\n","          x_embed_with_position = self.position_embedding(x_embed, conf)\n","          x_embed = x_embed_with_position\n","\n","        # out ~ [batch, seq_len, embed_dim] \n","        out = self.dropout(x_embed # [batch, seq_len, embed_dim] \n","                           )\n","\n","        '''\n","        Step 3: passing the embeddings to the transformer block\n","        '''\n","        # In the Encoder the query, key, value are all the same\n","        for t_layer in self.layers:\n","          # [batch, seq_len, embed_dim]\n","          out = t_layer(out,  # [batch, seq_len, embed_dim]\n","                        out,  # [batch, seq_len, embed_dim]\n","                        out,  # [batch, seq_len, embed_dim]\n","                        mask  # [batch, 1, 1, seq_len]\n","                      )\n","\n","        return out"],"metadata":{"id":"xRZh9FKHTm5O","executionInfo":{"status":"ok","timestamp":1653955759897,"user_tz":300,"elapsed":81,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Decoder Model"],"metadata":{"id":"OilFOBhFSsc1"}},{"cell_type":"code","source":["class MulHeadDecAttention2(nn.Module):\n","\n","    def __init__(self, conf: dict = {},):\n","        super(MulHeadDecAttention2, self).__init__()\n","\n","        self.embed_dim = conf['trans_dec'][\"embed_dim\"]\n","        self.heads = conf['trans_dec'][\"heads\"] #number of heads\n","        self.head_dim = conf['trans_dec'][\"embed_dim\"] // conf['trans_dec'][\"heads\"]\n","\n","        assert (\n","            self.head_dim * self.heads == conf['trans_dec'][\"embed_dim\"]\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","\n","    def forward(self, value, key, query, mask):\n","        \n","        '''\n","        value ~ # [batch, seq_len, embed_dim]\n","        key ~ # [batch, seq_len, embed_dim]\n","        query ~ # [batch, seq_len, embed_dim]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        # Get number of training examples\n","        batch = query.shape[0]\n","\n","        '''\n","        value_len ~ the max seq_len in values \n","        key_len ~ the max seq_len in keys \n","        query_len ~ the max seq_len in query \n","        here all of them are same\n","        '''\n","        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, value_len, heads, head_dim]\n","        value = value.reshape(batch, value_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, key_len, heads, head_dim]\n","        key = key.reshape(batch, key_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, query_len, heads, head_dim]\n","        query = query.reshape(batch, query_len, self.heads, self.head_dim)\n","\n","        # [batch, value_len, heads, head_dim]\n","        values = self.values(value  # [batch, value_len, heads, head_dim]\n","                             )\n","        # [batch, key_len, heads, head_dim]\n","        keys = self.keys(key  # [batch, key_len, heads, head_dim]\n","                         )\n","        # [batch, query_len, heads, head_dim]\n","        queries = self.queries(query  # [batch, query_len, heads, head_dim]\n","                               )\n","        \n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","        # [batch, head, query_len, key_len] \n","        # ~ we can say, query_len is our target and \n","        # key_len is our source and \n","        # energy is how much attention to pay on each word in key to predict query\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", \n","                              [queries, # [batch, query_len, heads, head_dim]\n","                               keys # [batch, key_len, heads, head_dim]\n","                               ])\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","          energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","            # energy = energy.masked_fill(mask == 0, float(\"0\"))\n","\n","        # print(\"energy:- \\n\",energy/ (self.embed_dim ** (1 / 2)))\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability ~ sqrt(d_k)\n","        # attention shape: [batch, heads, query_len, key_len]\n","        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n","\n","        # print(\"attention:- \\n\",attention)\n","        '''\n","        here key_len and value_len are same ~ denoted by l \n","        '''\n","        # [batch, query_len, heads, head_dim]\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,  # [batch, heads, query_len, key_len]\n","                                               values  # [batch, value_len, heads, head_dim]\n","                                               ])\n","\n","        # print(\"values:- \\n\",values)\n","        # print(\"out:- \\n\",out)\n","\n","        if mask is not None:\n","          out = out.masked_fill(values == 0, float(\"0\"))\n","\n","        # print(\"after out:- \\n\", out)\n","        # [batch, query_len, heads, head_dim] --converted--> [batch, query_len, embed_dim]\n","        out = out.reshape(batch, query_len, self.heads * self.head_dim)\n","\n","        return out"],"metadata":{"id":"op5BkUTZTrkn","executionInfo":{"status":"ok","timestamp":1653874088666,"user_tz":300,"elapsed":357,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["class MulHeadDecAttention1(nn.Module):\n","\n","    def __init__(self, conf: dict = {},):\n","        super(MulHeadDecAttention1, self).__init__()\n","\n","        self.embed_dim = conf['trans_dec'][\"embed_dim\"]\n","        self.heads = conf['trans_dec'][\"heads\"] #number of heads\n","        self.head_dim = conf['trans_dec'][\"embed_dim\"] // conf['trans_dec'][\"heads\"]\n","\n","        assert (\n","            self.head_dim * self.heads == conf['trans_dec'][\"embed_dim\"]\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","\n","    def forward(self, value, key, query, mask):\n","        \n","        '''\n","        value ~ # [batch, seq_len, embed_dim]\n","        key ~ # [batch, seq_len, embed_dim]\n","        query ~ # [batch, seq_len, embed_dim]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        # Get number of training examples\n","        batch = query.shape[0]\n","\n","        '''\n","        value_len ~ the max seq_len in values \n","        key_len ~ the max seq_len in keys \n","        query_len ~ the max seq_len in query \n","        here all of them are same\n","        '''\n","        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, value_len, heads, head_dim]\n","        value = value.reshape(batch, value_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, key_len, heads, head_dim]\n","        key = key.reshape(batch, key_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, query_len, heads, head_dim]\n","        query = query.reshape(batch, query_len, self.heads, self.head_dim)\n","\n","        # [batch, value_len, heads, head_dim]\n","        values = self.values(value  # [batch, value_len, heads, head_dim]\n","                             )\n","        # [batch, key_len, heads, head_dim]\n","        keys = self.keys(key  # [batch, key_len, heads, head_dim]\n","                         )\n","        # [batch, query_len, heads, head_dim]\n","        queries = self.queries(query  # [batch, query_len, heads, head_dim]\n","                               )\n","        \n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","        # [batch, head, query_len, key_len] \n","        # ~ we can say, query_len is our target and \n","        # key_len is our source and \n","        # energy is how much attention to pay on each word in key to predict query\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", \n","                              [queries, # [batch, query_len, heads, head_dim]\n","                               keys # [batch, key_len, heads, head_dim]\n","                               ])\n","\n","        print(\"DEC1 energy:-\\n\",energy)\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","          energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","            # energy = energy.masked_fill(mask == 0, float(\"0\"))\n","\n","        # print(\"energy:- \\n\",energy/ (self.embed_dim ** (1 / 2)))\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability ~ sqrt(d_k)\n","        # attention shape: [batch, heads, query_len, key_len]\n","        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n","\n","        # print(\"attention:- \\n\",attention)\n","        '''\n","        here key_len and value_len are same ~ denoted by l \n","        '''\n","        # [batch, query_len, heads, head_dim]\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,  # [batch, heads, query_len, key_len]\n","                                               values  # [batch, value_len, heads, head_dim]\n","                                               ])\n","\n","        print(\"DEC1 values:-\\n\",values)\n","        print(\"DEC1 out:-\\n\",out)\n","\n","        # if mask is not None:\n","        #   out = out.masked_fill(values == 0, float(\"0\"))\n","\n","        # print(\"after out:- \\n\", out)\n","        # [batch, query_len, heads, head_dim] --converted--> [batch, query_len, embed_dim]\n","        out = out.reshape(batch, query_len, self.heads * self.head_dim)\n","\n","        return out"],"metadata":{"id":"4cRA-UycZxrz","executionInfo":{"status":"ok","timestamp":1653874164575,"user_tz":300,"elapsed":326,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["class TransformerDecBlock(nn.Module):\n","    def __init__(self, conf):\n","        super(TransformerDecBlock, self).__init__()\n","\n","        self.attention_1 = MulHeadDecAttention1(conf) # with mask\n","        \n","        embed_dim = conf[\"trans_dec\"][\"embed_dim\"]\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","\n","        self.attention_2 = MulHeadDecAttention2(conf) # without mask\n","\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","        forward_expansion = conf[\"trans_enc\"][\"forward_expansion\"]\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_dim, forward_expansion * embed_dim),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_dim, embed_dim),\n","        )\n","\n","        self.norm3 = nn.LayerNorm(embed_dim)\n","\n","        dropout = conf['trans_enc'][\"dropout\"]\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, trgt_value, trgt_key, trgt_query, trgt_mask, src_value, src_key, src_mask):\n","\n","      '''\n","      trgt_value ~ # [batch, trgt_len, embed_dim]\n","      trgt_key ~ # [batch, trgt_len, embed_dim]\n","      trgt_query ~ # [batch, trgt_len, embed_dim]\n","      trgt_mask ~ # [batch, 1, trgt_len, trgt_len],\n","      src_value ~ # [batch, src_len, embed_dim]\n","      src_key ~ # [batch, src_len, embed_dim]\n","      src_mask # [batch, 1, 1, src_len],\n","      '''\n","\n","      '''\n","      Step 1: passing the value, key, and query to self attention layer\n","      '''\n","      # \n","      attention_1 = self.attention_1(trgt_value,  # [batch, trgt_len, embed_dim]\n","                                 trgt_key,  # [batch, trgt_len, embed_dim]\n","                                 trgt_query,  # [batch, trgt_len, embed_dim]\n","                                 trgt_mask  # [batch, 1, trgt_len, trgt_len]\n","                                 )\n","\n","      '''\n","      Step 2: normalizing the output\n","      '''\n","      # [batch, seq_len, embed_dim]   \n","      # Add skip connection, run through normalization and finally dropout\n","      x = self.dropout(self.norm1(attention_1 + trgt_query))\n","\n","\n","      # [batch, seq_len, embed_dim] \n","      attention_2 = self.attention_1(src_value,  # [batch, seq_len, embed_dim]\n","                                 src_key,  # [batch, seq_len, embed_dim]\n","                                 x,  # [batch, seq_len, embed_dim]\n","                                 src_mask  # [batch, 1, 1, seq_len]\n","                                 )\n","      \n","      x = self.dropout(self.norm1(attention_2 + x))\n","\n","      forward = self.feed_forward(x)\n","      out = self.dropout(self.norm3(forward + x))\n","\n","      return out"],"metadata":{"id":"yxUDSVw9bE0e","executionInfo":{"status":"ok","timestamp":1653874173695,"user_tz":300,"elapsed":270,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["class PositionalEncodingDecoder(nn.Module):\n","\n","    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def pos_mask(self, trgt, conf):\n","      trgt_pad_idx = conf[\"trans_dec\"][\"trgt_pad_idx\"]\n","      trgt_mask = (trgt != trgt_pad_idx)\n","      # [batch, seq_len, embed_dim]\n","      # the values are True or False\n","      return trgt_mask\n","\n","    def forward(self, x: Tensor, conf: dict) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        pos_mask_val = self.pos_mask(x, conf)\n","\n","        x = x + self.pe[:x.size(0)]\n","        x = x.masked_fill(pos_mask_val == 0, float(\"0\"))\n","\n","        return self.dropout(x)"],"metadata":{"id":"2ais-NE6a3-0","executionInfo":{"status":"ok","timestamp":1653874179741,"user_tz":300,"elapsed":402,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, trgt_vocab_size, conf):\n","\n","        super(TransformerDecoder, self).__init__()\n","        \n","        self.conf = conf\n","        embed_dim = conf['trans_dec']['embed_dim']\n","        padding_idx = conf['trans_dec']['trgt_pad_idx']\n","        self.word_embedding = nn.Embedding(num_embeddings = trgt_vocab_size, \n","                                           embedding_dim = embed_dim,\n","                                           padding_idx=1)\n","        \n","        # max_length is the max length of sentence in the entire input / batch\n","        max_length = conf['trans_dec']['max_length']\n","        self.position_embedding = PositionalEncodingDecoder(embed_dim,\n","                                                      max_length)\n","\n","        num_layers = conf['trans_dec'][\"num_Tlayers\"]\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerDecBlock(conf)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        dropout = conf['trans_dec'][\"dropout\"]\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.fc = nn.Linear(embed_dim, trgt_vocab_size)\n","\n","    def forward(self, tgt, trgt_mask, src_enc, src_mask):\n","\n","        '''\n","        tgt ~ # [batch, trgt_len]\n","        trgt_mask ~ # [batch, 1, trgt_len, trgt_len]\n","        src_enc ~ # [batch, src_len]\n","        src_mask ~ # [batch, 1, 1, src_len]\n","        '''\n","\n","        '''\n","        Step 1: pass through the embedding layer to convert text into vectors\n","        '''\n","        # tgt_embed ~ [batch, trgt_len, embed_dim] \n","        tgt_embed = self.word_embedding(tgt # [batch, trgt_len]\n","                                      )  \n","        \n","        '''\n","        Step 2: position_embedding incorporates the position information\n","        '''\n","        if self.conf['trans_dec'][\"ps_embed_enc_ind\"]:\n","          trgt_embed_with_position = self.position_embedding(tgt_embed, conf)\n","          trgt_embed = trgt_embed_with_position\n","\n","        out = self.dropout(trgt_embed)\n","\n","        '''\n","        Step 3: passing the embeddings to the transformer block\n","        '''\n","        # In the Encoder the query, key, value are all the same\n","        for t_layer in self.layers:\n","          # [batch, seq_len, embed_dim]\n","          out = t_layer(out,  # [batch, trgt_len, embed_dim]\n","                        out,  # [batch, trgt_len, embed_dim]\n","                        out,  # [batch, trgt_len, embed_dim]\n","                        trgt_mask,  # [batch, 1, trgt_len, trgt_len],\n","                        src_enc, # [batch, src_len, embed_dim]\n","                        src_enc, # [batch, src_len, embed_dim]\n","                        src_mask # [batch, 1, 1, src_len],\n","                      )\n","          \n","        out = self.fc(out)\n","\n","        return out"],"metadata":{"id":"sxcBIR-ZZsuc","executionInfo":{"status":"ok","timestamp":1653874182990,"user_tz":300,"elapsed":307,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Model"],"metadata":{"id":"C9H2zmTGSt29"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size: int, trgt_vocab_size: int, conf: dict = {},):\n","        super().__init__()\n","\n","        self.src_pad_idx = 1\n","        self.trgt_pad_idx = 1\n","        self.device = conf[\"device\"]\n","        self.encoder = TransformerEncoder(src_vocab_size=src_vocab_size, conf=conf)    \n","        # self.decoder = TransformerDecoder(trgt_vocab_size=trgt_vocab_size, conf=conf)\n","\n","    \n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        return src_mask.to(self.device)\n","\n","    def make_trgt_mask(self, trgt):\n","      batch, trgt_len = trgt.shape\n","      trgt_mask = torch.tril(torch.ones((trgt_len, trgt_len))).expand(batch, 1, trgt_len, trgt_len)\n","      return trgt_mask.to(self.device)\n","    \n","    def forward(self, src: Tensor, trgt: Tensor) -> Tensor:\n","      '''\n","      Step 1: create mask for the source text\n","      '''\n","      # [batch, 1, 1, src_len]\n","      # the values are True or False\n","      # all the ids (=src_pad_idx) will be False and rest will True\n","      src_mask = self.make_src_mask(src # [batch, src_len]\n","                                    )\n","      \n","      '''\n","      Step 2: encoding the source text\n","      '''\n","      # [batch, src_len, embed_dim]\n","      src_enc = self.encoder(src, # [batch, src_len]\n","                             src_mask # [batch, 1, 1, src_len]\n","                             )\n","\n","      # '''\n","      # Step 3: create mask for the target task\n","      # '''\n","      # # [batch, 1, trgt_len, trgt_len]\n","      # # triangular matrix\n","      # trgt_mask = self.make_trgt_mask(trgt # [batch, trgt_len]\n","      #                                 )\n","\n","      # '''\n","      # Step 4: encoding the target text\n","      # src_enc (source encoding) is used to the encoding the target\n","      # '''  \n","      # out = self.decoder(trgt, # [batch, trgt_len]\n","      #                    trgt_mask, # [batch, 1, trgt_len, trgt_len]\n","      #                    src_enc, # [batch, src_len]\n","      #                    src_mask # [batch, 1, 1, src_len]\n","      #                    )\n","\n","      return out"],"metadata":{"id":"rPGHEPTxR1LE","executionInfo":{"status":"ok","timestamp":1653955777233,"user_tz":300,"elapsed":97,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["### toy example\n","conf = {\n","    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"trans_enc\": {\n","      \"src_pad_idx\" : 0,\n","      \"embed_dim\" : 6,\n","      \"max_length\" : 20,\n","      \"ps_embed_enc_ind\": True,\n","      \"num_Tlayers\" : 1,\n","      \"heads\" : 2,\n","      \"dropout\" : 0.1,\n","      \"forward_expansion\" : 4\n","    },\n","    \"trans_dec\": {\n","      \"trgt_pad_idx\" : 0,\n","      \"embed_dim\" : 6,\n","      \"max_length\" : 20,\n","      \"ps_embed_enc_ind\": True,\n","      \"num_Tlayers\" : 1,\n","      \"heads\" : 2,\n","      \"dropout\" : 0.1,\n","      \"forward_expansion\" : 4\n","    }\n","}\n","\n","# 1 padding\n","x = torch.tensor([\n","                  [5,2,1],\n","                  [6,7,2]\n","]).to(conf['device'])\n","y = torch.tensor([\n","                  [7,4,3,5,9,2,1],\n","                  [5,6,2,4,7,6,2]\n","]).to(conf['device'])\n","\n","# print(y[:,:-1])\n","# print(\"x shape:- \", x.shape)\n","# print(\"y shape:-\", y.shape)\n","# print(\"y[:,:-1] shape:-\", y[:,:-1].shape)\n","\n","src_vocab_size = 10\n","trgt_vocab_size = 10\n","\n","# instantiate the model\n","model = Transformer(src_vocab_size, trgt_vocab_size, conf)\n","# output\n","out = model(x, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2Zd41_BCR1Sn","executionInfo":{"status":"error","timestamp":1653955784054,"user_tz":300,"elapsed":406,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"bb94858a-a50a-4780-a625-8e50ea79023c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ENC attention:- \n"," tensor([[[[0.6410, 0.3590, 0.0000],\n","          [0.5928, 0.4072, 0.0000],\n","          [0.5000, 0.5000, 0.0000]],\n","\n","         [[0.6649, 0.3351, 0.0000],\n","          [0.5128, 0.4872, 0.0000],\n","          [0.5000, 0.5000, 0.0000]]],\n","\n","\n","        [[[0.3879, 0.3269, 0.2852],\n","          [0.3314, 0.3327, 0.3359],\n","          [0.3005, 0.3295, 0.3699]],\n","\n","         [[0.4808, 0.3156, 0.2036],\n","          [0.2113, 0.2346, 0.5541],\n","          [0.3815, 0.2295, 0.3890]]]], grad_fn=<SoftmaxBackward0>)\n","ENC values:- \n"," tensor([[[[-0.5293,  0.4282, -0.8814],\n","          [ 0.7321,  1.3328,  0.5176]],\n","\n","         [[-0.4071, -1.0329, -0.6063],\n","          [ 0.1062,  0.1323,  0.1116]],\n","\n","         [[ 0.0000,  0.0000,  0.0000],\n","          [ 0.0000,  0.0000,  0.0000]]],\n","\n","\n","        [[[-0.4772,  0.9794, -1.0128],\n","          [ 0.1031,  1.8358, -0.4967]],\n","\n","         [[ 0.0455, -0.1329,  0.0962],\n","          [ 0.9042,  0.2113,  1.4982]],\n","\n","         [[ 0.6774, -0.7517,  1.1889],\n","          [ 0.4709, -0.9083,  0.8839]]]], grad_fn=<UnsafeViewBackward0>)\n","ENC out:- \n"," tensor([[[[-0.4854, -0.0963, -0.7827],\n","          [ 0.5223,  0.9305,  0.3816]],\n","\n","         [[-0.4795, -0.1668, -0.7694],\n","          [ 0.4271,  0.7479,  0.3198]],\n","\n","         [[-0.4682, -0.3024, -0.7438],\n","          [ 0.4191,  0.7326,  0.3146]]],\n","\n","\n","        [[[ 0.0230,  0.1220, -0.0223],\n","          [ 0.4308,  0.7644,  0.4140]],\n","\n","         [[ 0.0846,  0.0278,  0.0958],\n","          [ 0.4948, -0.0657,  0.7362]],\n","\n","         [[ 0.1222, -0.0275,  0.1671],\n","          [ 0.4300,  0.3954,  0.4982]]]], grad_fn=<ViewBackward0>)\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-586b05456b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-7625854f64d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trgt)\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0;31m#                    )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"]}]},{"cell_type":"code","source":["#define hyperparameters\n","eng_vocab_size = len(ENG_TEXT.vocab)\n","ben_vocab_size = len(BEN_TEXT.vocab)\n","\n","conf = {\n","    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"trans_enc\": {\n","      \"src_pad_idx\" : 1,\n","      \"embed_dim\" : 6,\n","      \"max_length\" : 20,\n","      \"ps_embed_enc_ind\": True,\n","      \"num_Tlayers\" : 2,\n","      \"heads\" : 2,\n","      \"dropout\" : 0.1,\n","      \"forward_expansion\" : 4\n","    },\n","    \"trans_dec\": {\n","      \"trgt_pad_idx\" : 1,\n","      \"embed_dim\" : 6,\n","      \"max_length\" : 20,\n","      \"ps_embed_enc_ind\": True,\n","      \"num_Tlayers\" : 2,\n","      \"heads\" : 2,\n","      \"dropout\" : 0.1,\n","      \"forward_expansion\" : 4\n","    }\n","}\n","\n","#instantiate the model\n","train_model = Transformer(vocab_size, conf)\n","train_model = train_model.to(device)"],"metadata":{"id":"049WsBxkR1ZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GAy8Qls2WZSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"buhQRzw_WZVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9yvGvyrXWZYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_9FMlOzzWZap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WS52pqj0WZdb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Qe8VtWXyWZgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uu26VLoJWZid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","A from scratch implementation of Transformer network,\n","following the paper Attention is all you need with a\n","few minor differences. I tried to make it as clear as\n","possible to understand and also went through the code\n","on my youtube channel!\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            print(\"energy:- \", energy.shape)\n","            print(\"mask:- \", mask.shape)\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length,\n","    ):\n","\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout,\n","                    forward_expansion=forward_expansion,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out = self.dropout(\n","            (self.word_embedding(x) + self.position_embedding(positions))\n","        )\n","\n","        # In the Encoder the query, key, value are all the same, it's in the\n","        # decoder this will change. This might look a bit odd in this case.\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.attention = SelfAttention(embed_size, heads=heads)\n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, trg_mask, enc_dec_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.transformer_block(value, key, query, enc_dec_mask)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super(Decoder, self).__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, trg_mask, enc_dec_mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, trg_mask, enc_dec_mask)\n","\n","        out = self.fc_out(x)\n","\n","        return out\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        trg_pad_idx,\n","        embed_size=15,\n","        num_layers=1,\n","        forward_expansion=4,\n","        heads=3,\n","        dropout=0,\n","        device=\"cpu\",\n","        max_length=100,\n","    ):\n","\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","      trgt_mask = (trg != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","      # (N, 1, 1, trgt_len)\n","      return trgt_mask.to(self.device)\n","\n","    def make_enc_dec_mask(self, src, trg):\n","        N, trg_len = trg.shape\n","        N, src_len = src.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, src_len))).expand(\n","            N, 1, trg_len, src_len\n","        )\n","\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_dec_mask = self.make_enc_dec_mask(src, trg)\n","        print(enc_dec_mask)\n","        enc_src = self.encoder(src, src_mask)\n","        print(enc_src.shape)\n","        out = self.decoder(trg, enc_src, trg_mask, enc_dec_mask)\n","        return out\n","\n","\n","if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","\n","    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n","        device\n","    )\n","    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","\n","    src_pad_idx = 0\n","    trg_pad_idx = 0\n","    src_vocab_size = 10\n","    trg_vocab_size = 10\n","    model = Transformer(src_vocab_size, \n","                        trg_vocab_size, \n","                        src_pad_idx, \n","                        trg_pad_idx, \n","                        device=device).to(\n","        device\n","    )\n","    out = model(x, trg[:, :])\n","    print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wa6_2cvLWZmb","executionInfo":{"status":"ok","timestamp":1653940000090,"user_tz":300,"elapsed":710,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"f251024f-9aec-4f61-c2a0-df79a07e25f4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 1., 0.]]],\n","\n","\n","        [[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 1., 0.]]]])\n","energy:-  torch.Size([2, 3, 9, 9])\n","mask:-  torch.Size([2, 1, 1, 9])\n","torch.Size([2, 9, 15])\n","energy:-  torch.Size([2, 3, 8, 8])\n","mask:-  torch.Size([2, 1, 1, 8])\n","energy:-  torch.Size([2, 3, 8, 9])\n","mask:-  torch.Size([2, 1, 8, 9])\n","torch.Size([2, 8, 10])\n"]}]},{"cell_type":"code","source":["attn = torch.tensor(\n","        [\n","         [[[0.6410, 0.3590, 0.0000],\n","          [0.5928, 0.4072, 0.0000],\n","          [0.5000, 0.5000, 0.0000]],\n","\n","         [[0.6649, 0.3351, 0.0000],\n","          [0.5128, 0.4872, 0.0000],\n","          [0.5000, 0.5000, 0.0000]]],\n","\n","\n","        [[[0.3879, 0.3269, 0.2852],\n","          [0.3314, 0.3327, 0.3359],\n","          [0.3005, 0.3295, 0.3699]],\n","\n","         [[0.4808, 0.3156, 0.2036],\n","          [0.2113, 0.2346, 0.5541],\n","          [0.3815, 0.2295, 0.3890]]]])\n","\n","val = torch.tensor([[[[-0.5293,  0.4282, -0.8814],\n","          [ 0.7321,  1.3328,  0.5176]],\n","\n","         [[-0.4071, -1.0329, -0.6063],\n","          [ 0.1062,  0.1323,  0.1116]],\n","\n","         [[ 0.0000,  0.0000,  0.0000],\n","          [ 0.0000,  0.0000,  0.0000]]],\n","\n","\n","        [[[-0.4772,  0.9794, -1.0128],\n","          [ 0.1031,  1.8358, -0.4967]],\n","\n","         [[ 0.0455, -0.1329,  0.0962],\n","          [ 0.9042,  0.2113,  1.4982]],\n","\n","         [[ 0.6774, -0.7517,  1.1889],\n","          [ 0.4709, -0.9083,  0.8839]]]])\n","\n","c = torch.einsum('nhql,nlhd->nqhd', [attn, val])\n","c = c.masked_fill(val == 0, float(\"0\"))\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVtGVBYFZOhs","executionInfo":{"status":"ok","timestamp":1653957148484,"user_tz":300,"elapsed":101,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"2fb172c9-c34a-4d73-c0f2-ed297f02a1c4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[-0.4854, -0.0963, -0.7826],\n","          [ 0.5224,  0.9305,  0.3815]],\n","\n","         [[-0.4795, -0.1668, -0.7694],\n","          [ 0.4272,  0.7479,  0.3198]],\n","\n","         [[ 0.0000,  0.0000,  0.0000],\n","          [ 0.0000,  0.0000,  0.0000]]],\n","\n","\n","        [[[ 0.0230,  0.1221, -0.0223],\n","          [ 0.4308,  0.7644,  0.4140]],\n","\n","         [[ 0.0845,  0.0279,  0.0957],\n","          [ 0.4948, -0.0658,  0.7363]],\n","\n","         [[ 0.1222, -0.0275,  0.1671],\n","          [ 0.4300,  0.3955,  0.4982]]]])\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","scores = F.softmax(attn, dim=-1)\n","print(scores.shape)\n","print(val.shape)\n","\n","scores = torch.matmul(scores, val)\n","print(scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"hrPR19cxf8ri","executionInfo":{"status":"error","timestamp":1653957012842,"user_tz":300,"elapsed":92,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"ee8dd036-12cb-40b7-a5c7-214186206c16"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 3, 3])\n","torch.Size([2, 3, 2, 3])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-0cdebeafa205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"]}]}]}