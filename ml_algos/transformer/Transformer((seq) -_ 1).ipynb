{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer((seq) -> 1).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOc9xKL/qlC32kZ8bUZ5UlN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oY20xRKx0_Co","executionInfo":{"status":"ok","timestamp":1651631156163,"user_tz":300,"elapsed":126206,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"f579e1f7-d690-4705-9acf-cb2b8bb9e6a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.10.0\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x21fe000 @  0x7f4298ca8615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 881.9 MB 18 kB/s \n","\u001b[?25hCollecting torchtext==0.11.0\n","  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n","\u001b[K     |████████████████████████████████| 8.0 MB 40.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (4.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.12.0\n","    Uninstalling torchtext-0.12.0:\n","      Successfully uninstalled torchtext-0.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0 torchtext-0.11.0\n"]}],"source":["!pip install -U torch==1.10.0 torchtext==0.11.0\n","\n","# Reload environment\n","exit()"]},{"cell_type":"code","source":["import random\n","import re\n","import math\n","import pandas as pd\n","import spacy\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch import Tensor\n","from torchtext.legacy import data"],"metadata":{"id":"QZOKvl0710lL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","g_path = \"/content/drive/My Drive/\"\n","data_path = g_path + \"ml-data/\"\n","code_path = g_path + \"pytorch-nlp/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMDFk4sw16lT","executionInfo":{"status":"ok","timestamp":1651631380285,"user_tz":300,"elapsed":15579,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"f2607eea-f4e1-4620-eba3-40f8b309e3fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_fl = 'data/IMDB_review_sentiment_small.csv'"],"metadata":{"id":"UWQwl8I22Amu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#reproducing the same result\n","SEED = 2021\n","torch.manual_seed(SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAkMpmi82DNZ","executionInfo":{"status":"ok","timestamp":1651631385300,"user_tz":300,"elapsed":130,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"13f30000-1105-48b0-9f69-85cbf6bf8fe9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd070241630>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["spacy_en = spacy.load('en')\n","def clean_data(texts):\n","    cleaned_text = []\n","    for text in texts:\n","        # remove break\n","        text = text.replace('br', '')\n","        # remove punctuation\n","        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n","        # remove multiple spaces\n","        text = re.sub(r' +', ' ', text)\n","        # remove newline\n","        text = re.sub(r'\\n', ' ', text)\n","        # strip the text\n","        text = text.strip()\n","        # lower the text\n","        text = text.lower()\n","\n","        if text != '':\n","          cleaned_text.append(text)\n","    return cleaned_text\n","\n","def tokenizer(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","TEXT = data.Field(preprocessing=clean_data,tokenize=tokenizer,batch_first=True,include_lengths=True)\n","LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n","fields = [('text',TEXT),('label', LABEL)]"],"metadata":{"id":"Y8NCCa4D2F3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loading the entire data\n","def load_data():\n","  imdb_data = data.TabularDataset(path = data_path+data_fl,format = 'csv', fields = fields, skip_header = True)\n","  return imdb_data\n","\n","imdb_data = load_data() \n","print(vars(imdb_data.examples[0]))\n","print(imdb_data.examples[0].text, imdb_data.examples[0].label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-HpiV7I2Jhl","executionInfo":{"status":"ok","timestamp":1651631392453,"user_tz":300,"elapsed":743,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"e1abb8d6-6a71-4c07-c8da-e88b8370bdb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'], 'label': '1'}\n","['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'utality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', 'n t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'does', 'n t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'n t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'] 1\n"]}]},{"cell_type":"code","source":["#splitting the data into training and validation dataset\n","def split_data(imdb_data):\n","  train_data, valid_data = imdb_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","  return train_data, valid_data\n","\n","train_data, valid_data = split_data(imdb_data)"],"metadata":{"id":"HGgkDubf2uW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#generate vocabulary\n","TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n","LABEL.build_vocab(train_data)\n","\n","#No. of unique tokens in text\n","print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n","#No. of unique tokens in label\n","print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n","\n","print(\"unk:- \", TEXT.vocab.stoi[\"<unk>\"])\n","print(\"pad:- \", TEXT.vocab.stoi[\"<pad>\"])\n","print(\"sos:- \", TEXT.vocab.stoi[\"<sos>\"]) #not present in dictionary\n","print(\"The first word in vocab is \", TEXT.vocab.itos[0])\n","print(\"The second word in vocab is \", TEXT.vocab.itos[1])\n","print(\"The third word in vocab is \", TEXT.vocab.itos[2])\n","print(\"The last word in vocab is \", TEXT.vocab.itos[len(TEXT.vocab)-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5pVbJe439Z5","executionInfo":{"status":"ok","timestamp":1651631602743,"user_tz":300,"elapsed":205676,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"16096e70-715e-423c-a2fe-f10bb89459a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n","100%|█████████▉| 399999/400000 [00:17<00:00, 23516.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Size of TEXT vocabulary: 466\n","Size of LABEL vocabulary: 2\n","unk:-  0\n","pad:-  1\n","sos:-  0\n","The first word in vocab is  <unk>\n","The second word in vocab is  <pad>\n","The third word in vocab is  the\n","The last word in vocab is  yes\n"]}]},{"cell_type":"code","source":["#preparing batches for training the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","#set batch size\n","BATCH_SIZE = 5\n","\n","#Load an iterator\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data), \n","    batch_size = BATCH_SIZE,\n","    sort=False, # Sort all examples in data using `sort_key`.\n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True, # Use `sort_key` to sort examples in each batch.\n","    device = device)"],"metadata":{"id":"UWfvzu2E5ARk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","\n","    def __init__(self, conf):\n","        super(SelfAttention, self).__init__()\n","\n","        self.embed_dim = conf[\"embed_dim\"]\n","        self.heads = conf[\"heads\"] #number of heads\n","        self.head_dim = conf[\"embed_dim\"] // conf[\"heads\"]\n","\n","        assert (\n","            self.head_dim * self.heads == conf[\"embed_dim\"]\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","\n","        self.fc_out = nn.Linear(self.heads * self.head_dim,  self.embed_dim)\n","\n","    def forward(self, values, keys, query, mask):\n","        \n","        '''\n","        values ~ # [batch, seq_len, embed_dim]\n","        keys ~ # [batch, seq_len, embed_dim]\n","        query ~ # [batch, seq_len, embed_dim]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        # Get number of training examples\n","        batch = query.shape[0]\n","\n","        '''\n","        value_len ~ the max seq_len in values \n","        key_len ~ the max seq_len in keys \n","        query_len ~ the max seq_len in query \n","        here all of them are same\n","        '''\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, value_len, heads, head_dim]\n","        values = values.reshape(batch, value_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, key_len, heads, head_dim]\n","        keys = keys.reshape(batch, key_len, self.heads, self.head_dim)\n","\n","        # [batch, seq_len, embed_dim] --converted--> [batch, seq_len, heads, head_dim]\n","        # [batch, query_len, heads, head_dim]\n","        query = query.reshape(batch, query_len, self.heads, self.head_dim)\n","\n","\n","\n","        # [batch, value_len, heads, head_dim]\n","        values = self.values(values  # [batch, value_len, heads, head_dim]\n","                             )\n","        # [batch, key_len, heads, head_dim]\n","        keys = self.keys(keys  # [batch, key_len, heads, head_dim]\n","                         )\n","        # [batch, query_len, heads, head_dim]\n","        queries = self.queries(query  # [batch, query_len, heads, head_dim]\n","                               )\n","\n","        \n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","        # [batch, head, query_len, heads]\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", \n","                              [queries, # [batch, query_len, heads, head_dim]\n","                               keys # [batch, key_len, heads, head_dim]\n","                               ])\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            # energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","            energy = energy.masked_fill(mask == 0, float(\"0\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability ~ sqrt(d_k)\n","        # attention shape: [batch, heads, query_len, key_len]\n","        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n","\n","        '''\n","        here key_len and value_len are same ~ denoted by l \n","        '''\n","        # [batch, query_len, heads, head_dim]\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,  # [batch, heads, query_len, key_len]\n","                                               values  # [batch, value_len, heads, head_dim]\n","                                               ])\n","        # [batch, query_len, heads, head_dim] --converted--> [batch, query_len, embed_dim]\n","        out = out.reshape(batch, query_len, self.heads * self.head_dim)\n","\n","\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # [batch, query_len, embed_dim]\n","        out = self.fc_out(out # [batch, query_len, embed_dim]\n","                          )\n","\n","        return out"],"metadata":{"id":"CWkD5kMh5HeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, conf):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(conf)\n","        self.norm1 = nn.LayerNorm(conf[\"embed_dim\"])\n","        self.norm2 = nn.LayerNorm(conf[\"embed_dim\"])\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(conf[\"embed_dim\"], \n","                      conf[\"forward_expansion\"] * conf[\"embed_dim\"]),\n","            nn.ReLU(),\n","            nn.Linear(conf[\"forward_expansion\"] * conf[\"embed_dim\"], \n","                      conf[\"embed_dim\"]),\n","        )\n","\n","        self.dropout = nn.Dropout(conf[\"dropout\"])\n","\n","    def forward(self, value, key, query, mask):\n","\n","      '''\n","      value ~ # [batch, seq_len, embed_dim]\n","      key ~ # [batch, seq_len, embed_dim]\n","      query ~ # [batch, seq_len, embed_dim]\n","      mask ~ # [batch, 1, 1, seq_len]\n","      '''\n","\n","      '''\n","      Step 1: passing the value, key, and query to self attention layer\n","      '''\n","      # [batch, seq_len, embed_dim] \n","      attention = self.attention(value,  # [batch, seq_len, embed_dim]\n","                                 key,  # [batch, seq_len, embed_dim]\n","                                 query,  # [batch, seq_len, embed_dim]\n","                                 mask  # [batch, 1, 1, seq_len]\n","                                 )\n","\n","      '''\n","      Step 2: normalizing the output\n","      '''\n","      # [batch, seq_len, embed_dim]   \n","      # Add skip connection, run through normalization and finally dropout\n","      x = self.dropout(self.norm1(attention + query))\n","      forward = self.feed_forward(x)\n","      out = self.dropout(self.norm2(forward + x))\n","\n","      return out"],"metadata":{"id":"7FmAIMjO5UxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def pos_mask(self, src, conf):\n","        src_mask = (src != conf[\"src_pad_idx\"])\n","        # [batch, seq_len, embed_dim]\n","        # the values are True or False\n","        return src_mask\n","\n","    def forward(self, x: Tensor, conf: dict) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        pos_mask_val = self.pos_mask(x, conf)\n","\n","        x = x + self.pe[:x.size(0)]\n","        x = x.masked_fill(pos_mask_val == 0, float(\"0\"))\n","\n","        return self.dropout(x)"],"metadata":{"id":"OenFYGVT5ZFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        conf\n","    ):\n","\n","        super(Transformer, self).__init__()\n","        \n","        self.word_embedding = nn.Embedding(num_embeddings = src_vocab_size, \n","                                           embedding_dim = conf[\"embed_dim\"],\n","                                           padding_idx=1)\n","        \n","        # max_length is the max length of sentence in the entire input / batch\n","        max_length = 10 \n","        self.position_embedding = PositionalEncoding(conf[\"embed_dim\"],\n","                                                      max_length)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(conf)\n","                for _ in range(conf[\"num_Tlayers\"])\n","            ]\n","        )\n","        self.dropout = nn.Dropout(conf[\"dropout\"])\n","\n","    def forward(self, x, mask):\n","\n","        '''\n","        x ~ # [batch, seq_len]\n","        mask ~ # [batch, 1, 1, seq_len]\n","        '''\n","\n","        '''\n","        Step 1: pass through the embedding layer to convert text into vectors\n","        '''\n","        # x_embed ~ [batch, seq_len, embed_dim] \n","        x_embed = self.word_embedding(x # [batch, seq_len]\n","                                      )  \n","        \n","        '''\n","        Step 2: position_embedding incorporates the position information\n","        '''\n","        if conf[\"ps_embed_enc_ind\"]:\n","          x_embed_with_position = self.position_embedding(x_embed, conf)\n","          x_embed = x_embed_with_position\n","\n","        out = self.dropout(x_embed)\n","\n","        '''\n","        Step 3: passing the embeddings to the transformer block\n","        '''\n","        # In the Encoder the query, key, value are all the same\n","        for t_layer in self.layers:\n","          # [batch, seq_len, embed_dim]\n","          out = t_layer(out,  # [batch, seq_len, embed_dim]\n","                        out,  # [batch, seq_len, embed_dim]\n","                        out,  # [batch, seq_len, embed_dim]\n","                        mask  # [batch, 1, 1, seq_len]\n","                      )\n","\n","        return out"],"metadata":{"id":"cdh8eUTW51-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerClassifier(nn.Module):\n","    def __init__(\n","        self, src_vocab_size, conf\n","    ):\n","\n","        super(TransformerClassifier, self).__init__()\n","\n","        self.src_pad_idx = conf[\"src_pad_idx\"]\n","        self.device = conf[\"device\"]\n","\n","        self.encoder = Transformer(\n","            src_vocab_size,\n","            conf\n","        )\n","\n","        self.output_dim = 1\n","        #dense layer / linear layer\n","        self.fc = nn.Linear(conf[\"embed_dim\"], self.output_dim)\n","\n","        #activation function\n","        self.act = nn.Sigmoid()\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # [batch, 1, 1, seq_len]\n","        # the values are True or False\n","        return src_mask.to(self.device)\n","\n","    def forward(self, src):\n","      \n","      '''\n","      src ~ # [batch, seq_len]\n","      '''\n","\n","      '''\n","      Step 1: create mask for the attention layer\n","      '''\n","      # [batch, 1, 1, seq_len]\n","      # the values are True or False\n","      src_mask = self.make_src_mask(src # [batch, seq_len]\n","                                    )\n","\n","      '''\n","      Step 2: passing the input to transformer\n","      '''\n","      # [batch, seq_len, embed_dim]\n","      enc_out = self.encoder(src,  # [batch, seq_len]\n","                             src_mask  # [batch, 1, 1, seq_len]\n","                             )\n","      '''\n","      Step 3: sum all the embed dim of each word in a sentence\n","      '''\n","      # [batch_size, embed_dim]\n","      weighted_out = enc_out.sum(dim=1)\n","\n","      '''\n","      Step 4: feeding the weighted value to a linear layer\n","      '''\n","      # fc_out ~ [batch_size, output_dim]\n","      fc_out = self.fc(weighted_out  # [batch_size, embed_dim]\n","                       )\n","\n","      '''\n","      Step 5: feeding the linear output to activation function\n","      '''\n","      # out ~ [batch_size, output_dim]\n","      out = self.act(fc_out)\n","\n","      return out"],"metadata":{"id":"Nc346eMu56pB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define hyperparameters\n","vocab_size = len(TEXT.vocab)\n","\n","conf = {\n","    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"src_pad_idx\" : 1,\n","    \"embed_dim\" : 6,\n","    \"ps_embed_enc_ind\": True,\n","    \"num_Tlayers\" : 2,\n","    \"heads\" : 2,\n","    \"dropout\" : 0.1,\n","    \"forward_expansion\" : 4\n","}\n","\n","#instantiate the model\n","train_model = TransformerClassifier(vocab_size, conf)\n","train_model = train_model.to(device)"],"metadata":{"id":"4iat7lys5HnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(preds)\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc"],"metadata":{"id":"BruC6Z0F6PA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_model(valid_iterator, train_model, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  with torch.no_grad():\n","    for valid_batch in valid_iterator:\n","      \n","      #retrieve text and no. of words\n","      text, text_lengths = valid_batch.text\n","          \n","      #get prediction\n","      predictions = train_model(text)\n","      preds = predictions.squeeze(-1) #convert to 1D tensor\n","\n","      #compute the loss\n","      loss = criterion(preds, valid_batch.label)\n","\n","      #compute the binary accuracy\n","      acc = binary_accuracy(preds, valid_batch.label)\n","\n","      # compute loss and accuracy\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  valid_epoc_loss = epoch_loss / len(valid_iterator)\n","  valid_epoch_acc = epoch_acc / len(valid_iterator)\n","\n","  return valid_epoc_loss, valid_epoch_acc"],"metadata":{"id":"huIf0vvH6WMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training the model\n","\n","#define the optimizer\n","optimizer = optim.Adam(train_model.parameters())\n","\n","#define the loss\n","criterion = nn.BCELoss()\n","criterion = criterion.to(device)\n","\n","#set the model in training phase\n","train_model.train()\n","\n","N_EPOCHS = 6\n","VALIDATION_EPOCH = 2\n","\n","for epoch in range(N_EPOCHS+1):\n","\n","  #initialize every epoch \n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  for batch in train_iterator:\n","    #resets the gradients after every batch\n","    optimizer.zero_grad() \n","\n","    #retrieve text and no. of words\n","    text, text_lengths = batch.text\n","\n","    #get prediction\n","    predictions = train_model(text)\n","    preds = predictions.squeeze(-1) #convert to 1D tensor\n","\n","    #compute the loss\n","    loss = criterion(preds, batch.label)\n","\n","    #compute the binary accuracy\n","    acc = binary_accuracy(preds, batch.label)   \n","\n","    #backpropage the loss and compute the gradients\n","    loss.backward()\n","\n","    #update the weights\n","    optimizer.step() \n","\n","    # compute loss and accuracy\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  if epoch%VALIDATION_EPOCH == 0:\n","    train_model.eval() # set the model in eval phase\n","    valid_epoc_loss, valid_epoch_acc = valid_model(valid_iterator, train_model, criterion)\n","    train_model.train() # return back to training phase\n","\n","    print(\"epoch:- \",epoch)\n","    print(\"training===> \",\"loss:- \", epoch_loss / len(train_iterator), \"  accuracy:- \", epoch_acc / len(train_iterator))\n","    print(\"validation===> \",\"loss:- \", valid_epoc_loss, \"  accuracy:- \", valid_epoch_acc)\n","\n","  if epoch == N_EPOCHS-1:\n","    torch.save(train_model.state_dict(), code_path+\"model/classification_model.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-KCgH0_6hyy","executionInfo":{"status":"ok","timestamp":1651631809215,"user_tz":300,"elapsed":3042,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"00ed160d-9f0f-4d17-d3f7-9409bdc7513e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:-  0\n","training===>  loss:-  32.47466836656843   accuracy:-  0.37142858122076305\n","validation===>  loss:-  15.013405839602152   accuracy:-  0.6000000089406967\n","epoch:-  2\n","training===>  loss:-  32.84489893913269   accuracy:-  0.45714287034102846\n","validation===>  loss:-  16.63049602508545   accuracy:-  0.46666668355464935\n","epoch:-  4\n","training===>  loss:-  30.872309923171997   accuracy:-  0.42857143708637785\n","validation===>  loss:-  15.66829220453898   accuracy:-  0.533333346247673\n","epoch:-  6\n","training===>  loss:-  33.50921652998243   accuracy:-  0.3714285833495004\n","validation===>  loss:-  14.718185702959696   accuracy:-  0.5333333412806193\n"]}]},{"cell_type":"code","source":["###  Inference  ###\n","\n","#define hyperparameters\n","vocab_size = len(TEXT.vocab)\n","\n","conf = {\n","    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"src_pad_idx\" : 0,\n","    \"embed_dim\" : 6,\n","    \"ps_embed_enc_ind\": True,\n","    \"num_Tlayers\" : 2,\n","    \"heads\" : 2,\n","    \"dropout\" : 0.1,\n","    \"forward_expansion\" : 4\n","}\n","\n","#instantiate the model\n","test_model = TransformerClassifier(vocab_size, conf)\n","test_model = test_model.to(device)\n","\n","#loading the model\n","model_path = code_path+\"model/classification_model.pt\"\n","test_model.load_state_dict(torch.load(model_path))\n","\n","test_model.eval() # set the model in eval phase\n","\n","\n","# test_sentence = \"Are there any sports that you don't like?\"\n","# test_sentence = \"I love the movie\"\n","# test_sentence = \"I dislike the movie\"\n","test_sentence = \"I don't like the movie\"\n","\n","test_data = \" \".join(clean_data(test_sentence.split(\" \"))) # clean the data\n","tokenized_test_data = tokenizer(test_data)  #tokenize the sentence\n","\n","indexed_test_data = [TEXT.vocab.stoi[t] for t in tokenized_test_data]  #convert to integer sequence\n","txt_tensor = torch.LongTensor(indexed_test_data).to(device) #convert to tensor\n","txt_tensor_ip = txt_tensor.unsqueeze(1).T #reshape in form of batch,no. of words\n","\n","length = [len(indexed_test_data)]  #compute no. of words\n","length_tensor_ip = torch.LongTensor(length) #convert to tensor \n","\n","prediction = test_model(txt_tensor_ip) #prediction\n","\n","print(prediction.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PEz_mgNN7S1J","executionInfo":{"status":"ok","timestamp":1651631815720,"user_tz":300,"elapsed":119,"user":{"displayName":"Jaydeep Chakraborty","userId":"04716741579620071152"}},"outputId":"94a58407-493b-4dc0-9054-b3ad82384a03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4924543499946594\n"]}]}]}