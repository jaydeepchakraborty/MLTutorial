{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e630ed93-976f-4a44-ba4e-10e52664e5d6",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051cbe9-eb19-4a76-9916-358b85c22e02",
   "metadata": {},
   "source": [
    "Below are key <b>concepts</b> in RL <br>\n",
    "--> <b>Agent</b> <br>\n",
    "The learner and decision-maker. It takes action based on the interaction with the environment. <br>\n",
    "<br>\n",
    "--> <b> Action </b> <br>\n",
    "Decision taken by agent within the environment. <br>\n",
    "<br>\n",
    "--> <b> Environment </b> <br>\n",
    "World where agent operates in. <br>\n",
    "<br>\n",
    "--> <b> Observation </b> <br>\n",
    "The state of the environment after taking an action by agent. <br>\n",
    "<br>\n",
    "--> <b> Reward </b> <br>\n",
    "Reward (positive/negative) after taking an action by agent. <br>\n",
    "<br>\n",
    "--> <b> Policy </b> <br>\n",
    "It is set of rules which tells agent how to operate in the environment. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67c3a9-c458-4ebd-960c-2af192cd6aaf",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7cuAqjQ97x1H_sBIeAVVZg.png' height=\"500px\" width=\"500px\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b7e27-40bb-4ae3-9fd1-d00e13369242",
   "metadata": {},
   "source": [
    "<b>limitation</b> <br>\n",
    "1) Simple problems RL can be overkill <br>\n",
    "2) Assumes environment is Markovian i.e. future state is not random <br>\n",
    "3) Training takes long time and is not stable <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b98a0a-9836-4968-9da0-e8ec2022e950",
   "metadata": {},
   "source": [
    "<b> Available Packages </b> <br>\n",
    "&nbsp;&nbsp; <b>Reinforcement Learning</b> <br>\n",
    "&nbsp;&nbsp; 1) Stable Baseline [https://stable-baselines3.readthedocs.io/en/master/] <br>\n",
    "&nbsp;&nbsp; 2) TorchRL [https://pytorch.org/rl/] <br>\n",
    "&nbsp;&nbsp; <b>Simulated Environment</b> <br>\n",
    "&nbsp;&nbsp; 1) gym [https://github.com/openai/gym/] <br>\n",
    "&nbsp;&nbsp; 2) gymnasium [https://gymnasium.farama.org/] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74c9be-3ebe-49a0-8eff-b8c0c9e3d65c",
   "metadata": {},
   "source": [
    "<b> Available Algorithms </b> <br>\n",
    "\n",
    "A) Model-Free RL <br>\n",
    "&nbsp;&nbsp; a) Policy Optimization <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 1) Policy Gradient <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2) A2C/A3C <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3) PPO <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4) TRPO <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5) SAC <br>\n",
    "\n",
    "&nbsp;&nbsp; b) Q-Learning <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 1) DQN <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2) QR-DQN <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3) HER <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4) SAC <br>\n",
    "\n",
    "B) Model-Based RL <br>\n",
    "&nbsp;&nbsp; a) Learn the Model <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 1) World Models <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2) I2A <br>\n",
    "\n",
    "&nbsp;&nbsp; b) Given the Model <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 1) AlphaZero <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8f6a4-77cc-41b1-95b3-9aec894e35af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Metrics</b> <br>\n",
    "\n",
    "<b>Stable Baseline3 Metrics</b> <br>\n",
    "1) Evaluation Metrics: <br>\n",
    "\n",
    "1.a) ep_len_mean: <br>\n",
    "mean episode length\n",
    "1.b) ep_rew_mean: <br>\n",
    "mean episode reward\n",
    "\n",
    "2) Time Metrics: <br>\n",
    "\n",
    "2.a) fps: <br>\n",
    "frames per second i.e. how fast it is processing.    \n",
    "2.b) iterations: <br>\n",
    "\n",
    "2.c) time_elapsed: <br>\n",
    "\n",
    "2.d) total_timesteps: <br>\n",
    "\n",
    "3) Train Metric <br>\n",
    "3.a) entropy_loss: <br>\n",
    "\n",
    "3.b) explained_variance: <br>\n",
    "\n",
    "3.c) learning_rate: <br>\n",
    "\n",
    "3.d) loss: <br>\n",
    "\n",
    "3.e) n_updates: <br>\n",
    "\n",
    "3.f) policy_gradient_loss: <br>\n",
    "\n",
    "3.g) value_loss: <br>\n",
    "\n",
    "3.h) approx_kl: <br>\n",
    "\n",
    "3.i) clip_fraction: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f15d53-cb01-40c1-9170-057fa1b23164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406c0c7-39f2-4b61-9645-0f3c9f491c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793bd6e-1294-4ab5-90fb-6d769da649b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae78c0c-38b7-41a9-9704-80558ac3e01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b71e656-a76b-4090-bb11-e4c49fdecf57",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9c9d2-fd34-4b0a-ac01-b32c85567bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) https://www.youtube.com/watch?v=Mut_u40Sqz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e6a5b-7192-4fe8-a0dd-974228e25a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565eac6a-ed5e-4ff9-812f-9899ff115325",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Best resources to check\n",
    "1) https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning\n",
    "2) https://www.kaggle.com/code/aithammadiabdellatif/lux-ai-reinforcement-learning\n",
    "3) https://www.kaggle.com/code/charel/learn-by-example-reinforcement-learning-with-gym\n",
    "4) https://medium.com/@qempsil0914/zero-to-one-deep-q-learning-part1-basic-introduction-and-implementation-bb7602b55a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c084e-b1ec-4689-8dba-5b2e1fe51ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b1299-0601-4a13-8e8d-446946e15194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "1) https://www.kaggle.com/competitions/santas-uncertain-bags/overview\n",
    "2) https://github.com/mohammadasghari/dqn-multi-agent-rl\n",
    "3) https://www.youtube.com/watch?v=R5S2FmtFnt8&list=PLgPe2w6G51vODwbFrMWW-wtTA3EyEXfcI\n",
    "4) https://www.youtube.com/watch?v=psDlXfbe6ok&list=PLd_Oyt6lAQ8RNofJqUduCqC3O0mxcyArM&index=5\n",
    "5) https://www.anylogic.com/upload/pdf/alc-2021/decision-lab-using-reinforcement-learning-to-solve-a-variation-of-the-3d-bin-packing-problem.pdf\n",
    "6) pytorch RL\n",
    "7) https://towardsdatascience.com/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9\n",
    "8) https://github.com/ylchan87/gym-BinPack3D/blob/master/gym_BinPack3D/envs/EnvCheck.ipynb\n",
    "9) https://github.com/alexfrom0815/Online-3D-BPP-DRL/blob/ff0405662fa8f0fb83598dd6138087bbe079597a/acktr/envs.py\n",
    "10) https://www.kaggle.com/code/basu369victor/my-first-attempt-with-reinforcement-learning\n",
    "11) https://arxiv.org/pdf/2006.14978v1.pdf\n",
    "12) https://github.com/alexfrom0815/Online-3D-BPP-DRL\n",
    "13) https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch\n",
    "14) https://iopscience.iop.org/article/10.1088/1742-6596/2181/1/012002/pdf\n",
    "15) https://www.youtube.com/watch?v=myEfUoYrbts\n",
    "16) https://nsidn98.github.io/files/Packman_Presentation.pdf\n",
    "17) https://www.youtube.com/watch?v=xVkPh9E9GfE&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=14\n",
    "18) https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c195adbae0504b6504c93e0fd18235ce/mario_rl_tutorial.ipynb#scrollTo=kfvqYaM93G2a\n",
    "19) https://deeplizard.com/learn/video/nyjbcRQ-uQ8\n",
    "20) https://www.davidsilver.uk/teaching/\n",
    "21) https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf\n",
    "22) https://dl.acm.org/doi/10.1145/3459637.3481933\n",
    "23) http://aaai-rlg.mlanctot.info/2019/papers/AAAI19-RLG-Paper13.pdf\n",
    "24) https://arxiv.org/pdf/2107.04333.pdf\n",
    "25) https://www.semanticscholar.org/reader/04184e01d783031034ecc52d0f953a4520dec368\n",
    "26) https://jonathan-hui.medium.com/rl-tips-on-reinforcement-learning-fbd121111775\n",
    "\n",
    "Applications of BIN packing:\n",
    "Retail packing\n",
    "Truck/Flight loading\n",
    "Placing ads in newspaper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
