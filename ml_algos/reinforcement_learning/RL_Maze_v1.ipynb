{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4829a2-7d4a-44bd-b3ac-4efed526d039",
   "metadata": {},
   "source": [
    "## Maze Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9b3454-cafd-411d-b117-d66c112d65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import random\n",
    "\n",
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1193824-1415-4a65-9b30-9e2a35f19166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, x=0, y=0):\n",
    "        self.name = 'MazeAgent'\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.val = 6 # this is to denote that cell contains the agent\n",
    "        self.action_happened = set()\n",
    "        self.last_action = ''\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Agent:- move: ({self.x} , {self.y}) ~ move happened: {self.action_happened} ~ last action: {self.last_action}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85784fed-937a-402c-bf45-c4c4435dcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    \n",
    "    # method -1\n",
    "    '''\n",
    "    The init method intialize all the variables needed.\n",
    "    '''\n",
    "    def __init__(self, conf):\n",
    "        try:\n",
    "            \n",
    "            self.conf = conf\n",
    "            \n",
    "            # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "            # It describes the numerical structure of the legitimate actions that can be applied to the environment.\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "            \n",
    "            rows = self.conf['env']['rows']\n",
    "            cols = self.conf['env']['cols']\n",
    "            \n",
    "            # observation is the x, y coordinate of the grid - agent's current cell position\n",
    "            # for 4x4 maze, low pos: [0, 0] high pos: [3, 3]\n",
    "            low = np.array([0, 0], dtype=np.int64)\n",
    "            high = np.array([rows-1, cols-1], dtype=np.int64)\n",
    "            self.observation_space = spaces.Box(low, high, shape=(2,), dtype=np.int64)\n",
    "\n",
    "            \n",
    "            # generate environment\n",
    "            self.maze = np.zeros((rows, cols))\n",
    "\n",
    "            # generate dummy env for tracking agent's visited cels\n",
    "            self.visited = np.zeros((rows, cols))\n",
    "\n",
    "            # generate Agent, Agent will always start from (0,0) cell\n",
    "            self.agent = Agent(0, 0)\n",
    "            self._updt_agent_pos(self.agent)\n",
    "\n",
    "            # generate initial state of maze and agent\n",
    "            self._gen_init_cnst_state()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # method -2\n",
    "    '''\n",
    "    The reset method will be called to initiate a new episode. \n",
    "    '''\n",
    "    def reset(self):\n",
    "        try:\n",
    "            rows = self.conf['env']['rows']\n",
    "            cols = self.conf['env']['cols']\n",
    "            \n",
    "            # re-initializing the visited grid\n",
    "            self.visited = np.zeros((rows, cols))\n",
    "           \n",
    "            # replacing agent at (0, 0) cell\n",
    "            self._updt_agent_pos(Agent(0, 0))\n",
    "            \n",
    "            observation = self._get_obs()\n",
    "            \n",
    "            return observation\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "     \n",
    "    # method -3\n",
    "    '''\n",
    "    The step method takes an action as an input and applies it to the environment, \n",
    "    which leads to the environment transitioning to a new state.\n",
    "    action: 'up', 'down', 'right', 'left'\n",
    "    '''\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            # get the direction where agent should move\n",
    "            dir_num, dir_pos = self._action_to_direction(action)\n",
    "\n",
    "            # Whether the episode has been terminated\n",
    "            terminated = False\n",
    "            # The reward that you can get from the environment after executing the action \n",
    "            # that was given as the input to the step function.\n",
    "            reward = 0\n",
    "\n",
    "            # agent's new position\n",
    "            new_x = self.agent.x + dir_pos[0]\n",
    "            new_y = self.agent.y + dir_pos[1]\n",
    "\n",
    "            if not self._chk_pos_validity(new_x, new_y):\n",
    "                reward = -1 \n",
    "                self.agent.action_happened.add(action)\n",
    "                if len(self.agent.action_happened) == 4: # agent can not move any more\n",
    "                    terminated = True   \n",
    "            else:\n",
    "                self._updt_agent_pos(Agent(new_x, new_y))\n",
    "                \n",
    "                terminated = True if self._win() else False\n",
    "                reward = 1 if terminated else 0.001\n",
    "\n",
    "            \n",
    "            self.agent.last_action = dir_num\n",
    "            \n",
    "            # The observation of the state of the environment.\n",
    "            observation = self._get_obs()\n",
    "            \n",
    "            # This provides additional information depending on the environment.\n",
    "            info = self._get_info()\n",
    "            \n",
    "            return observation, reward, terminated, info\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "    \n",
    "    # method -4\n",
    "    '''\n",
    "    The render method is for rendering the environment\n",
    "    '''\n",
    "    def render(self):\n",
    "        try:\n",
    "            self._visualize()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # method -5\n",
    "    '''\n",
    "    The close method should close any open resources that were used by the environment.\n",
    "    '''\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    # translates the environmentâ€™s state into an observation\n",
    "    def _get_obs(self):\n",
    "        try:\n",
    "            return np.array([self.agent.x, self.agent.y], dtype=np.int64)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # auxiliary information\n",
    "    def _get_info(self):\n",
    "        try:\n",
    "            return {\"visited\": self.visited}\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # updating agent position in maze\n",
    "    def _updt_agent_pos(self, agent):\n",
    "        try:\n",
    "            self.agent = agent\n",
    "\n",
    "            self.visited[self.agent.x, self.agent.y] = 1\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # checking agent position is valid or not in maze\n",
    "    def _chk_pos_validity(self,  x, y):\n",
    "        try:\n",
    "            rows = self.conf['env']['rows']\n",
    "            cols = self.conf['env']['cols']\n",
    "\n",
    "            # agent can not visit out of bound, obstacles, already visited cells\n",
    "            if x<0 or y<0 or x>=rows or y>=cols or self.maze[x][y] == -1 or self.visited[x][y] == 1:\n",
    "                return False\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # condition for wining the game\n",
    "    def _win(self):\n",
    "        try:\n",
    "            if self.maze[self.agent.x, self.agent.y] == 1: # agent reached last cell\n",
    "                return True     \n",
    "            return False\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    # It is for initializing maze and agent position\n",
    "    def _gen_init_cnst_state(self):\n",
    "        try:\n",
    "            \n",
    "            rows = self.conf['env']['rows']\n",
    "            cols = self.conf['env']['cols']\n",
    "            \n",
    "            # target will be always at the last cell\n",
    "            self.maze[-1, -1] = 1\n",
    "\n",
    "            # placing obstacles in maze\n",
    "            self.maze[0, 1:3] = -1\n",
    "            self.maze[1, 2:] = -1\n",
    "            self.maze[2, 0] = -1\n",
    "            self.maze[3, 0:2] = -1\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    # It is for visualizing the maze's current position\n",
    "    def _visualize(self):\n",
    "        try:\n",
    "            maze = self.maze.copy()\n",
    "            maze[self.agent.x, self.agent.y] = self.agent.val\n",
    "            print(self.agent)\n",
    "            print(maze)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "    # converting action to direction\n",
    "    def _action_to_direction(self, action_num):\n",
    "        try:\n",
    "            ACTION = [\"down\", \"right\", \"up\", \"left\"]\n",
    "            \n",
    "            \n",
    "            action = {\n",
    "                'down': np.array([1, 0]),\n",
    "                'right': np.array([0, 1]),\n",
    "                'up': np.array([-1, 0]),\n",
    "                'left': np.array([0, -1]),\n",
    "            }\n",
    "            \n",
    "            return ACTION[action_num], action[ACTION[action_num]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ddbfb54-0017-4943-b2a1-8096865114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf():\n",
    "    try:\n",
    "        conf = {\n",
    "            'env':{\n",
    "                    'rows': 4,\n",
    "                    'cols': 4\n",
    "                },\n",
    "            \"model_path\": \"/Users/jaydeepchakraborty/JC/git-projects/model_util/Models/RL/maze_v1/\",\n",
    "            \"model_nm\": \"maze_v1_gym_stable_baselines3\",\n",
    "            \"log_path\": \"/Users/jaydeepchakraborty/JC/git-projects/model_util/logs/RL/maze_v1\"\n",
    "        }       \n",
    "        return conf\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d05763-8fc3-4941-bd86-896d7a438244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env):\n",
    "    try:\n",
    "        # checking using in-built method\n",
    "        check_env(env)\n",
    "        \n",
    "        # checking using manual check\n",
    "        test_episodes = 2\n",
    "\n",
    "        for episode in range(1, test_episodes+1):\n",
    "\n",
    "            obs = env.reset() # initial set of observation\n",
    "            terminated = False\n",
    "            score = 0\n",
    "\n",
    "            env.render()\n",
    "            while not terminated:\n",
    "                action = env.action_space.sample()\n",
    "                nxt_obs, reward, terminated, info = env.step(action)\n",
    "                print(f\"nxt_obs:- {nxt_obs}, reward:- {reward}\")\n",
    "                score += reward\n",
    "                env.render()\n",
    "            \n",
    "            print(f\"after episode:- {episode}, score:- {score}\")\n",
    "\n",
    "        env.close()\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdc8b206-405f-4c49-8767-998b5f1c1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, conf):\n",
    "    try:\n",
    "        print(\"In Train method\")\n",
    "        train_episodes = 15\n",
    "        log_path = conf[\"log_path\"] \n",
    "        # MlpPolicy ~ multilayer perceptron policy\n",
    "        model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=log_path)\n",
    "        for ep in range(train_episodes):\n",
    "            model.learn(total_timesteps=10000, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d64d41-18fd-4f89-8c1a-db646df7bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, conf):\n",
    "    try:\n",
    "        print(\"In Save method\")\n",
    "        model_path = conf[\"model_path\"] + conf[\"model_nm\"] \n",
    "        model.save(model_path)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81f4cdeb-fbf5-4d0a-82e6-8c2fe83677d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(conf):\n",
    "    try:\n",
    "        print(\"In Eval method\")\n",
    "        env = MazeEnv(conf)\n",
    "        \n",
    "        model_path = conf[\"model_path\"] + conf[\"model_nm\"] + \".zip\"\n",
    "        model = PPO.load(model_path, env=env) \n",
    "        \n",
    "        evaluate_policy(model, env, n_eval_episodes=10, render=False)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b471ad-5adc-4e2d-8cec-7ef8c9970278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(conf):\n",
    "    try:\n",
    "        print(\"In Inference method\")\n",
    "        env = MazeEnv(conf)\n",
    "        \n",
    "        model_path = conf[\"model_path\"] + conf[\"model_nm\"] + \".zip\"\n",
    "        model = PPO.load(model_path, env=env)    \n",
    "\n",
    "        terminated = False\n",
    "        \n",
    "        obs = env.reset()\n",
    "        while not terminated:\n",
    "            env.render()\n",
    "            action, _states = model.predict(obs)\n",
    "            action = action.item(0) # predict returns ndarray but model action is dicrete ~ int\n",
    "            obs, rewards, terminated, info = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d730186f-7067-4ad5-8739-e72987d77485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Train method\n",
      "In Save method\n",
      "In Eval method\n",
      "In Inference method\n",
      "Agent:- move: (0 , 0) ~ move happened: set() ~ last action: \n",
      "[[ 6. -1. -1.  0.]\n",
      " [ 0.  0. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (1 , 0) ~ move happened: set() ~ last action: down\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 6.  0. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (1 , 1) ~ move happened: set() ~ last action: right\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  6. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (2 , 1) ~ move happened: set() ~ last action: down\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  0. -1. -1.]\n",
      " [-1.  6.  0.  0.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (2 , 2) ~ move happened: set() ~ last action: right\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  0. -1. -1.]\n",
      " [-1.  0.  6.  0.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (2 , 3) ~ move happened: set() ~ last action: right\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  0. -1. -1.]\n",
      " [-1.  0.  0.  6.]\n",
      " [-1. -1.  0.  1.]]\n",
      "Agent:- move: (3 , 3) ~ move happened: set() ~ last action: down\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  0. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [-1. -1.  0.  6.]]\n",
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydeepchakraborty/JC/git-projects/py_stat/venv/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:71: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # get config\n",
    "        conf = get_conf()\n",
    "        \n",
    "        # Create the environment ~ MAZE\n",
    "        m = MazeEnv(conf)\n",
    "        \n",
    "        # checking whether environment is working or not\n",
    "        # test_env(m) \n",
    "        \n",
    "        # model: train\n",
    "        model = train_model(m, conf)\n",
    "        \n",
    "        # model: save\n",
    "        save_model(model, conf)\n",
    "        \n",
    "        # model: evaluation\n",
    "        eval_model(conf)\n",
    "        \n",
    "        # model: inference\n",
    "        inference_model(conf)\n",
    "        \n",
    "        print(\"DONE\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f414ca-fb8f-4eda-94ce-e379bb59fa90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88c77c5e-9483-4ca2-bf02-018a5770b1a1",
   "metadata": {},
   "source": [
    "# Resources\n",
    "1) https://blog.paperspace.com/getting-started-with-openai-gym/\n",
    "2) https://stable-baselines3.readthedocs.io/en/master/\n",
    "3) https://www.youtube.com/watch?v=uKnjGn8fF70&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1&index=1\n",
    "4) https://www.youtube.com/watch?v=Mut_u40Sqz4\n",
    "5) https://www.youtube.com/watch?v=psDlXfbe6ok&list=PLd_Oyt6lAQ8RNofJqUduCqC3O0mxcyArM&index=6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
