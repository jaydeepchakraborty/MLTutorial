{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb31c110-4ae2-4c3a-ba8a-ac436209e9ad",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Naive Bayes](#naive_bayes)\n",
    "* [Bayes Theorem](#bayes_theorem)\n",
    "* [Different types of Naive Bayes Classifier](#naive_bayes_types)\n",
    "* [Questions](#questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064bbdd3-a6ae-4c3b-b7ce-301224cd5bac",
   "metadata": {},
   "source": [
    "## Naive Bayes <a class=\"anchor\" id=\"naive_bayes\"></a>\n",
    "--> Naive Bayes is a generative model.\n",
    "--> Naive Bayes is a set of supervised learning algorithms based on applying Bayes' theorem. <br>\n",
    "--> Classification, Regression\n",
    "\n",
    "--> <b>Assumptions:</b> <br>\n",
    "features are independent within each class (no co-relation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc53241-a861-4484-9a6c-eaf34b189493",
   "metadata": {},
   "source": [
    "## Bayes Theorem <a class=\"anchor\" id=\"bayes_theorem\"></a>\n",
    "$ \n",
    "\\begin{align}\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "A and B are independent events <br>\n",
    "$ P(A|B) $ - conditional prob, likelihood of A given B happened ~ <b>posterior</b><br>\n",
    "$ P(B|A) $ - conditional prob, likelihood of B given A happened ~ <b>likelihood</b><br>\n",
    "$ P(A) $ - marginal probability, probabilities of A independently ~ <b>prior</b><br>\n",
    "$ P(B) $ - marginal probability, probabilities of B independently ~ <b>evidence</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7de9c-63e9-41b7-8ad0-064862b461ef",
   "metadata": {},
   "source": [
    "If we use the same in-terms of <b>ML usecases.</b> <br>\n",
    "$ \n",
    "\\begin{align}\n",
    "P(y|X) = \\frac{P(X|y) P(y)}{P(X)} \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$y$ is target class. <br>\n",
    "$X$ are features ~ $[x_1, x_2, x_3, ..., x_d]$ <br>\n",
    "$d$ is the number of features\n",
    "<br>\n",
    "\n",
    "let's say, <br>\n",
    "We have three features X = $[x_1, x_2, x_3]$, and <br>\n",
    "there are two target classes $y_1$ and $y_2$<br>\n",
    "then the equation will be, <br>\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "P(y=y_1|X=[x_1,x_2,x_3]) \\\\ \n",
    "& = \\frac{P(X=[x_1,x_2,x_3]|y=y_1) P(y=y_1)}{P(X=[x_1,x_2,x_3])}  \\\\ \n",
    "& = \\frac{P(X=x_1|y=y_1) P(X=x_2|y=y_1) P(X=x_3|y=y_1) P(y=y_1)}{P(X=x_1) P(X=x_2) P(X=x_3)}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "P(y=y_2|X=[x_1,x_2,x_3]) \\\\ \n",
    "& = \\frac{P(X=[x_1,x_2,x_3]|y=y_2) P(y=y_2)}{P(X=[x_1,x_2,x_3])} \\\\ \n",
    "& = \\frac{P(X=x_1|y=y_2) P(X=x_2|y=y_2) P(X=x_3|y=y_2) P(y=y_2)}{P(X=x_1) P(X=x_2) P(X=x_3)} \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Now compare $ P(y=y_1|X=[x_1,x_2,x_3]) $ and $ P(y=y_2|X=[x_1,x_2,x_3]) $ <br>\n",
    "which ever is higher, the $[x_1, x_2, x_3]$ (~ test/inference data) will be assigned to $y_1$ or $y_2$ <br>\n",
    "\n",
    "<br>\n",
    "In case of comparison, the denominator is same, <br>\n",
    "so we can rewrite the above two equations <br>\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "P(y=y_1|X=[x_1,x_2,x_3]) \\propto P(X=x_1|y=y_1) P(X=x_2|y=y_1) P(X=x_3|y=y_1) P(y=y_1)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "P(y=y_2|X=[x_1,x_2,x_3]) \\propto P(X=x_1|y=y_2) P(X=x_2|y=y_2) P(X=x_3|y=y_2) P(y=y_2)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d65a9-7787-4ada-a8a8-d2ad84faad95",
   "metadata": {},
   "source": [
    "So, in concise manner, we can write, <br><br>\n",
    "\n",
    "$K$ number of classes, <br>\n",
    "$d$ number of features <br>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& P(y_k|X) = \\frac{P(X|y_k) P(y_k)}{P(X)} \\\\ \n",
    "& P(y_k|X) = \\frac{P(X = [x_1, x_2, .. ,x_d] |y_k) P(y_k)}{P(X = [x_1, x_2, .., x_d])} \\\\\n",
    "& P(y_k|X) \\propto P(X = [x_1, x_2, .. ,x_d] |y_k) P(y_k) \\\\\n",
    "& P(y_k|X) \\propto P(y_k) P(X = [x_1, x_2, .., x_d] |y_k) \\\\\n",
    "& P(y_k|X) \\propto P(y_k) P(X = x_1 |y_k) P(X = x_2 |y_k) .. P(X = x_d |y_k) \\\\\n",
    "& P(y_k|X) \\propto P(y_k) \\prod_{i=1}^{d} p(x_i | y_k)\\\\\n",
    "\\end{align}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\begin{align}\n",
    "\\hat{y} = \\underset{k \\in {1, .., K}}{\\mathrm{arg\\,max}} P(y_k) \\prod_{i=1}^{d} p(x_i | y_k)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "Here the <b>ASSUMPTION</b> is, <br>\n",
    "the data/features are conditionally independant given the class label. It allows us to write the class conditional density as a <b>product</b>: <br>\n",
    "$ P(y_k|X) \\propto P(y_k) \\prod_{i=1}^{d} p(x_i | y_k) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979a0d6-7618-4a6b-b68c-7632585f3aae",
   "metadata": {},
   "source": [
    "==> maximum a posteriori (MAP) hypothesis. <br>\n",
    "MAP(h) = max(P(h|d)) ~ hypothesis given data <br>\n",
    "$\n",
    "\\begin{align}\n",
    "MAP(h) = {\\mathrm{max}} P(X|y_k) P(y_k)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1c75d-5d64-4de3-95d9-4f0b8797a03a",
   "metadata": {},
   "source": [
    "## Different types of NB classifiers <a class=\"anchor\" id=\"naive_bayes_types\"></a>\n",
    "\n",
    "1) Categorical NB <br>\n",
    "When all the features are categorical. [It also works on continuous data]<br>\n",
    "[NaiveBayesCategorical.ipynb](https://github.com/jaydeepchakraborty/NLP/blob/master/NaiveBayesCategorical.ipynb)\n",
    "\n",
    "\n",
    "2) Gaussian NB <br>\n",
    "When all the features are continuous.<br>\n",
    "[NaiveBayesGaussian.ipynb](https://github.com/jaydeepchakraborty/NLP/blob/master/NaiveBayesGaussian.ipynb)\n",
    "\n",
    "3) Bernoulli NB <br>\n",
    "features are 0 and 1 only <br>\n",
    "Bernoulli is a binary algorithm used when the feature is present or not. <br>\n",
    "[NaiveBayesBernoulli.ipynb](https://github.com/jaydeepchakraborty/NLP/blob/master/NaiveBayesBernoulli.ipynb)\n",
    "\n",
    "\n",
    "Text/NLP ~ word's presence <br>\n",
    "\n",
    "It assumes that all the features are binary such that they take only two values. Means 0s can represent \n",
    "“word does not occur in the document” and 1s as \"word occurs in the document\"\n",
    "\n",
    "4) Multinomial NB <br>\n",
    "features are integers/categorical <br>\n",
    "Multinomial Naïve Bayes consider a feature vector where a given term represents the number of times it appears or very often i.e. frequency.  <br>\n",
    "[NaiveBayesMultinomial.ipynb](https://github.com/jaydeepchakraborty/NLP/blob/master/NaiveBayesMultinomial.ipynb)\n",
    "\n",
    "Text/NLP ~ word's frequency <br>\n",
    "\n",
    "Its is used when we have discrete data (e.g. frequency of words present in the dicument). In text learning \n",
    "we have the count of each word to predict the class or label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284be57-3f5d-448e-8a60-132445d2e385",
   "metadata": {},
   "source": [
    "### Resources\n",
    "1) https://remykarem.github.io/blog/naive-bayes.html\n",
    "2) https://www.youtube.com/watch?v=IvTCdrx1SHQ\n",
    "3) https://github.com/ConsciousML/Naive-Bayes-Classifier-from-scratch/blob/master/Naive%20Bayes%20Classifier.ipynb\n",
    "4) https://machinelearningmastery.com/naive-bayes-for-machine-learning/\n",
    "5) https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd473be1-b499-4ec5-be13-16a648bb3701",
   "metadata": {},
   "source": [
    "## QUESTIONS <a class=\"anchor\" id=\"questions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee2496-6db7-4c99-917d-87115cec7810",
   "metadata": {},
   "source": [
    "1) <b>Is Naive Bayes affected by Imbalanced data, if yes how to resolve it?</b> <br>\n",
    "Yes, naive Bayes is affected by imbalanced data. The posterior probability is \n",
    "badly affected by prior probabilities.\n",
    "\n",
    "<br>\n",
    "\n",
    "The above example the class +ve prior probability will be 998 times higher than the class -ve, this difference in naive bayes creates a bias for class +ve.\n",
    "\n",
    "'P_y1:- 0.001727485630620034' <br>\n",
    "'P_y0:- 0.9982725143693799'\n",
    "\n",
    "One simple solution is to ignore the prior probabilities. <br>\n",
    "undersampling  <br>\n",
    "oversampling.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db91180-ba60-4192-8ee5-f3ec2105b3e8",
   "metadata": {},
   "source": [
    "2) <b>How to combine categorical and numerical </b> <br>\n",
    "https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9a0e3-831f-460d-917b-9a9abb855b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
