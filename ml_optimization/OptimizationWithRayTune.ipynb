{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9350d4-0f2a-4f6f-a479-7dd351f1b3c3",
   "metadata": {},
   "source": [
    "In this Excercise we will work with Ray Tune and explore different options to optimize network hyperparametrs.\n",
    "\n",
    "Ray Tune:\n",
    "===========\n",
    "\n",
    "Tune is a framework/library for distributed hyper-parameter search. Its purpose is to link an optimization algorithm \n",
    "and a trial scheduler together to run asynchronous trials.\n",
    "\n",
    "\n",
    "<b>Basic Concepts:- </b>\n",
    "\n",
    "1) <b>Search space:- </b>\n",
    "define hyperparameters to tune in <br>\n",
    "2) <b>Trainable:- </b>\n",
    "Objective function to tune. Search space is passed to this objective function <br>\n",
    "3) <b>Search Algorithm:- </b>\n",
    "Algorithm which effectively optimize the parameters <br>\n",
    "4) <b>Scheduler:- </b>\n",
    "It is optional. It is used for early stopping and speed up your experiment <br>\n",
    "5) <b>Trials:- </b>\n",
    "Trainable, Search Algorithm and Scheduler are passed to Tuner which runs the experiment and creats trails. <br>\n",
    "6) <b>Analyses:- </b>\n",
    "Trails are passed to analyses to inspect the experiment results. <br>\n",
    "\n",
    "\n",
    "Search space \\ \n",
    " >             \\--> Trainable \\ \n",
    " >                             \\ \n",
    " >           Search Algorithm    -- > Trails --> Analyses \n",
    " >                             / \n",
    " >                  Scheduler / \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b698d-c5a9-42c0-8ff1-1150f235c81f",
   "metadata": {},
   "source": [
    "### Search space\n",
    "\n",
    "config = { <br>\n",
    "    \"uniform\": tune.uniform(-5, -1),  # Uniform float between -5 and -1 <br>\n",
    "    \"quniform\": tune.quniform(3.2, 5.4, 0.2),  # Round to multiples of 0.2 <br>\n",
    "    \"loguniform\": tune.loguniform(1e-4, 1e-1),  # Uniform float in log space <br>\n",
    "    \"qloguniform\": tune.qloguniform(1e-4, 1e-1, 5e-5),  # Round to multiples of 0.00005 <br>\n",
    "    \"randn\": tune.randn(10, 2),  # Normal distribution with mean 10 and sd 2 <br>\n",
    "    \"qrandn\": tune.qrandn(10, 2, 0.2),  # Round to multiples of 0.2 <br>\n",
    "    \"randint\": tune.randint(-9, 15),  # Random integer between -9 and 15 <br>\n",
    "    \"qrandint\": tune.qrandint(-21, 12, 3),  # Round to multiples of 3 (includes 12) <br>\n",
    "    \"lograndint\": tune.lograndint(1, 10),  # Random integer in log space <br>\n",
    "    \"qlograndint\": tune.qlograndint(1, 10, 2),  # Round to multiples of 2 <br>\n",
    "    \"choice\": tune.choice([\"a\", \"b\", \"c\"]),  # Choose one of these options uniformly <br>\n",
    "    \"func\": tune.sample_from( lambda spec: spec.config.uniform * 0.01 ),  # Depends on other value <br>\n",
    "    \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)) # Return random integers from low (inclusive) to high (exclusive). <br>\n",
    "    \"grid\": tune.grid_search([32, 64, 128]),  # Search over all these values <br>\n",
    "} <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bff2a2-d7ed-485d-9a72-fdcf5804e254",
   "metadata": {},
   "source": [
    "### Trainable\n",
    "\n",
    "--> Trainable is an object that is passed to tune.\n",
    "--> There are two options.\n",
    "#### a) Function API:- \n",
    "\n",
    "\n",
    "```\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "def trainable_func(config: dict):\n",
    "    intermediate_score = 0\n",
    "    for x in range(20):\n",
    "        intermediate_score = objective(x, config[\"a\"], config[\"b\"])\n",
    "        session.report({\"score\": intermediate_score})  # This sends the score to Tune.\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(trainable_func, param_space={\"a\": 2, \"b\": 4})\n",
    "results = tuner.fit()  \n",
    "```\n",
    "\n",
    "#### b) Class API:- \n",
    "\n",
    "```\n",
    "from ray import air, tune\n",
    "\n",
    "class TrainableCls(tune.Trainable):\n",
    "    def setup(self, config: dict):\n",
    "        # config (dict): A dict of hyperparameters\n",
    "        self.x = 0\n",
    "        self.a = config['a']\n",
    "        self.b = config['b']\n",
    "\n",
    "    def step(self):  # This is called iteratively.\n",
    "        score = objective(self.x, self.a, self.b)\n",
    "        self.x += 1\n",
    "        return {\"score\": score}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    TrainableCls,\n",
    "    run_config=air.RunConfig(\n",
    "        # Train for 20 steps\n",
    "        stop={\"training_iteration\": 20},\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            # We haven't implemented checkpointing yet. See below!\n",
    "            checkpoint_at_end=False\n",
    "        ),\n",
    "    ),\n",
    "    param_space={\"a\": 2, \"b\": 4},\n",
    ")\n",
    "results = tuner.fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6309b-f4a2-4cb5-8cca-e5783ee80512",
   "metadata": {},
   "source": [
    "### Search Algorithm\n",
    "\n",
    "--> Search Algorithm suggests hyperparameter configurations. If nothing is specified random search is used by default.\n",
    "\n",
    "--> Different Search Algorithms\n",
    "1) BasicVariantGenerator\n",
    "The BasicVariantGenerator is used per default if no search algorithm is passed to Tuner. It is mainly used for Random search and grid search.\n",
    "\n",
    "```\n",
    "from ray import tune\n",
    "\n",
    "# This will automatically use the `BasicVariantGenerator`\n",
    "tuner = tune.Tuner(\n",
    "    lambda config: config[\"a\"] + config[\"b\"],\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=4\n",
    "    ),\n",
    "    param_space={\n",
    "        \"a\": tune.grid_search([1, 2]),\n",
    "        \"b\": tune.randint(0, 3)\n",
    "    },\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "In the example above, 8 trials will be generated: For each sample (4), each of the grid search variants for a will \n",
    "be sampled once. The b parameter will be sampled randomly.\n",
    "\n",
    "\n",
    "2) AxSearch\n",
    "\n",
    "Ax is a platform for understanding, managing, deploying, and automating adaptive experiments. Ax provides an easy to use \n",
    "interface with BoTorch, a flexible, modern library for Bayesian optimization in PyTorch.\n",
    "\n",
    "\n",
    "```\n",
    "from ray import tune\n",
    "from ray.tune.search.ax import AxSearch\n",
    "\n",
    "config = {\n",
    "    \"x1\": tune.uniform(0.0, 1.0),\n",
    "    \"x2\": tune.uniform(0.0, 1.0)\n",
    "}\n",
    "\n",
    "def easy_objective(config):\n",
    "        result = train_model(config[\"x1\"] , config[\"x2\"])\n",
    "        tune.report(score=result)\n",
    "\n",
    "ax_search = AxSearch(metric=\"score\", mode=\"max\")\n",
    "tuner = tune.Tuner(\n",
    "    easy_objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=ax_search,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "\n",
    "3) BayesOptSearch\n",
    "\n",
    "BayesOpt is a constrained global optimization package utilizing Bayesian inference on gaussian processes, where the emphasis is on finding the maximum value of \n",
    "an unknown function in as few iterations as possible. \n",
    "\n",
    "```\n",
    "# pip install bayesian-optimization\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray import air\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 20)}\n",
    "\n",
    "algo = BayesOptSearch(random_search_steps=4)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_func,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"score\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    run_config=air.RunConfig(stop={\"training_iteration\": 20}),\n",
    "    param_space=search_space,\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "```\n",
    "Repeater to average over multiple evaluations of the same hyperparameter configurations. \n",
    "This is useful in cases where the evaluated training procedure has high variance (i.e., in reinforcement learning).\n",
    "\n",
    "from ray.tune.search import Repeater\n",
    "\n",
    "search_alg = BayesOptSearch(...)\n",
    "re_search_alg = Repeater(search_alg, repeat=10)\n",
    "\n",
    "# Repeat 2 samples 10 times each.\n",
    "tuner = tune.Tuner(\n",
    "    trainable,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=re_search_alg,\n",
    "        num_samples=20,\n",
    "    ),\n",
    ")\n",
    "tuner.fit()\n",
    "\n",
    "```\n",
    "\n",
    "4) Optuna\n",
    "\n",
    "```\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "config = {\n",
    "    \"a\": tune.uniform(6, 8)\n",
    "    \"b\": tune.loguniform(1e-4, 1e-2)\n",
    "}\n",
    "\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "5) Scikit-Optimize\n",
    "\n",
    "```\n",
    "config = {\n",
    "    \"width\": tune.uniform(0, 20),\n",
    "    \"height\": tune.uniform(-100, 100)\n",
    "}\n",
    "\n",
    "skopt_search = SkOptSearch(\n",
    "    metric=\"mean_loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_function,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=skopt_search\n",
    "    ),\n",
    "    param_space=config\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "6) HyperOpt\n",
    "\n",
    "```\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 20)}\n",
    "\n",
    "algo = HyperOptSearch()\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_func,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"score\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "tuner.fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe871e0f-396b-4313-b17f-cfbf86e1041b",
   "metadata": {},
   "source": [
    "### Scheduler\n",
    "\n",
    "--> schedulers can stop, pause, or tweak the hyperparameters of running trials, potentially making your hyperparameter tuning process much faster.\n",
    "\n",
    "```\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "# Create HyperBand scheduler and minimize the score\n",
    "hyperband = HyperBandScheduler(metric=\"score\", mode=\"max\")\n",
    "\n",
    "config = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 1)}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_func,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=20,\n",
    "        scheduler=hyperband,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "tuner.fit()\n",
    "```\n",
    "\n",
    "--> Other example of schedulers are <br>\n",
    "1) ASHAScheduler\n",
    "2) HyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9922ca-7137-4766-b1dc-875e185758ce",
   "metadata": {},
   "source": [
    "### Trials\n",
    "--> Tuner.fit execute and manage hyperparameters tuning and generate trials\n",
    "Example_1:\n",
    "space = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 1)}\n",
    "tuner = tune.Tuner(\n",
    "    trainable_func, \n",
    "    param_space=space, \n",
    "    tune_config=tune.TuneConfig(num_samples=10)\n",
    ")\n",
    "tuner.fit()\n",
    "\n",
    "1) trainable_func --> the function with config parameter\n",
    "2) param_space --> the hyperparameter search space\n",
    "3) TuneConfig: <br>\n",
    "    3.a) <b>num_samples</b> - total number of trials to run. Tune automatically determines how many trials will run in parallel\n",
    "    You can specify the cores. <br>\n",
    "    3.b) <b>max_concurrent_trials</b> - It specifies the max number of trials to run concurrently\n",
    "4) resources:\n",
    "    it specifies resources ~ cpu/gpu to run the trials\n",
    "  \n",
    "```\n",
    "trainable_with_resources = tune.with_resources(trainable_func,\n",
    "    resources=lambda spec: {\"gpu\": 1} if spec.config.use_gpu else {\"cpu\": 4})\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_resources,\n",
    "    tune_config=tune.TuneConfig(\n",
    "                                num_samples=10,\n",
    "                                max_concurrent_trials=10,)\n",
    ")\n",
    "results = tuner.fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a3df5-5d3e-450f-848a-c41ab28f0185",
   "metadata": {},
   "source": [
    "### Analyses\n",
    "\n",
    "--> Tuner.fit() returns an ResultGrid object which has methods you can use for analyzing your training.\n",
    "\n",
    "```\n",
    "tuner = tune.Tuner(\n",
    "    trainable_func,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"score\",\n",
    "        mode=\"min\",\n",
    "        search_alg=BayesOptSearch(random_search_steps=4),\n",
    "    ),\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\"training_iteration\": 20},\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result()  # Get best result object\n",
    "best_config = best_result.config  # Get best trial's hyperparameters\n",
    "best_logdir = best_result.log_dir  # Get best trial's logdir\n",
    "best_checkpoint = best_result.checkpoint  # Get best trial's best checkpoint\n",
    "best_metrics = best_result.metrics  # Get best trial's last results\n",
    "best_result_df = best_result.metrics_dataframe  # Get best result as pandas dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73540c0b-d6ae-499f-8f46-155f0b210172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "import pickle5 as pickle\n",
    "\n",
    "import os\n",
    "from ray.tune import TuneConfig\n",
    "\n",
    "import numpy as np\n",
    "from ray.air import session\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray import air\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.bayesopt import BayesOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd814dfc-8ee6-4f4b-8748-617dc1179cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom pytorch model\n",
    "'''\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        super(Net, self).__init__()\n",
    "        self.net_layer = nn.Sequential(OrderedDict([\n",
    "            ('1_LINEAR_LAYER', nn.Linear(8, 4)),\n",
    "            ('2_LINEAR_LAYER', nn.Linear(4, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        layer_1 = self.net_layer._modules['1_LINEAR_LAYER']\n",
    "        layer_1_op = layer_1(x)\n",
    "        \n",
    "        layer_2 = self.net_layer._modules['2_LINEAR_LAYER']\n",
    "        layer_2_op = layer_2(layer_1_op)\n",
    "        \n",
    "        op = torch.sigmoid(layer_2_op)\n",
    "        \n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62ef17-b91f-4956-832a-176b2c393533",
   "metadata": {},
   "source": [
    "## Data load <a class=\"anchor\" id=\"data_load\"></a>\n",
    "<b>Data:</b> <br>\n",
    "https://www.kaggle.com/datasets/whenamancodes/predict-diabities <br>\n",
    "-- selected three features ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'] <br>\n",
    "-- target is 'Outcome' ~ Diabetes (1 - yes), (0 - no) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3227463b-fa12-4c0c-9386-7bdc5e1b8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf():\n",
    "    try:\n",
    "        conf = {\n",
    "            \"data_fl_path\": \"../DataSets/diabetes.csv\",\n",
    "            \"data_ratio\":{\n",
    "                \"train_ratio\":0.8,\n",
    "                \"test_ratio\":0.1,\n",
    "                \"valid_ratio\":0.1 \n",
    "            },\n",
    "            \"device\": \"cpu\",\n",
    "            \"epochs\": 10,\n",
    "            \"lr\": 0.00001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"model_nm\": \"diabetes_model.pth\",\n",
    "            \"model_path\": \"../Models/\"\n",
    "        }     \n",
    "        return conf\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3998d79b-a19f-4f23-976c-13570313dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values,dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25d8b5e-35a0-4235-aaa3-806002d9be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEngine:\n",
    "    def __init__(self, conf):\n",
    "        self.df = pd.read_csv(conf['data_fl_path'])\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "    def split_data(self, conf):\n",
    "        try:\n",
    "            X = self.df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "            y = self.df[['Outcome']]\n",
    "\n",
    "            X_train, X_rem, y_train, y_rem = train_test_split(X,y, stratify=y, train_size=conf['data_ratio']['train_ratio']) #0.8\n",
    "\n",
    "            X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, stratify=y_rem, test_size=0.5) #0.1, 0.1\n",
    "\n",
    "            print(\"X_train: \" , X_train.shape, \"y_train: \" , y_train.shape)\n",
    "            print(\"X_valid: \" , X_valid.shape, \"y_valid: \" , y_valid.shape)\n",
    "            print(\"X_test: \" , X_test.shape, \"y_test: \" , y_test.shape)\n",
    "\n",
    "            data = {\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"X_valid\": X_valid,\n",
    "                \"y_valid\": y_valid,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_test\": y_test\n",
    "            }\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8520fb-5daa-409d-a90b-5f7b0b54bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEngine:\n",
    "    def __init__(self, conf):\n",
    "        self.net = Net(conf)\n",
    "        self.net.to(conf[\"device\"])\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr=conf[\"lr\"], momentum=conf[\"momentum\"])\n",
    "        \n",
    "    def loss_fn(self, outputs, targets):\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        return loss\n",
    "    \n",
    "    def train(self, db_dataloader, conf):     \n",
    "        # move the model into train mode\n",
    "        self.net.train()  \n",
    "        train_loss = 0.0\n",
    "        for X_sample, y_sample in db_dataloader:\n",
    "            X_sample, y_sample = X_sample.to(conf[\"device\"]), y_sample.to(conf[\"device\"])\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.net(X_sample)\n",
    "            \n",
    "            loss = self.loss_fn(outputs, y_sample)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = round(train_loss/len(db_dataloader), 5)\n",
    "        \n",
    "        return train_loss\n",
    "        \n",
    "    def evaluate(self, db_dataloader, conf):\n",
    "        # move the model into eval mode\n",
    "        self.net.eval()\n",
    "        test_loss = 0.0\n",
    "        for X_sample, y_sample in db_dataloader:\n",
    "\n",
    "            X_sample, y_sample = X_sample.to(conf[\"device\"]), y_sample.to(conf[\"device\"])\n",
    "\n",
    "            outputs = self.net(X_sample)\n",
    "\n",
    "            loss = self.loss_fn(outputs, y_sample)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "        test_loss = round(test_loss/len(db_dataloader), 5)\n",
    "        \n",
    "        return test_loss\n",
    "    \n",
    "    def save_model(self, conf, ind):\n",
    "        torch.save(self.net.state_dict(), ind+conf['model_nm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0260b8c3-81b5-4e41-9999-9491247937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config,\n",
    "               db_train_dataloader=None, db_valid_dataloader=None, conf=None):\n",
    "    try:\n",
    "        conf[\"lr\"] = config[\"lr\"]\n",
    "        conf[\"momentum\"] = config[\"momentum\"]\n",
    "        modelEngg = ModelEngine(conf)\n",
    "        total_loss = 0\n",
    "        for epoch in range(conf[\"epochs\"]+1):\n",
    "            train_loss = modelEngg.train(db_train_dataloader, conf)\n",
    "            valid_loss = modelEngg.evaluate(db_valid_dataloader, conf)\n",
    "            \n",
    "            print(f'Epoch:- {epoch}  TrainLoss:- {train_loss}  ValidLoss:- {valid_loss}')\n",
    "            total_loss += train_loss\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                # This saves the model to the trial directory\n",
    "                ind = \"epoch_\"+str(epoch)+\"_\"\n",
    "                modelEngg.save_model(conf, ind)\n",
    "        \n",
    "        # This saves the model to the trial directory\n",
    "        ind = \"final_\"\n",
    "        modelEngg.save_model(conf, ind) \n",
    "        # tune.report(loss=total_loss/conf[\"epochs\"])\n",
    "        session.report({\"loss\": total_loss/conf[\"epochs\"]})\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3242a9-96ec-4a9c-9083-2b5393a95f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainableCls(tune.Trainable):\n",
    "    \n",
    "#     # setup function is invoked once training starts.\n",
    "#     def setup(self, config: dict, db_train_dataloader=None, db_valid_dataloader=None, conf=None):\n",
    "#         # config (dict): A dict of hyperparameters\n",
    "#         self.db_train_dataloader = db_train_dataloader\n",
    "#         self.db_valid_dataloader = db_valid_dataloader\n",
    "#         self.conf = conf\n",
    "#         self.lr = config[\"lr\"]\n",
    "\n",
    "#     # step function is called iteratively. Each time, the Trainable object executes one logical iteration of training in the tuning process\n",
    "#     def step(self):\n",
    "#         self.conf[\"lr\"] = self.lr\n",
    "#         loss = train_func(self.db_train_dataloader, self.db_valid_dataloader, self.conf) #objective function\n",
    "#         return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e15f27-bb57-442d-8749-df415ddc02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tune_random(db_train_dataloader, db_valid_dataloader, conf):\n",
    "    try:\n",
    "        # defining search space\n",
    "        # config = {\n",
    "        #     \"lr\": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand()))\n",
    "        # }\n",
    "        \n",
    "        config = {\n",
    "            \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "            \"momentum\": tune.choice([0.7, 0.8, 0.9])\n",
    "        }\n",
    "        \n",
    "        # defining search algorithm and metric\n",
    "        # bayesopt = BayesOptSearch(random_search_steps=4)\n",
    "        \n",
    "        scheduler = ASHAScheduler(\n",
    "                        metric=\"loss\",\n",
    "                        mode=\"min\",\n",
    "                        max_t=10,\n",
    "                        grace_period=1,\n",
    "                        reduction_factor=2)\n",
    "        \n",
    "        reporter = CLIReporter(\n",
    "                # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "                metric_columns=[\"loss\", \"training_iteration\"])\n",
    "        \n",
    "        result = tune.run(\n",
    "            partial(train_func, db_train_dataloader=db_train_dataloader, db_valid_dataloader=db_valid_dataloader, conf=conf),\n",
    "            config=config,\n",
    "            num_samples=10,\n",
    "            scheduler=scheduler,\n",
    "            progress_reporter=reporter\n",
    "        )\n",
    "\n",
    "        # results = tuner.fit()\n",
    "        \n",
    "        # best_trial = results.get_best_result() \n",
    "        best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        \n",
    "        return best_trial\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3f73d2-d8af-447f-8323-c4eb4d50406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tune_bayes_opt(db_train_dataloader, db_valid_dataloader, conf):\n",
    "    try:        \n",
    "        param_space = {\n",
    "            \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "            \"momentum\": tune.uniform(0.6, 0.9),\n",
    "        }\n",
    "        bayesopt = BayesOptSearch(metric=\"loss\", mode=\"min\")\n",
    "        tune_config = TuneConfig(\n",
    "                            max_concurrent_trials=10,\n",
    "                            num_samples=10,\n",
    "                            search_alg=bayesopt,\n",
    "                        )\n",
    "        \n",
    "        tuner = Tuner(\n",
    "                trainable=tune.with_parameters(train_func, \n",
    "                                               db_train_dataloader=db_train_dataloader, \n",
    "                                               db_valid_dataloader=db_valid_dataloader, \n",
    "                                               conf=conf),\n",
    "                run_config=RunConfig(name=\"bayesopt_tuner\", local_dir=\"~/ray_results\"),\n",
    "                param_space=param_space,\n",
    "                tune_config=tune_config\n",
    "            )\n",
    "\n",
    "        results = tuner.fit()\n",
    "        \n",
    "        best_trial = results.get_best_result(metric=\"loss\", mode=\"min\") \n",
    "        # best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        \n",
    "        return best_trial\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91239199-8ddc-47c9-a024-f86b7df47c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(best_trial, conf):\n",
    "    try:\n",
    "        logdir = best_trial.log_dir\n",
    "        \n",
    "        ind = \"final_\"\n",
    "        state_dict = torch.load(os.path.join(logdir, ind+conf['model_nm']))\n",
    "        \n",
    "        conf[\"lr\"] = best_trial.config[\"lr\"]\n",
    "        conf[\"momentum\"] = best_trial.config[\"momentum\"]\n",
    "        \n",
    "        net = Net(conf)\n",
    "        net.load_state_dict(state_dict)\n",
    "        \n",
    "        torch.save(net.state_dict(), conf['model_path']+conf['model_nm'])\n",
    "        \n",
    "        origin = os.path.join(logdir, \"params.json\")\n",
    "        target = conf['model_path']+\"params.json\"\n",
    "        # copying best parameters\n",
    "        shutil.copy(origin, target)\n",
    "        \n",
    "        origin = os.path.join(logdir, \"params.pkl\")\n",
    "        target = conf['model_path']+\"params.pkl\"\n",
    "        # copying best parameters\n",
    "        shutil.copy(origin, target)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec48e42-f4ca-48ad-8f05-5b7414a4c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(db_test_dataloader, conf):\n",
    "    try:\n",
    "        conf[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        state_dict = torch.load(conf['model_path']+conf['model_nm'])\n",
    "        \n",
    "        with open(conf['model_path']+'params.json', 'r') as f:\n",
    "            best_result = json.load(f)\n",
    "        \n",
    "        conf[\"lr\"] = best_result[\"lr\"]\n",
    "        conf[\"momentum\"] = best_result[\"momentum\"]\n",
    "        \n",
    "        print(conf)\n",
    "        \n",
    "        modelEngg = ModelEngine(conf)\n",
    "        \n",
    "        test_loss = modelEngg.evaluate(db_test_dataloader, conf)\n",
    "        \n",
    "        print(f'test_loss:- {test_loss}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a3bbd2-8cec-444e-9db5-325bf65c7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # get conf\n",
    "        conf = get_conf()\n",
    "        conf[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # loading the data\n",
    "        dataEng = DataEngine(conf)\n",
    "        \n",
    "        # splitting the data\n",
    "        db_df_split = dataEng.split_data(conf)\n",
    "        \n",
    "        # converting data to dataloder\n",
    "        db_train_data = DiabetesDS(db_df_split[\"X_train\"], db_df_split[\"y_train\"])\n",
    "        db_train_dataloader = DataLoader(db_train_data, batch_size=5, shuffle=False, num_workers=1)\n",
    "        \n",
    "        db_valid_data = DiabetesDS(db_df_split[\"X_valid\"], db_df_split[\"y_valid\"])\n",
    "        db_valid_dataloader = DataLoader(db_valid_data, batch_size=5, shuffle=False, num_workers=1)\n",
    "        \n",
    "        db_test_data = DiabetesDS(db_df_split[\"X_test\"], db_df_split[\"y_test\"])\n",
    "        db_test_dataloader = DataLoader(db_test_data, batch_size=5, shuffle=False, num_workers=1)\n",
    "        \n",
    "        \n",
    "        # training the model\n",
    "        # train_func(db_train_dataloader, db_valid_dataloader, conf) \n",
    "        \n",
    "        # hyper parameter tuning\n",
    "        # best_trial = hyperparam_tune_random(db_train_dataloader, db_valid_dataloader, conf)\n",
    "        best_trial = hyperparam_tune_bayes_opt(db_train_dataloader, db_valid_dataloader, conf)\n",
    "        \n",
    "        # saving the best model for further use\n",
    "        save_best_model(best_trial, conf)\n",
    "        \n",
    "        # test with best model\n",
    "        test_func(db_test_dataloader, conf)\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4253ee0f-bccf-44a9-8304-e74f52a19226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (614, 8) y_train:  (614, 1)\n",
      "X_valid:  (77, 8) y_valid:  (77, 1)\n",
      "X_test:  (77, 8) y_test:  (77, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 14:54:24,061\tWARNING bayesopt_search.py:425 -- BayesOpt does not support specific sampling methods. The LogUniform sampler will be dropped.\n",
      "2023-01-21 14:54:24,069\tWARNING bayesopt_search.py:425 -- BayesOpt does not support specific sampling methods. The Uniform sampler will be dropped.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-21 14:55:16</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:52.82        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.0/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/5.12 GiB heap, 0.0/2.0 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_func_c92dec0c</td><td>TERMINATED</td><td>127.0.0.1:31339</td><td style=\"text-align: right;\">0.00380795 </td><td style=\"text-align: right;\">  0.885214</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.51219</td><td style=\"text-align: right;\">38.5495  </td></tr>\n",
       "<tr><td>train_func_d0c767ea</td><td>TERMINATED</td><td>127.0.0.1:31347</td><td style=\"text-align: right;\">0.00734674 </td><td style=\"text-align: right;\">  0.779598</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.88843</td><td style=\"text-align: right;\">37.9517  </td></tr>\n",
       "<tr><td>train_func_d55edfae</td><td>TERMINATED</td><td>127.0.0.1:31375</td><td style=\"text-align: right;\">0.00164458 </td><td style=\"text-align: right;\">  0.646798</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.1439 </td><td style=\"text-align: right;\"> 0.888123</td></tr>\n",
       "<tr><td>train_func_d5698aa8</td><td>TERMINATED</td><td>127.0.0.1:31339</td><td style=\"text-align: right;\">0.000675028</td><td style=\"text-align: right;\">  0.859853</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.2805 </td><td style=\"text-align: right;\"> 0.892541</td></tr>\n",
       "<tr><td>train_func_d57396ce</td><td>TERMINATED</td><td>127.0.0.1:31377</td><td style=\"text-align: right;\">0.00605104 </td><td style=\"text-align: right;\">  0.812422</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5841 </td><td style=\"text-align: right;\">70.6311  </td></tr>\n",
       "<tr><td>train_func_d57902bc</td><td>TERMINATED</td><td>127.0.0.1:31347</td><td style=\"text-align: right;\">0.000303786</td><td style=\"text-align: right;\">  0.890973</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.0359 </td><td style=\"text-align: right;\"> 1.00031 </td></tr>\n",
       "<tr><td>train_func_dc52e7ec</td><td>TERMINATED</td><td>127.0.0.1:31339</td><td style=\"text-align: right;\">0.00834118 </td><td style=\"text-align: right;\">  0.663702</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.1355 </td><td style=\"text-align: right;\">38.2234  </td></tr>\n",
       "<tr><td>train_func_dc5f58e2</td><td>TERMINATED</td><td>127.0.0.1:31347</td><td style=\"text-align: right;\">0.00190007 </td><td style=\"text-align: right;\">  0.655021</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.74702</td><td style=\"text-align: right;\"> 0.866063</td></tr>\n",
       "<tr><td>train_func_e2f70dee</td><td>TERMINATED</td><td>127.0.0.1:31339</td><td style=\"text-align: right;\">0.003112   </td><td style=\"text-align: right;\">  0.757427</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.76063</td><td style=\"text-align: right;\"> 0.869702</td></tr>\n",
       "<tr><td>train_func_e3137c22</td><td>TERMINATED</td><td>127.0.0.1:31375</td><td style=\"text-align: right;\">0.00437626 </td><td style=\"text-align: right;\">  0.687369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.48505</td><td style=\"text-align: right;\"> 0.987656</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 0  TrainLoss:- 37.12057  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 1  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 2  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 3  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 4  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 5  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 6  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 7  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 8  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 9  TrainLoss:- 34.8374  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 10  TrainLoss:- 34.8374  ValidLoss:- 37.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag              </th><th>hostname                  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     loss</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_func_c92dec0c</td><td>2023-01-21_14-54-43</td><td>True  </td><td>                </td><td>83178d67ea1f40a8aef669c1784e2718</td><td>1_lr=0.0038,momentum=0.8852 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">38.5495  </td><td>127.0.0.1</td><td style=\"text-align: right;\">31339</td><td style=\"text-align: right;\">             6.51219</td><td style=\"text-align: right;\">           6.51219</td><td style=\"text-align: right;\">       6.51219</td><td style=\"text-align: right;\"> 1674334483</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>c92dec0c  </td><td style=\"text-align: right;\">   0.00585079</td></tr>\n",
       "<tr><td>train_func_d0c767ea</td><td>2023-01-21_14-54-54</td><td>True  </td><td>                </td><td>cf2fd35482ac43d59248027f8e81275f</td><td>2_lr=0.0073,momentum=0.7796 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">37.9517  </td><td>127.0.0.1</td><td style=\"text-align: right;\">31347</td><td style=\"text-align: right;\">             9.88843</td><td style=\"text-align: right;\">           9.88843</td><td style=\"text-align: right;\">       9.88843</td><td style=\"text-align: right;\"> 1674334494</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d0c767ea  </td><td style=\"text-align: right;\">   0.00598717</td></tr>\n",
       "<tr><td>train_func_d55edfae</td><td>2023-01-21_14-55-09</td><td>True  </td><td>                </td><td>3ba37c720b3e439aa55810e20a4d3163</td><td>3_lr=0.0016,momentum=0.6468 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 0.888123</td><td>127.0.0.1</td><td style=\"text-align: right;\">31375</td><td style=\"text-align: right;\">            13.1439 </td><td style=\"text-align: right;\">          13.1439 </td><td style=\"text-align: right;\">      13.1439 </td><td style=\"text-align: right;\"> 1674334509</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d55edfae  </td><td style=\"text-align: right;\">   0.00909495</td></tr>\n",
       "<tr><td>train_func_d5698aa8</td><td>2023-01-21_14-54-52</td><td>True  </td><td>                </td><td>83178d67ea1f40a8aef669c1784e2718</td><td>4_lr=0.0007,momentum=0.8599 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 0.892541</td><td>127.0.0.1</td><td style=\"text-align: right;\">31339</td><td style=\"text-align: right;\">             8.2805 </td><td style=\"text-align: right;\">           8.2805 </td><td style=\"text-align: right;\">       8.2805 </td><td style=\"text-align: right;\"> 1674334492</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d5698aa8  </td><td style=\"text-align: right;\">   0.00585079</td></tr>\n",
       "<tr><td>train_func_d57396ce</td><td>2023-01-21_14-55-09</td><td>True  </td><td>                </td><td>7780d34da54341cab9176f20dd8f921a</td><td>5_lr=0.0061,momentum=0.8124 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">70.6311  </td><td>127.0.0.1</td><td style=\"text-align: right;\">31377</td><td style=\"text-align: right;\">            13.5841 </td><td style=\"text-align: right;\">          13.5841 </td><td style=\"text-align: right;\">      13.5841 </td><td style=\"text-align: right;\"> 1674334509</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d57396ce  </td><td style=\"text-align: right;\">   0.00447702</td></tr>\n",
       "<tr><td>train_func_d57902bc</td><td>2023-01-21_14-55-07</td><td>True  </td><td>                </td><td>cf2fd35482ac43d59248027f8e81275f</td><td>6_lr=0.0003,momentum=0.8910 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 1.00031 </td><td>127.0.0.1</td><td style=\"text-align: right;\">31347</td><td style=\"text-align: right;\">            11.0359 </td><td style=\"text-align: right;\">          11.0359 </td><td style=\"text-align: right;\">      11.0359 </td><td style=\"text-align: right;\"> 1674334507</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d57902bc  </td><td style=\"text-align: right;\">   0.00598717</td></tr>\n",
       "<tr><td>train_func_dc52e7ec</td><td>2023-01-21_14-55-07</td><td>True  </td><td>                </td><td>83178d67ea1f40a8aef669c1784e2718</td><td>7_lr=0.0083,momentum=0.6637 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">38.2234  </td><td>127.0.0.1</td><td style=\"text-align: right;\">31339</td><td style=\"text-align: right;\">            11.1355 </td><td style=\"text-align: right;\">          11.1355 </td><td style=\"text-align: right;\">      11.1355 </td><td style=\"text-align: right;\"> 1674334507</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>dc52e7ec  </td><td style=\"text-align: right;\">   0.00585079</td></tr>\n",
       "<tr><td>train_func_dc5f58e2</td><td>2023-01-21_14-55-16</td><td>True  </td><td>                </td><td>cf2fd35482ac43d59248027f8e81275f</td><td>8_lr=0.0019,momentum=0.6550 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 0.866063</td><td>127.0.0.1</td><td style=\"text-align: right;\">31347</td><td style=\"text-align: right;\">             8.74702</td><td style=\"text-align: right;\">           8.74702</td><td style=\"text-align: right;\">       8.74702</td><td style=\"text-align: right;\"> 1674334516</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>dc5f58e2  </td><td style=\"text-align: right;\">   0.00598717</td></tr>\n",
       "<tr><td>train_func_e2f70dee</td><td>2023-01-21_14-55-16</td><td>True  </td><td>                </td><td>83178d67ea1f40a8aef669c1784e2718</td><td>9_lr=0.0031,momentum=0.7574 </td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 0.869702</td><td>127.0.0.1</td><td style=\"text-align: right;\">31339</td><td style=\"text-align: right;\">             8.76063</td><td style=\"text-align: right;\">           8.76063</td><td style=\"text-align: right;\">       8.76063</td><td style=\"text-align: right;\"> 1674334516</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>e2f70dee  </td><td style=\"text-align: right;\">   0.00585079</td></tr>\n",
       "<tr><td>train_func_e3137c22</td><td>2023-01-21_14-55-16</td><td>True  </td><td>                </td><td>3ba37c720b3e439aa55810e20a4d3163</td><td>10_lr=0.0044,momentum=0.6874</td><td>JAYDEEPs-MacBook-Pro.local</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 0.987656</td><td>127.0.0.1</td><td style=\"text-align: right;\">31375</td><td style=\"text-align: right;\">             7.48505</td><td style=\"text-align: right;\">           7.48505</td><td style=\"text-align: right;\">       7.48505</td><td style=\"text-align: right;\"> 1674334516</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>e3137c22  </td><td style=\"text-align: right;\">   0.00909495</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 0  TrainLoss:- 2.68961  ValidLoss:- 0.6147\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 1  TrainLoss:- 0.63117  ValidLoss:- 0.61167\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 2  TrainLoss:- 0.62833  ValidLoss:- 0.61231\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 0  TrainLoss:- 32.17545  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 3  TrainLoss:- 0.62682  ValidLoss:- 0.61203\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 1  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 4  TrainLoss:- 0.62545  ValidLoss:- 0.61153\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 2  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 5  TrainLoss:- 0.62406  ValidLoss:- 0.61104\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 3  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 6  TrainLoss:- 0.62266  ValidLoss:- 0.61058\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 4  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 7  TrainLoss:- 0.62128  ValidLoss:- 0.61015\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 5  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 8  TrainLoss:- 0.61995  ValidLoss:- 0.60972\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 6  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 9  TrainLoss:- 0.61866  ValidLoss:- 0.6093\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 7  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 10  TrainLoss:- 0.61742  ValidLoss:- 0.60889\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 8  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 9  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 10  TrainLoss:- 34.73417  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 0  TrainLoss:- 34.71633  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 0  TrainLoss:- 3.45041  ValidLoss:- 0.63438\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 1  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 1  TrainLoss:- 0.69101  ValidLoss:- 0.66169\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 0  TrainLoss:- 2.40219  ValidLoss:- 0.60544\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 2  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 2  TrainLoss:- 0.6721  ValidLoss:- 0.65048\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 0  TrainLoss:- 54.68459  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 1  TrainLoss:- 0.65977  ValidLoss:- 0.60556\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 3  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 3  TrainLoss:- 0.6617  ValidLoss:- 0.6322\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 1  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 2  TrainLoss:- 0.65503  ValidLoss:- 0.60666\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 4  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 4  TrainLoss:- 0.65519  ValidLoss:- 0.62362\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 2  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 3  TrainLoss:- 0.65246  ValidLoss:- 0.60726\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 5  TrainLoss:- 0.65056  ValidLoss:- 0.62139\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 5  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 3  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 4  TrainLoss:- 0.65024  ValidLoss:- 0.60761\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 6  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 6  TrainLoss:- 0.64739  ValidLoss:- 0.62224\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 4  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 5  TrainLoss:- 0.64821  ValidLoss:- 0.60785\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 7  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 7  TrainLoss:- 0.64547  ValidLoss:- 0.62261\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 5  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 6  TrainLoss:- 0.64628  ValidLoss:- 0.60798\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 8  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 6  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 8  TrainLoss:- 0.64438  ValidLoss:- 0.62094\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 7  TrainLoss:- 0.64441  ValidLoss:- 0.60804\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 7  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 9  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 9  TrainLoss:- 0.6432  ValidLoss:- 0.6193\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 8  TrainLoss:- 0.6426  ValidLoss:- 0.60805\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 10  TrainLoss:- 0.6417  ValidLoss:- 0.61855\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 8  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 10  TrainLoss:- 34.75173  ValidLoss:- 37.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 0  TrainLoss:- 2.22775  ValidLoss:- 0.64148\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 9  TrainLoss:- 0.64086  ValidLoss:- 0.60803\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 0  TrainLoss:- 2.06788  ValidLoss:- 0.68576\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 9  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 10  TrainLoss:- 0.63918  ValidLoss:- 0.60801\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 1  TrainLoss:- 0.65877  ValidLoss:- 0.63876\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 1  TrainLoss:- 0.70387  ValidLoss:- 0.65786\n",
      "\u001b[2m\u001b[36m(train_func pid=31377)\u001b[0m Epoch:- 10  TrainLoss:- 65.1626  ValidLoss:- 62.5\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 0  TrainLoss:- 2.93955  ValidLoss:- 0.79705\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 2  TrainLoss:- 0.64829  ValidLoss:- 0.63965\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 2  TrainLoss:- 0.69059  ValidLoss:- 0.65051\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 1  TrainLoss:- 0.77858  ValidLoss:- 0.67511\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 3  TrainLoss:- 0.64579  ValidLoss:- 0.63588\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 3  TrainLoss:- 0.67717  ValidLoss:- 0.64331\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 2  TrainLoss:- 0.74287  ValidLoss:- 0.67958\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 4  TrainLoss:- 0.64452  ValidLoss:- 0.63398\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 4  TrainLoss:- 0.66674  ValidLoss:- 0.63979\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 3  TrainLoss:- 0.71404  ValidLoss:- 0.6737\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 5  TrainLoss:- 0.64306  ValidLoss:- 0.63254\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 5  TrainLoss:- 0.6592  ValidLoss:- 0.63771\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 4  TrainLoss:- 0.69273  ValidLoss:- 0.65942\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 6  TrainLoss:- 0.65354  ValidLoss:- 0.63686\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 6  TrainLoss:- 0.64147  ValidLoss:- 0.63126\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 5  TrainLoss:- 0.67918  ValidLoss:- 0.6493\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 7  TrainLoss:- 0.63991  ValidLoss:- 0.6301\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 7  TrainLoss:- 0.6491  ValidLoss:- 0.63712\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 6  TrainLoss:- 0.67243  ValidLoss:- 0.64634\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 8  TrainLoss:- 0.63841  ValidLoss:- 0.62904\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 8  TrainLoss:- 0.64552  ValidLoss:- 0.63772\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 7  TrainLoss:- 0.66783  ValidLoss:- 0.64857\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 9  TrainLoss:- 0.637  ValidLoss:- 0.62807\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 9  TrainLoss:- 0.64273  ValidLoss:- 0.63819\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 8  TrainLoss:- 0.6657  ValidLoss:- 0.65586\n",
      "\u001b[2m\u001b[36m(train_func pid=31347)\u001b[0m Epoch:- 10  TrainLoss:- 0.63566  ValidLoss:- 0.6272\n",
      "\u001b[2m\u001b[36m(train_func pid=31339)\u001b[0m Epoch:- 10  TrainLoss:- 0.64068  ValidLoss:- 0.63843\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 9  TrainLoss:- 0.6631  ValidLoss:- 0.65805\n",
      "\u001b[2m\u001b[36m(train_func pid=31375)\u001b[0m Epoch:- 10  TrainLoss:- 0.66055  ValidLoss:- 0.65439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 14:55:17,170\tINFO tune.py:778 -- Total run time: 53.17 seconds (52.79 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.0019000671753502962, 'momentum': 0.6550213529560301}\n",
      "{'data_fl_path': '../DataSets/diabetes.csv', 'data_ratio': {'train_ratio': 0.8, 'test_ratio': 0.1, 'valid_ratio': 0.1}, 'device': device(type='cpu'), 'epochs': 10, 'lr': 0.0019000671753502962, 'momentum': 0.6550213529560301, 'model_nm': 'diabetes_model.pth', 'model_path': '../Models/'}\n",
      "test_loss:- 50.16075\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afa53d-477f-435e-9018-4da35820cc7a",
   "metadata": {},
   "source": [
    "## Resources\n",
    "1) https://docs.ray.io/en/latest/tune/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
