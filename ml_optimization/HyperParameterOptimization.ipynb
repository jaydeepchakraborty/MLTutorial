{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9cf7de-6ef5-4724-b3fe-8e9cb0653cf3",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ad60-b50d-4567-a119-3e48b5fa0268",
   "metadata": {},
   "source": [
    "1) Model parameter:- \n",
    "It is what model learns from the data. e.g. Weights, biases etc. <br>\n",
    "2) Model hyperparameter:- <br>\n",
    "Manually enters to tune model. e.g. learning rate, number of layers, number of units per layer, type of regularization, scheduler etc. <br>\n",
    "3) Optimization:-  <br>\n",
    "A process of adjusting hyperparameters in order to minimize the cost function by using one of the optimization techniques. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edb0f6-d3ee-465c-872f-e9a2a44ea348",
   "metadata": {},
   "source": [
    "### Why hyperparameter optimization is difficult?\n",
    "1) time-consuming\n",
    "2) expensive model training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10861e72-637e-4496-9c68-92495d61fe8b",
   "metadata": {},
   "source": [
    "### Optimization methods\n",
    "<b>1) Manual tuning </b> <br>\n",
    "It depends on developer's knowledge. <br>\n",
    "\n",
    "<b>2) Grid Search </b> <br>\n",
    "In grid search we choose a set of values for each parameter and the set of trials is formed by assembling every possible combination of values. It is simple to implement. It is computationally expensive. <br>\n",
    "\n",
    "<b>3) Random Search </b> <br>\n",
    "In random search, random combinations of the hyperparameters are used to find the best solution. It randomly samples from a grid of hyperparameters instead of conducting an exhaustive search. Here, we can specify the number of total runs the random search should try before returning the best model. Here each new guess is independent of the previous iteration and therefore the search is incapable of leveraging learning for improvement<br>\n",
    "\n",
    "<b>4) Bayesian optimization </b> <br>\n",
    "It is sequential model-based optimization, is a powerful tool for the joint optimization of hyperparameters. <br>\n",
    "\n",
    "(max or min) $f(x) , x \\in X$\n",
    "\n",
    "--> $x$ is input(hyperparameter) $x \\in R^{d}$, d<20 <br>\n",
    "--> $X$ is feasible set (domain). <br>\n",
    "--> $f$ is continuous. <br>\n",
    "--> $f$ is time-consuming and expensive to evaluate. <br>\n",
    "\n",
    "It overcomes the drawbacks of random search by exploring search spaces in more efficient manner. Although Bayesian optimization performs better than grid search and random search, in its naive form it has two main drawbacks: it is not a parallel algorithm and it works <b>only on continuous hyper-parameters</b> and not categorical ones. <br>\n",
    "Here every search trial considers previous trials. So, based on the prior data, it estimates the next set of hyperparameters. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& P(score|hyperparameters) = \\frac{P(hyperparameters|score) * P(score)}{P(hyperparameters)} \\\\\n",
    "\\end{align}\n",
    "$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d64dd-9da7-4ab1-aef1-d7af331ba62d",
   "metadata": {},
   "source": [
    "Some pointers: <br>\n",
    "1) first have a baseline model\n",
    "2) seperate data into training, validation, and testing set\n",
    "3) prevent overfitting: early stopping, regularization\n",
    "4) set up entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfb7d1-12c1-4f88-a368-ea81c9fd84fc",
   "metadata": {},
   "source": [
    "### Tools available for model hyperparameter tuning\n",
    "1) Optuna <br>\n",
    "2) RayTune <br>\n",
    "[OptimizationWithRayTune.ipynb](https://github.com/jaydeepchakraborty/NLP/blob/master/OptimizationWithRayTune.ipynb) <br>\n",
    "3) Ax <br>\n",
    "https://ax.dev/tutorials/tune_cnn.html <br>\n",
    "4) BoTorch <br>\n",
    "https://botorch.org/tutorials/ <br>\n",
    "5) GPyTorch <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919faea7-432d-418c-879c-b6537a7ab353",
   "metadata": {},
   "source": [
    "## Resources\n",
    "1) https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization <br>\n",
    "2) https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49 <br>\n",
    "3) Algorithms for Hyper-Parameter Optimization <br>\n",
    "https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf <br>\n",
    "4) https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/ <br>\n",
    "5) https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html <br>\n",
    "6) https://www.youtube.com/watch?v=RublDm4J1vY <br>\n",
    "7) https://arxiv.org/pdf/1912.05686.pdf <br>\n",
    "8) https://github.com/google-research/tuning_playbook?fbclid=IwAR15cJEgYfYGlkfm2JRnFueNN6wo20qNy1bFu7N_25dpuS_HOEGJOa8lsIE <br>\n",
    "9) https://druce.ai/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna <br>\n",
    "10) https://www.reddit.com/r/MachineLearning/comments/b10hmx/d_hyperopt_vs_tune_vs_ray/ <br>\n",
    "11) https://www.youtube.com/watch?v=4MK_OJJ82YI <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13eac27-ea85-4915-bf74-4c6d436c93a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
