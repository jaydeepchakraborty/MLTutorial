{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a841e7-076f-432c-9130-ce3373c6fca0",
   "metadata": {},
   "source": [
    "Defining cost/loss function is very critical in model training. The main purpose of backpropagation during training process is to reduce the defined cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf43cf-8e41-4c36-bc05-9f802259f689",
   "metadata": {},
   "source": [
    "1) Loss <br>\n",
    "&emsp;     a) Regression <br>\n",
    "&emsp;&emsp;         i)   Mean Absolute Error (L1 loss) <br>\n",
    "&emsp;&emsp;         ii)  Mean Squared Error (L2 loss) <br>\n",
    "&emsp;&emsp;         iii) Root Mean Squared Error <br>\n",
    "&emsp;&emsp;         iV) Huber loss <br>\n",
    "&emsp;    b) Classification <br>\n",
    "&emsp;&emsp;        i)   Cross Entropy (Binary, Multiclass, Multilabel)<br>\n",
    "&emsp;&emsp;        ii)  KL divergence <br>\n",
    "&emsp;&emsp;        iii) Hinge loss <br>\n",
    "&emsp;&emsp;        iv)  Focal loss <br>\n",
    "&emsp;    c) Ranking <br>\n",
    "&emsp;&emsp;        i)   Margin Ranking loss<br>\n",
    "&emsp;&emsp;        ii)  Triplet Ranking loss <br>\n",
    "2) Metrics <br>\n",
    "&emsp;    a) Regression <br>\n",
    "&emsp;    b) Classification <br>\n",
    "&emsp;&emsp;        i)   Acuuracy, Precision, Recall, F1, Sensitivity, Specificity, AUC<br>\n",
    "&emsp;    b) Ranking <br>\n",
    "&emsp;&emsp;        i)   MAP (Mean Average Precision)<br>\n",
    "&emsp;&emsp;        ii)   NDCG (Normalized Discounted Cumulative Gain)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c87c392-a6a8-43a1-a095-9c1206273439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryHingeLoss\n",
    "from torchmetrics.classification import MulticlassHingeLoss\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb7104-acae-48f9-b648-1ff12cbacb77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c57faf-9770-4908-95df-02265526e179",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e3c1c-6773-47c3-a944-13ab75ba55bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2799c-23b0-425b-9096-64a63276ce1f",
   "metadata": {},
   "source": [
    "--> It is also called L1 loss. It computes the average of absolute differences between actual values and predicted values. \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& L = \\frac{1}{n}\\sum_{i=1}^{n}|\\hat{y_i}-y_i| \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8a8486-19eb-4b30-82a2-93dd50731c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([1.2000, 1.9000, 0.0000], dtype=torch.float64)\n",
      "output:  tensor(0.7345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.2,  1.9, 0.0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a8e211-f092-41e2-964a-dd2baf87c8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
      "        [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
      "        [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
      "        [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
      "        [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]], dtype=torch.float64)\n",
      "output:  tensor(1.1340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## if we have multiple/multilevel regression output \n",
    "\n",
    "pred = np.array([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
    "                [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
    "                [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
    "                   [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
    "                   [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d429a-01d9-4c0b-a1ed-ad0edaf56c42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### MSE (Mean Squared Error) (Quadratic loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253564dc-d96a-4bd5-8358-4a75ee61034d",
   "metadata": {},
   "source": [
    "--> It is also called L2 loss. It computes the average of the squared differences between actual values and predicted values. \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& L = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_i}-y_i)^2 \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbcaf2d-02b8-4f9f-868e-575db3cb0aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([1.2000, 1.9000, 0.0000], dtype=torch.float64)\n",
      "output:  tensor(0.6359, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.2,  1.9, 0.0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d28e84c-af93-446c-a3df-41b278e75479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
      "        [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
      "        [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
      "        [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
      "        [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]], dtype=torch.float64)\n",
      "output:  tensor(1.8773, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## if we have multiple/multilevel regression output \n",
    "\n",
    "pred = np.array([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
    "                [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
    "                [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
    "                   [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
    "                   [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaffffe-6907-458b-b83e-d15a3200db38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RMSE (Root Mean Squared Error)\n",
    "--> PyTorch does not have RMSE Loss, one simple way to implement is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74431177-60bf-4b4f-a1f1-83c06b0a9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        #  if the mse=0, there will be issue during the backward pass as you multiply 0 by infinity (derivative of sqrt at 0).\n",
    "        mse_val = self.mse(yhat,y)\n",
    "        mse_val = (mse_val + self.eps) if mse_val == 0 else mse_val\n",
    "        loss = torch.sqrt(mse_val)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b810c198-e527-4f09-b546-621f512c2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([1.2000, 1.9000, 0.0000], dtype=torch.float64)\n",
      "output:  tensor(0.7974, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.2,  1.9, 0.0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = RMSELoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6af93e4-7263-4525-b450-a1afa7d5e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
      "        [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
      "        [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
      "        [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
      "        [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]], dtype=torch.float64)\n",
      "output:  tensor(1.3701, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## if we have multiple/multilevel regression output \n",
    "\n",
    "pred = np.array([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
    "                [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
    "                [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
    "                   [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
    "                   [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = RMSELoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940eb6b-2f1e-4b3f-8cc2-da5a860943c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Huber Loss (Smooth MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a293f-5eae-4e27-ae14-2d5aeb8afaf7",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& L_{\\delta}=\n",
    "&    \\left\\{\\begin{matrix}\n",
    "&        \\frac{1}{2}(y - \\hat{y})^{2} & if \\left | (y - \\hat{y})  \\right | < \\delta\\\\\n",
    "&        \\delta ((y - \\hat{y}) - \\frac1 2 \\delta) & otherwise\n",
    "&    \\end{matrix}\\right. \\\\\n",
    "& L - \\text{loss} \\frac{1}{n} \\sum_{i=1}^{N} L_{\\delta i} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81806c61-7454-40aa-a003-37c3026d8141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([1.2000, 1.9000, 0.0000], dtype=torch.float64)\n",
      "output:  tensor(0.3129, dtype=torch.float64, grad_fn=<HuberLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.2,  1.9, 0.0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4aacc8d-5737-4bb8-84ee-df23c466093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
      "        [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
      "        [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "target:  tensor([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
      "        [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
      "        [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]], dtype=torch.float64)\n",
      "output:  tensor(0.7171, dtype=torch.float64, grad_fn=<HuberLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## if we have multiple/multilevel regression output \n",
    "\n",
    "pred = np.array([[ 0.1158,  0.6110,  1.4007,  0.9448, -1.3474],\n",
    "                [ 0.5122,  1.9972, -0.4636,  0.4942, -0.7275],\n",
    "                [ 0.7347,  1.0200,  0.6460, -1.0006,  0.5116]])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([[-0.0830, -0.7957,  1.0230, -1.5763,  0.3064],\n",
    "                   [ 0.0061, -0.0928,  0.4557, -1.2883,  0.0917],\n",
    "                   [-0.3364, -1.3713,  0.1138, -0.3415,  0.4300]])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10b01b-b506-4631-b16b-2412c8816ad8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Quantile loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ee05aa-7e68-4079-b9ca-2db0f968d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd886c0-6110-43d3-8a3a-cdd9edb5725b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c35c5d-2dbe-4346-a856-63c56cfbc130",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece21eb6-d619-451f-acac-211d9d74f923",
   "metadata": {},
   "source": [
    "--> In machine learning, entropy is a measure of randomness in information. High entropy means there is too much noise/randomness in the information, <br>\n",
    "so that we can not conclude any decision from the information. <br>\n",
    "--> Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. <br>\n",
    "--> Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. <br>\n",
    "--> The logistic loss is sometimes called cross-entropy loss. It is also known as log loss. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f11c70-55f2-4e7a-9557-8a9e134cece6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e71ab0-8d02-4777-b4ac-4626ef717583",
   "metadata": {},
   "source": [
    "--> It is used when we have two classes. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c8caa-ee55-43df-ad93-16871388c80a",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& L = - \\frac{1}{N} \\sum_{i=1}^{N} {(y_{i}\\log(\\hat{y_{i}}) + (1 - y_{i})\\log(1 - \\hat{y_{i}}))} \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c88f1e-f6f2-4713-9c0f-2ff95fd2bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([0.8482, 0.9558, 0.3753], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "target:  tensor([1., 1., 0.], dtype=torch.float64)\n",
      "output:  tensor(0.2268, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "pred = m(pred)\n",
    "\n",
    "target = np.array([1.,  1., 0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5fbec40-a063-4208-b172-f2c66f0c1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([1., 1., 0.], dtype=torch.float64)\n",
      "output:  tensor(0.2268, dtype=torch.float64,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.,  1., 0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss() # no need to use sigmoid - it is combination of a Sigmoid layer and the BCELoss in one single class\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d490bd-57c8-4174-b6b6-004623ddecd9",
   "metadata": {},
   "source": [
    "<b> Q) Why log is used in BCE? </b> <br>\n",
    "The probability of an output or the likelihood function is <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& \\hat{y_{i}}^{y_i}(1-\\hat{y_{i}})^{(1-y_{i})}\n",
    "\\end{align}\n",
    "$ \n",
    "<br>\n",
    "$y_{i}$ is encoded as 0 or 1. To calculate the maximum likelihood, product is difficult to compute. <br>\n",
    "Using log to transfer the product into summation. It is easier. <br>\n",
    "\n",
    "Another reason is, <br>\n",
    "The log value offers less penalty for small differences between predicted probability and corrected probability. when the difference is large the penalty will be higher.\n",
    "$\n",
    "log(0) = \\text{infinite/undefined} \\\\\n",
    "log(1) = 0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df718f88-7ed8-41a2-94d8-9480b0e12467",
   "metadata": {},
   "source": [
    "<b> Q) Why negative sign in BCE </b> <br>\n",
    "probabilities lie between 0 and 1, all the log values are negative. In order to compensate for this negative value, we will use a negative average of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a22d6d-7fef-4812-ba47-3d0baf085c6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Multi-Class Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ef9ba-ae51-48cb-9431-bebeaaf3121c",
   "metadata": {},
   "source": [
    "--> It is used when we have multiple classes. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d2ee3-9cdb-4f9e-a690-2627312db692",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} {(y_{i,c}\\log(\\hat{y_{i,c}})} \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples} \\\\\n",
    "& C - \\text{total number of classes}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bafc39b-3a12-4b81-9763-b2f610cc166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[-1.1425, -1.0835, -1.0713],\n",
      "        [-1.9806, -3.1442, -0.1998],\n",
      "        [-0.7065, -0.6992, -4.6438],\n",
      "        [-0.6962, -1.3050, -1.4680]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "target:  tensor([2, 1, 1, 0])\n",
      "predicted:  tensor([[-1.1425, -1.0835, -1.0713],\n",
      "        [-1.9806, -3.1442, -0.1998],\n",
      "        [-0.7065, -0.6992, -4.6438],\n",
      "        [-0.6962, -1.3050, -1.4680]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "target:  tensor([2, 1, 1, 0])\n",
      "output:  tensor(1.4027, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "predicted:  tensor([2, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# nsamples X nclasses\n",
    "# this is very important. let's say we have 3 classes - 0, 1, 2\n",
    "# From Deep learning model, \n",
    "#  the idea is, at the very last layer, it should produce 3 values (same as number of classes)\n",
    "#   \n",
    "pred = np.array([\n",
    "        [-0.2412, -0.1822, -0.1700],\n",
    "        [ 0.3150, -0.8486,  2.0958],\n",
    "        [ -0.2112, -0.2039, -4.1485],\n",
    "        [0.9562, 0.3474, 0.1844]\n",
    "    ])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "pred = m(pred)\n",
    "\n",
    "print('predicted: ', pred)\n",
    "\n",
    "target = np.array([2.,  1., 1., 0])\n",
    "target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "print('target: ', target)\n",
    "\n",
    "loss = nn.NLLLoss()\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "# during inference we have to use torch max\n",
    "_, inf_pred = torch.max(pred, 1)\n",
    "print('predicted: ', inf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "158656f5-bc16-4f25-a481-9a8dd034e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[-0.2412, -0.1822, -0.1700],\n",
      "        [ 0.3150, -0.8486,  2.0958],\n",
      "        [-0.2112, -0.2039, -4.1485],\n",
      "        [ 0.9562,  0.3474,  0.1844]], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([2, 1, 1, 0])\n",
      "predicted:  tensor([[-0.2412, -0.1822, -0.1700],\n",
      "        [ 0.3150, -0.8486,  2.0958],\n",
      "        [-0.2112, -0.2039, -4.1485],\n",
      "        [ 0.9562,  0.3474,  0.1844]], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([2, 1, 1, 0])\n",
      "output:  tensor(1.4027, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "predicted:  tensor([2, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# nsamples X nclasses\n",
    "# this is very important. let's say we have 3 classes - 0, 1, 2\n",
    "# From Deep learning model, \n",
    "#  the idea is, at the very last layer, it should produce 3 values (same as number of classes)\n",
    "#   \n",
    "pred = np.array([\n",
    "        [-0.2412, -0.1822, -0.1700],\n",
    "        [ 0.3150, -0.8486,  2.0958],\n",
    "        [ -0.2112, -0.2039, -4.1485],\n",
    "        [0.9562, 0.3474, 0.1844]\n",
    "    ])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "print('predicted: ', pred)\n",
    "\n",
    "target = np.array([2.,  1., 1., 0])\n",
    "target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "print('target: ', target)\n",
    "\n",
    "loss = nn.CrossEntropyLoss() # no need to have softmax, - it is combination of a Sigmoid layer and the NLLLoss (negative log likelihood loss) in one single class\n",
    "output = loss(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "# during inference we have to use torch max\n",
    "_, inf_pred = torch.max(pred, 1)\n",
    "print('predicted: ', inf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642271d-5100-4b78-9f24-8757446dda06",
   "metadata": {},
   "source": [
    "<b> Q) Difference between Sigmoid and Softmax </b> <br>\n",
    "\n",
    "Sigmoid:  <br>\n",
    "It squashes a vector in the range (0, 1). It is applied independently to each element of $s$ -- $s_{i}$. It’s also called logistic function.  <br>\n",
    "$\n",
    "\\sigma(s_{i}) = \\frac{1} {1 + e^{-s_{i}}}\n",
    "$\n",
    "<br>\n",
    "\n",
    "Softmax: <br>\n",
    "It squashes a vector in the range (0, 1) and all the resulting elements add up to 1. It is applied to the output scores s. As elements represent a class, they can be interpreted as class probabilities.  <br>\n",
    "$\n",
    "\\sigma(s_{i}) = \\frac{e^{s_{i}}}{\\sum_{c=1}^C e^{s_{i,c}}} \\ \\ \\ for\\ c=1,2,\\dots,C\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69840c-25ae-4a64-836e-99b85a5e78e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Multi-Labels Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef703571-936e-4ab1-85ea-479133cd853d",
   "metadata": {},
   "source": [
    "--> In multi-label problem, the class can be represented as one-hot code vector.\n",
    "--> We can use BCE loss for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94e04cf3-b4ef-45de-9fb6-756bf29e818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:-  1.628549 1.628549\n",
      "tensor([[1., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### batch_size/num_of_samples = 3\n",
    "### num_classes = 5\n",
    "\n",
    "pred_before_sigmoid = np.array([\n",
    "        [ 1.4397182 , -0.7993438 ,  4.113389  ,  3.2199187 ,  4.5777845 ],\n",
    "        [ 0.30619335,  0.10168511,  4.253479  ,  2.3782277 ,  4.7390924 ],\n",
    "        [ 1.124632  ,  1.6056736 ,  2.9778094 ,  2.0808482 ,  2.0735667 ]\n",
    "    ])\n",
    "pred_before_sigmoid = torch.tensor(pred_before_sigmoid, requires_grad=True)\n",
    "\n",
    "pred_after_sigmoid = torch.sigmoid(pred_before_sigmoid)\n",
    "\n",
    "\n",
    "target_classes = np.array([\n",
    "        [1., 1., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 1.],\n",
    "        [1., 1., 1., 1., 0.]\n",
    "    ])\n",
    "target_classes = torch.tensor(target_classes, dtype=torch.double)\n",
    "\n",
    "\n",
    "\n",
    "fn_bce_loss = torch.nn.BCELoss()\n",
    "bce_loss = fn_bce_loss(pred_after_sigmoid, target_classes)\n",
    "\n",
    "fn_bce_loss_logits = torch.nn.BCEWithLogitsLoss()\n",
    "bce_loss_logit = fn_bce_loss_logits(pred_before_sigmoid, target_classes)\n",
    "\n",
    "\n",
    "bce_loss = round(bce_loss.detach().item(), 6)\n",
    "bce_loss_logit = round(bce_loss_logit.detach().item(), 6)\n",
    "\n",
    "print(\"Loss:- \",bce_loss, bce_loss_logit)\n",
    "\n",
    "assert bce_loss == bce_loss_logit\n",
    "\n",
    "prediction = (pred_after_sigmoid > 0.7).type(torch.float64)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cdb34-dc54-4608-ba11-cb1b2bb797c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb4c8d-2554-4f60-b198-67f0f912e01c",
   "metadata": {},
   "source": [
    "--> KL divergence is different from Cross Entropy. It calculates relative (average) entropy between two probability distributions. <br>\n",
    "--> The KL divergence between two probability distributions measures how different the two distributions are. <br>\n",
    "--> When two probability distributions are exactly similar, then the KL divergence between them is 0. <br>\n",
    " <br>\n",
    "For example, let’s say that we have a true distribution 𝑃 and an approximate distribution 𝑄. Then KL divergence will calculate the similarity  <br>\n",
    "(or dissimilarity) between the two probability distributions.  <br>\n",
    "\n",
    "$\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in \\chi}^{} P(x) log{\\frac{P(X)}{Q(X)}}\n",
    "$ <br>\n",
    "where $\\chi$ is the probability space. <br>\n",
    "--> We need to keep in mind that although KL divergence tells us how one probability distribution is different from another, it is not a distance metric. That is, it does not calculate the distance between the probability distributions 𝑃 and 𝑄. <br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Use cases of KL divergence: <br>\n",
    "    1) Autoencoder,  Variational Autoencoders <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "32d217ff-9b1c-47da-b232-9b24ef503909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence of p1 and p2 = 0.511531182594443\n",
      "KL Divergence of p1 and p3 = 0.2787421654654177\n"
     ]
    }
   ],
   "source": [
    "p1 = np.array([\n",
    "        [-1.0156, -0.2873, -0.4043,  0.2866,  0.0977],\n",
    "        [-1.1921,  0.4708,  2.1700, -0.1780, -0.2064],\n",
    "        [ 1.5237, -0.5784, -0.4806,  1.5462, -1.4848]\n",
    "    ])\n",
    "p1 = torch.tensor(p1, requires_grad=True)\n",
    "p2 = np.array([\n",
    "        [0.4866, 0.6380, 0.0370, 0.3784, 0.5492],\n",
    "        [0.4328, 0.9565, 0.5474, 0.4046, 0.2326],\n",
    "        [0.4962, 0.7836, 0.2754, 0.3386, 0.6686]\n",
    "    ])\n",
    "p2 = torch.tensor(p2)\n",
    "p3 = np.array([\n",
    "        [0.5202, 0.7551, 0.7836, 0.0691, 0.4091],\n",
    "        [-1.1921,  0.4708,  2.1700, -0.1780, -0.2064],\n",
    "        [0.9314, 0.4530, 0.7055, 0.0060, 0.4177]\n",
    "    ])\n",
    "p3 = torch.tensor(p3)\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "p1 = F.log_softmax(p1, dim=1) # prediction should be a distribution in the log space\n",
    "\n",
    "p2 = F.log_softmax(p2, dim=1) # target\n",
    "p3 = F.log_softmax(p3, dim=1) # target\n",
    "\n",
    "output_1 = kl_loss(p1, p2)\n",
    "output_2 = kl_loss(p1, p3)\n",
    "\n",
    "print('KL Divergence of p1 and p2 = {}'.format(output_1))\n",
    "print('KL Divergence of p1 and p3 = {}'.format(output_2))\n",
    "\n",
    "# p1-p3 are more similar distribution compare to p1-p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bed6ca-1a2b-466f-b566-dfc8df45357c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570726f-1cea-48a1-91c6-0e6f2fc5642d",
   "metadata": {},
   "source": [
    "--> It is very particular to <b>SVM algorithm.</b>. Mainly used for maximum margin classification models. <br>\n",
    "--> It includes margin or distance from the classification. Even if new observations are classified correctly, still penalty will be incured if the margin from the decision boundary is not large enugh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf798bda-d483-4c4f-976d-658e80b263d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Binary Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f688544-1cf1-419d-9a15-f4753f6c9b02",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& L = \\sum_{i=1}^{N} max(0, 1 - y_{i} \\cdot \\hat{y_{i}}) \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\in -1, 1\\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3c5c52-f5ab-41f1-afb6-a946e40b9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([0.5622, 0.5622, 0.6341, 0.6792, 0.6792], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "target:  tensor([0, 0, 1, 1, 1])\n",
      "output:  tensor(0.8264, grad_fn=<SqueezeBackward0>)\n",
      "predicted:  tensor([0.5622, 0.5622, 0.6341, 0.6792, 0.6792], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "target:  tensor([0, 0, 1, 1, 1])\n",
      "output:  tensor(1.0441, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([0.25, 0.25, 0.55, 0.75, 0.75])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "pred = m(pred)\n",
    "\n",
    "target = np.array([0, 0, 1, 1, 1])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = BinaryHingeLoss() # Binary Hinge loss\n",
    "output = loss(pred, target)\n",
    "# output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "loss_squared = BinaryHingeLoss(squared=True) # Binary squared Hinge loss\n",
    "output = loss_squared(pred, target)\n",
    "output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4424a0-77e3-4136-a21f-1a416d6e8e2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Multi-Class Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b86e7e-181d-4b02-919f-e9bb63469b51",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& L = \\sum_{i=1}^{N} max(0, 1 - \\hat{y_{i,y}} + \\max_{i \\neq y}{(\\hat{y_{i,y})}}) \\\\\n",
    "& L - \\text{loss} \\\\\n",
    "& i - i^{th} \\text{ training sample} \\\\\n",
    "& y_{i} - \\text{actual value of the } i^{th} \\text{training sample} \\in -1, 1\\\\\n",
    "& \\hat{y_{i}} - \\text{predicted value of the } i^{th} \\text{training sample} \\\\\n",
    "& N - \\text{total number of training samples} \\\\\n",
    "& y \\in 0, .., C is the target class\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e78ca0-ff06-40d4-b49f-685d9db63842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  tensor([[0.5622, 0.5498, 0.6341],\n",
      "        [0.6341, 0.5125, 0.5987],\n",
      "        [0.5250, 0.5744, 0.6457],\n",
      "        [0.7109, 0.5125, 0.5125]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "target:  tensor([0, 1, 2, 0])\n",
      "output:  tensor(0.9810, grad_fn=<SqueezeBackward0>)\n",
      "predicted:  tensor([[0.5622, 0.5498, 0.6341],\n",
      "        [0.6341, 0.5125, 0.5987],\n",
      "        [0.5250, 0.5744, 0.6457],\n",
      "        [0.7109, 0.5125, 0.5125]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "target:  tensor([0, 1, 2, 0])\n",
      "output:  tensor([1.3178, 1.8515, 1.9099], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([\n",
    "                [0.25, 0.20, 0.55],\n",
    "                [0.55, 0.05, 0.40],\n",
    "                [0.10, 0.30, 0.60],\n",
    "                [0.90, 0.05, 0.05]\n",
    "            ])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "pred = m(pred)\n",
    "\n",
    "target = np.array([0, 1, 2, 0])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "loss = MulticlassHingeLoss(num_classes=3) # Multiclass Hinge loss\n",
    "output = loss(pred, target)\n",
    "# output.backward()\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "loss_squared = MulticlassHingeLoss(num_classes=3, multiclass_mode='one-vs-all', squared=True) # Multiclass squared Hinge loss\n",
    "output = loss_squared(pred, target)\n",
    "output.mean().backward() # *** backward is only valid if loss is a tensor containing a single element.\n",
    "\n",
    "print('predicted: ', pred)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55c53a-0a83-4eaa-bd9d-f3c2d2d23982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378f31a-307b-4423-b89e-dbf530be0890",
   "metadata": {},
   "source": [
    "--> Focal Loss is particularly useful in cases where there is a class imbalance. <br>\n",
    "--> Focal Loss was introduced by Lin et al of Facebook AI Research in 2017 as a means of combatting extremely imbalanced datasets <br>\n",
    "where positive cases were relatively rare. Their paper \"Focal Loss for Dense Object Detection\" is retrievable here: https://arxiv.org/abs/1708.02002. <br>\n",
    "--> The idea is give high weights to the rare class and small weights to the dominating or common class. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& FL(p_{t}) = \\alpha_{t}(1 - p_{t})^{\\gamma}log(p_{t}) \\\\\n",
    "& p_{t} = \\text{probability of ground truth class} \\\\\n",
    "& \n",
    "\\end{align}\n",
    "$ <br>\n",
    "--> https://github.com/pytorch/vision/blob/main/torchvision/ops/focal_loss.py#L38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61889a3d-6535-4f09-81a9-af35897dab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "\n",
    "'''\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha # weight assigned to class 1\n",
    "        self.gamma = gamma   \n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        '''\n",
    "            :param inputs: batch_size * dim\n",
    "            :param targets: (batch,)\n",
    "            :return:\n",
    "        '''\n",
    "        \n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets) # reduction should be none\n",
    "        \n",
    "        p = torch.sigmoid(preds)\n",
    "        pt = p * targets + (1 - p) * (1 - targets)\n",
    "        \n",
    "        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        \n",
    "        \n",
    "        if self.reduction == \"none\":\n",
    "            pass\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        \n",
    "        print(\"preds:- \",preds)\n",
    "        print(\"targets:- \", targets)\n",
    "        print(\"pt:- \", pt)\n",
    "        print(\"at:- \", at)\n",
    "        print(\"BCE_loss:- \", BCE_loss)\n",
    "        print(\"F_loss:- \", loss)\n",
    "        \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c04ea956-4a51-40a9-9f98-4c1f4fa0b941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight:-  0.6666666666666666\n",
      "preds:-  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "targets:-  tensor([1., 0., 0.], dtype=torch.float64)\n",
      "pt:-  tensor([0.8482, 0.0442, 0.6247], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "at:-  tensor([0.6667, 0.3333, 0.3333], dtype=torch.float64)\n",
      "BCE_loss:-  tensor([0.1647, 3.1187, 0.4705], dtype=torch.float64,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "F_loss:-  tensor(0.3248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pos_weight:-  0.3333333333333333\n",
      "preds:-  tensor([ 1.7204,  3.0735, -0.5096], dtype=torch.float64, requires_grad=True)\n",
      "targets:-  tensor([1., 1., 0.], dtype=torch.float64)\n",
      "pt:-  tensor([0.8482, 0.9558, 0.6247], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "at:-  tensor([0.3333, 0.3333, 0.6667], dtype=torch.float64)\n",
      "BCE_loss:-  tensor([0.1647, 0.0452, 0.4705], dtype=torch.float64,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "F_loss:-  tensor(0.0152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([1.7204,  3.0735, -0.5096])\n",
    "pred = torch.tensor(pred, requires_grad=True)\n",
    "\n",
    "target = np.array([1.,  0., 0.]) \n",
    "target = torch.tensor(target)\n",
    "\n",
    "o_cls_num = torch.numel(target[target == 0])\n",
    "total_cls_num = torch.numel(target)\n",
    "\n",
    "pos_weight = o_cls_num/total_cls_num\n",
    "print(\"pos_weight:- \",pos_weight)\n",
    "loss = FocalLoss(alpha=pos_weight)\n",
    "\n",
    "f_loss = loss(pred, target)\n",
    "\n",
    "target = np.array([1.,  1., 0.]) \n",
    "target = torch.tensor(target)\n",
    "\n",
    "o_cls_num = torch.numel(target[target == 0])\n",
    "total_cls_num = torch.numel(target)\n",
    "\n",
    "pos_weight = o_cls_num/total_cls_num\n",
    "print(\"pos_weight:- \",pos_weight)\n",
    "loss = FocalLoss(alpha=pos_weight)\n",
    "\n",
    "f_loss = loss(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2024062-dc80-4bdd-9546-b28537ba5edc",
   "metadata": {},
   "source": [
    "<b>q) How we can caluculate alpha ? </b> <br>\n",
    "\n",
    "The idea is give high weights to the rare class and small weights to the dominating or common class.<br>\n",
    "<br>\n",
    "we know, aplha = weight assigned to class 1 <br>\n",
    "<br>\n",
    "if 1 is high/common class , then alpha should be small<br>\n",
    "if 1 is low/rare class , then alpha should be high<br>\n",
    "<br>\n",
    "so alpha = [1 - (1_num/total_num)]<br>\n",
    "which means, <br>\n",
    "if 1_num is high then alpha will be small<br>\n",
    "if 1_num is low then alpha will be high<br>\n",
    "<br>\n",
    "the above equation can be written<br>\n",
    "so alpha = [0_num/total_num]<br>\n",
    "<br>\n",
    "that's why we did in code, <br>\n",
    "o_cls_num = torch.numel(target[target == 0])<br>\n",
    "total_cls_num = torch.numel(target)<br>\n",
    "\n",
    "pos_weight = o_cls_num/total_cls_num<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df57f67-5be8-4646-b799-a1928ed7666c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ranking Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95098067-ba0e-48d8-b278-2e417e6e48dc",
   "metadata": {},
   "source": [
    "--> Learning to Rank (LTR) refers to ML techniques for solving ranking task. (applicable to any search/recommendation system) <br>\n",
    "--> We try to learn a function  f(q,D), given a query  q and a relevant list of items  D, to predict the order (ranking) of all items within list. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255ebe3-2280-4c82-9ec0-0af3acbb51e9",
   "metadata": {},
   "source": [
    "--> All learning to rank models are ML model (e.g. Decision tree, Neural model) to compute f(x). <br>\n",
    "--> The choice of loss function is the distinctive element for LTR models.\n",
    "--> There are three approaches, depending on how the loss is calcualted.\n",
    "1) pointwise\n",
    "Input: single candidate Y_i = f(q, d_i)\n",
    "loss: score (treat as regression) i.e. how accurate are s_i (actual score) and Y_i (predicted score).\n",
    "2) pairwise\n",
    "Input: pair candidate Y_i = (q, d_i) and Y_j = (q, d_j) . \n",
    "loss: if s_i > s_j, then are Y_i > Y_j, (treat as binary classification)\n",
    "3) listwise\n",
    "Input: list candidate (q, d_1), (q, d_2), ...., (q, d_n) . \n",
    "loss: RankNet,LambdaRank, LambdaMART, Plackett-Luce model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62d2a8-aee1-4570-9c1a-ba6013b9db11",
   "metadata": {},
   "source": [
    "--> the objective of Ranking Losses is to predict relative distances between inputs.\n",
    "--> It is mainly used in Siamese nets and Triplet nets.\n",
    "--> There are two types Ranking loss:\n",
    "1) Pairwise Ranking loss\n",
    "2) Triplet Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860a994-4376-4f51-86f2-8e623d006486",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Margin Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08d6d3-dd58-4fb4-b553-5b85cd7a8c1d",
   "metadata": {},
   "source": [
    "--> It computes the relative distances between inputs. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& loss(x, y) = max(0, -y * (x1 - x2) + margin) \\\\\n",
    "& x1, x2 :- inputs \\\\\n",
    "& y : label (1 or -1) \\\\\n",
    "& \\text{when y == 1, the first input will be ranked higher. when y == -1, the second input will be ranked higher.}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cc5062ef-05e9-45ed-bcb5-27b9dc75926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one:  tensor([-0.2305,  0.1481,  1.7113], dtype=torch.float64, requires_grad=True)\n",
      "input two:  tensor([-1.6993,  2.2877, -0.4003], dtype=torch.float64, requires_grad=True)\n",
      "target:  tensor([-1.,  1.,  1.], dtype=torch.float64)\n",
      "output:  tensor(1.2028, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_one = np.array([-0.2305,  0.1481,  1.7113])\n",
    "input_one = torch.tensor(input_one, requires_grad=True)\n",
    "\n",
    "\n",
    "input_two = np.array([-1.6993,  2.2877, -0.4003])\n",
    "input_two = torch.tensor(input_two, requires_grad=True)\n",
    "\n",
    "target = np.array([-1.,  1.,  1.])\n",
    "target = torch.tensor(target)\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310dba0-c068-4a95-b3f8-e5788da1c8a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Triplet Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef661f56-9c29-4812-b882-7edab947fa79",
   "metadata": {},
   "source": [
    "--> It computes a criterion for measuring the triplet loss in models. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& loss(a, p, n) = max\\{d(a_{i}, p_{i}) - d(a_{i}, n_{i}) + margin, 0\\} \\\\\n",
    "& x1, x2, x3 :- inputs \\\\\n",
    "& \\text{A triplet consists of a (anchor), p (positive examples), and n (negative examples)} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f78a547-8cc0-4106-b3d5-55e956c28bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([[-0.8134, -0.8855],\n",
      "        [-1.7437,  1.3037],\n",
      "        [-1.3493,  0.3088]], dtype=torch.float64, requires_grad=True)\n",
      "p:  tensor([[ 0.1226, -1.6654],\n",
      "        [-0.1654, -0.1100],\n",
      "        [ 0.9043,  0.4446]], dtype=torch.float64, requires_grad=True)\n",
      "n:  tensor([[-0.3027,  1.3877],\n",
      "        [ 1.1685,  0.9808],\n",
      "        [-0.4683,  0.3750]], dtype=torch.float64, requires_grad=True)\n",
      "output:  tensor(0.3159, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[-0.8134, -0.8855],\n",
    "        [-1.7437,  1.3037],\n",
    "        [-1.3493,  0.3088]]) # anchor\n",
    "a = torch.tensor(a, requires_grad=True)\n",
    "\n",
    "\n",
    "p = np.array([[ 0.1226, -1.6654],\n",
    "        [-0.1654, -0.1100],\n",
    "        [ 0.9043,  0.4446]]) # positive\n",
    "p = torch.tensor(p, requires_grad=True)\n",
    "\n",
    "n = np.array([[-0.3027,  1.3877],\n",
    "        [ 1.1685,  0.9808],\n",
    "        [-0.4683,  0.3750]]) # negative\n",
    "n = torch.tensor(n, requires_grad=True)\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(a, p, n)\n",
    "output.backward()\n",
    "\n",
    "print('a: ', a)\n",
    "print('p: ', p)\n",
    "print('n: ', n)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928e353-5426-44dc-96e8-acf6bac1503a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reinforcement Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd4059-0855-48de-baab-1b1a6bde86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ef2c0-1618-4886-8c50-40a4b3eb9fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b81e37-1c37-4f3a-bde4-99e4593840ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        loss = ((input-target)**2).mean()  \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "# Forward pass to the Network\n",
    "# then, \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b421211-4daa-46d4-9a36-de4131384158",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c59f9b-863a-4e08-bde4-a2ac4ef55bb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315f0b9-a4dc-43bd-82fd-e59f295c8f9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e765e1-14ac-4dd1-881a-7df1645005af",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} \\\\\n",
    "& Precision = \\frac{TP}{TP+FP} \\\\\n",
    "& Recall = \\frac{TP}{TP+FN} \\\\\n",
    "& F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccc116-c36a-43a6-8675-478e7023e7ec",
   "metadata": {},
   "source": [
    "--> <b>ROC curve </b> <br>\n",
    "--> An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds.  <br>\n",
    "This curve plots two parameters: <br>\n",
    "1) True Positive Rate (Y axis) <br>\n",
    "2) False Positive Rate (X axis) <br>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& TPR = \\frac{TP}{TP+FN} \\\\\n",
    "& FPR = \\frac{FP}{FP+TN} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5179c43-71d5-4482-bb4f-fec399dccb3d",
   "metadata": {},
   "source": [
    "--> <b>AUC</b> stands for \"Area under the ROC Curve.\" <br>\n",
    "That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb495be0-ac6e-456e-a72d-90ffa170298d",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& Sensitivity = Recall = \\frac{TP}{TP+FN} \\\\\n",
    "& Specificity = \\frac{TN}{FP+TN} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25380ac1-e532-4ed3-af3a-87add0934366",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8077e307-ee1f-469c-81dc-299182f9ca4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c975155-267d-4c8d-8d63-d4e8d1a698e3",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& Precision = \\frac{TP}{TP+FP} \\\\\n",
    "& Recall = \\frac{TP}{TP+FN} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a0e96-32cc-4455-8d29-f1192de050b8",
   "metadata": {},
   "source": [
    "--> In <b>information retrieval</b>, the definition is different. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& Precision = \\frac{|\\{\\text{relevant doc}\\} \\cap \\{\\text{retrieved doc}\\}|}{|\\{\\text{retrieved doc}\\}|} \\\\\n",
    "& Recall = \\frac{|\\{\\text{relevant doc}\\} \\cap \\{\\text{retrieved doc}\\}|}{|\\{\\text{relevant doc}\\}|} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd3cb5-7684-4317-9f1d-56617703ff62",
   "metadata": {},
   "source": [
    "By default, precision takes all the retrieved documents into account, but however, it can also be evaluated at a given number of retrieved documents, <br>\n",
    "commonly known as cut-off rank, where the model is only assessed by considering only its top-most queries. The measure is called precision at k or <b> P@K </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89256e53-06e2-44d8-b590-23322a0719e1",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& AP@n = \\frac{1}{GTP} \\sum_{k}^{n} (P@k * rel@k) \\\\\n",
    "& GTP total number of ground truth positives \\\\\n",
    "& P@k = precision@k \\\\\n",
    "& rel@k = relevance@k\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448692c-76f1-4dd2-8731-a1e2045b5187",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "& mAP = \\frac{1}{N} \\sum_{i = 1}^{N} AP_{i}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828012fa-d8f5-4920-ae67-6c4a697ca792",
   "metadata": {},
   "source": [
    "[source:- https://www.youtube.com/watch?v=pM6DJ0ZZee0] <br>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/MAP.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57099639-c13c-4f23-b993-e67bae842fdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9e730-1fb7-4f3f-ae4e-586ad45015d6",
   "metadata": {},
   "source": [
    "--> Discounted Cumulative Gain (DCG) is the metric of measuring <b>ranking quality</b>. <br>\n",
    "--> It is mostly used in information retrieval problems such as measuring the effectiveness of the search engine algorithm by ranking the <br>\n",
    "articles it displays according to their relevance in terms of the search keyword. <br>\n",
    "--> let's <b>example</b>, <br>\n",
    "Google shows the below documents for a search query. <br>\n",
    "D_1 <br>\n",
    "D_2 <br>\n",
    "D_3 <br>\n",
    "D_4 <br>\n",
    "D_5 <br>\n",
    "--> Now give relevance score to every document. [0 : not relevant, 1-2 : somewhat relevant, 3 : completely relevant] <br>\n",
    "D_1 :- 3 <br>\n",
    "D_2 :- 2 <br>\n",
    "D_3 :- 0 <br>\n",
    "D_4 :- 0 <br>\n",
    "D_5 :- 1 <br>\n",
    "\n",
    "<b>STEP 1: </b> <br>\n",
    "Calculate <b>Cumulative Gain</b>. $CG_{p}$ at a particular rank position p is defined as <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& CG_{p} = \\sum_{i=1}^{p}rel_{i} \\\\\n",
    "& rel_{i} :-  \\text{graded relevance of the result at position i}\n",
    "\\end{align}\n",
    "$ <br>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& CG = \\sum_{i=1}^{5}rel_{i} = 3 + 2 + 0 + 0 + 1 = 6 \n",
    "\\end{align}\n",
    "$ <br>\n",
    "\n",
    "<b>STEP 2: </b> <br>\n",
    "Calculate <b>Discounted Cumulative Gain</b>. DCG is that highly relevant documents appearing lower in a search result list should be penalized as the <br>\n",
    "graded relevance value is reduced logarithmically proportional to the position of the result. <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& DCG_{p} = \\sum_{i=1}^{p} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& rel_{i} :-  \\text{graded relevance of the result at position i}\n",
    "\\end{align}\n",
    "$ <br>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& DCG_{p} = \\sum_{i=1}^{p} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& = \\frac{3}{log_{2}(2)} + \\frac{2}{log_{2}(3)} + \\frac{0}{log_{2}(4)} + \\frac{0}{log_{2}(5)} + \\frac{1}{log_{2}(6)} \\\\\n",
    "& = 4.67\n",
    "\\end{align}\n",
    "$ <br>\n",
    "\n",
    "<b>STEP 3: </b> <br>\n",
    "Search result lists vary in length depending on the query. Comparing a search engine's performance from one query to the next cannot be consistently <br>\n",
    "achieved using DCG alone, so the cumulative gain at each position for a chosen value of p should be normalized across queries.  <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& nDCG_{p} = \\frac{DCG_{p}}{IDCG_{p}} \\\\\n",
    "& DCG_{p} = \\sum_{i=1}^{p} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& IDCG_{p} = \\sum_{i=1}^{|REL_{p}|} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& REL_{p} :-  \\text{represents the list of relevant documents ordered by their relevance in the corpus up to position p.} \\\\\n",
    "& rel_{i} :-  \\text{graded relevance of the result at position i}\n",
    "\\end{align}\n",
    "$ <br>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "& DCG_{p} = \\sum_{i=1}^{p} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& = \\frac{3}{log_{2}(2)} + \\frac{2}{log_{2}(3)} + \\frac{0}{log_{2}(4)} + \\frac{0}{log_{2}(5)} + \\frac{1}{log_{2}(6)} \\\\\n",
    "& = 4.67 \\\\\n",
    "& \\text{Now we need to arrange these articles in descending order by rankings and calculate DCG to get the Ideal Discounted Cumulative Gain (IDCG) ranking. } \\\\\n",
    "& IDCG_{p} = \\sum_{i=1}^{|REL_{p}|} \\frac{2^{rel_{i} - 1}}{log_{2} (i+1)} \\\\\n",
    "& = \\frac{3}{log_{2}(2)} + \\frac{2}{log_{2}(3)} + \\frac{1}{log_{2}(4)} + \\frac{0}{log_{2}(5)} + \\frac{0}{log_{2}(6)}  \\\\\n",
    "& = 4.76  \\\\\n",
    "& nDCG_{p} = \\frac{4.67}{4.76} = 0.98\n",
    "\\end{align}\n",
    "$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815adb1-eb08-470a-b623-39264ee0dba7",
   "metadata": {},
   "source": [
    "'''\n",
    "For code follow \n",
    "https://github.com/dkaterenchuk/ranking_measures\n",
    "https://github.com/karlhigley/ranking-metrics-torch\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02919b-0c23-4976-9f07-454873a9b62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b638dd-7286-439e-9b1f-0e75a47d68d8",
   "metadata": {},
   "source": [
    "<b>Q) Difference between loss, cost function, Objective function</b> <br>\n",
    "<b>Loss function</b> is usually a function defined on a data point, prediction and label, and measures the penalty. For example: <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& \\text{square loss} :->  \\\\\n",
    "& 𝑙(𝑓(𝑥_{𝑖}|\\theta),𝑦_{i})=(\\sum_{j=0}^{d}𝑥_{𝑖}|\\theta_{j}−𝑦_{𝑖})^2\n",
    "\\end{align}\n",
    "$ <br>\n",
    "<br>\n",
    "<b>Cost function</b> is usually more general. It might be a sum of loss functions over your training set plus some model complexity penalty (regularization). For example: <br>\n",
    "$\n",
    "\\begin{align}\n",
    "& \\text{Mean Squared Error} \\\\\n",
    "& 𝑀𝑆𝐸(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N}(\\sum_{j=0}^{d}𝑥_{𝑖}|\\theta_{j})−𝑦_{𝑖})^2 + \\lambda \\sum_{j=0}^{d}\\theta_{j}^2\n",
    "\\end{align}\n",
    "$ <br>\n",
    "<br>\n",
    "<b>Objective function</b> is the most general term for any function that you optimize during training. <br>\n",
    "    Not all objective functions are cost function. For example, <br>\n",
    "    MLE(Maximum liklihood) is a type of objective function, which is maximized.\n",
    "A loss function is a part of a cost function which is a type of an objective function.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b35de9-b0b7-42fc-8a3d-d577ff966f2f",
   "metadata": {},
   "source": [
    "<b> Q) When to use L1 loss and L2 loss? </b> <br>\n",
    "L2 is much more sensitive to outliers because the differences are squared, whilst L1 is the absolute difference and is therefore not as sensitive. <br>\n",
    "The choice between L1 and L2 comes down to how much you want to punish outliers in your predictions. If minimising large outliers is important for your <br>\n",
    "model then L2 is best as this will highlight them more due to the squaring, however if occasional large outliers are not an issue then L1 may be best <br>\n",
    "\n",
    "<b>Huber Loss</b> is a combination of MAE and MSE. It overcomes the problem with MAE and MSE. <br>\n",
    "Problem with MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. <br>\n",
    "For MSE, gradient decreases as the loss gets close to its minima, making it more precise, but it is less robust to outliers. <br>\n",
    "\n",
    "So Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. <br> \n",
    "Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyper-parameter delta which is an iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8317d-573a-41bf-92e7-dc3089b8c034",
   "metadata": {},
   "source": [
    "# Resources\n",
    "1) https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "2) https://neptune.ai/blog/pytorch-loss-functions\n",
    "3) https://www.youtube.com/watch?v=7q7E91pHoW4\n",
    "4) https://towardsdatascience.com/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9\n",
    "5) https://stats.stackexchange.com/questions/207794/what-loss-function-for-multi-class-multi-label-classification-tasks-in-neural-n\n",
    "6) https://stackoverflow.com/questions/52855843/multi-label-classification-in-pytorch\n",
    "7) https://discuss.pytorch.org/t/what-kind-of-loss-is-better-to-use-in-multilabel-classification/32203/43\n",
    "8) https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-kullback-leibler-divergence-kl-divergence-with-keras.md\n",
    "9) https://debuggercafe.com/sparse-autoencoders-using-kl-divergence-with-pytorch/\n",
    "10) https://shangeth.com/post/kl-divergence/\n",
    "11) https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "12) https://gombru.github.io/2019/04/03/ranking_loss/\n",
    "13) https://amaarora.github.io/2020/06/29/FocalLoss.html\n",
    "14) https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "15) https://www.geeksforgeeks.org/normalized-discounted-cumulative-gain-multilabel-ranking-metrics-ml/\n",
    "16) https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "17) https://www.youtube.com/watch?v=pM6DJ0ZZee0&t=141s\n",
    "18) https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html\n",
    "19) https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4\n",
    "20) https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\n",
    "21) https://stephenallwright.com/l1-vs-l2-loss/\n",
    "22) https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "23) https://www.kaggle.com/questions-and-answers/173937\n",
    "24) https://stackoverflow.com/questions/53613722/loss-function-for-simple-reinforcement-learning-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea02c5-75ce-4ad8-b44c-fb1723b92156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
